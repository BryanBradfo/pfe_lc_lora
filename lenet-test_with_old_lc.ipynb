{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\utils\\generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "import old_lc.main as olc\n",
    "from src.models.LeNet import LeNet\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.LeNet_LowRank import getBase, LeNet_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, lazy_restore, evaluate_compression\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.1307, 0.3081)])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    # Reintroduce the 2000 datapoints model has not seen before.\n",
    "    trainset.data = trainset.data.clone()[-2000:-1000]\n",
    "    trainset.targets = trainset.targets.clone()[-2000:-1000]\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "    testset = datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    testset.data = trainset.data[-1000:]\n",
    "    testset.targets = trainset.targets[-1000:]\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "# def data_loader():\n",
    "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(0.1307, 0.3081)])\n",
    "\n",
    "#     trainset = datasets.MNIST(root='./data', train=True,\n",
    "#                                           download=True, transform=transform)\n",
    "#     # Reintroduce the 7000 datapoints model has not seen before.\n",
    "#     trainset.data = trainset.data.clone()[-7000:]\n",
    "#     trainset.targets = trainset.targets.clone()[:-7000:]\n",
    "#     train_loader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "#                                               shuffle=False, num_workers=2)\n",
    "\n",
    "#     testset = datasets.MNIST(root='./data', train=False,\n",
    "#                                          download=True, transform=transform)\n",
    "\n",
    "#     testset.data = trainset.data[-2000:]\n",
    "#     testset.targets = trainset.targets[-2000:]\n",
    "#     test_loader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "#                                              shuffle=False, num_workers=2)\n",
    "    \n",
    "#     return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2) tensor(3) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "# Adjust these values to match the normalization values used during the loading of your dataset\n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    # Adjusting unnormalization for potentially 3-channel images\n",
    "    img = img * std + mean  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Assuming train_loader is defined and loaded as before\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images[:3]))\n",
    "# Print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/lobranch\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "\n",
    "SAVE_LOC_OLC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/old-lc\"\n",
    "if not os.path.exists(SAVE_LOC_OLC):\n",
    "    os.makedirs(SAVE_LOC_OLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binary(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = torch.sigmoid(model(input))  # Apply sigmoid to model output to get probabilities.\n",
    "            output = torch.where(output > 0.5, 1, 0)  # Threshold probabilities at 0.5 to decide between classes 0 and 1.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += (output == label).sum().item()  # Increment correct predictions by the number of matches in the batch.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n",
    "\n",
    "def accuracy_multiclass(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = model(input)  # Get the raw logits from the model.\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max logit which represents the predicted class.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += pred.eq(label.view_as(pred)).sum().item()  # Compare predictions with true labels and sum up correct predictions.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the base model (LeNet)\n",
    "\n",
    "model_for_checkpoint = LeNet()\n",
    "\n",
    "# creating branchpoints : \n",
    "\n",
    "epochs = 10\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_for_checkpoint.parameters(), lr=0.01, momentum=0.9) # momentum=0.9\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # print(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_for_checkpoint(inputs)\n",
    "\n",
    "        # if self.config.loss_function == \"binary_cross_entropy\":\n",
    "        #     outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # loss = loss_function(outputs, labels)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Epoch {} | Iteration {} : Loss {}\".format(epoch, iter, loss.item()))\n",
    "        if iter % 20 == 0:\n",
    "            print(\"Running validation for {} Epoch, {} Iteration...\".format(epoch, iter))\n",
    "            # Previously: res = accuracy_binary(model_for_checkpoint, test_loader)\n",
    "            res = accuracy_multiclass(model_for_checkpoint, test_loader)\n",
    "\n",
    "            # res = accuracy_binary(model_for_checkpoint, test_loader)\n",
    "            \n",
    "            print(\"ACCURACY: {}\".format(res))\n",
    "            if res > 0.7:\n",
    "                torch.save(model_for_checkpoint.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(res))\n",
    "            if res > 0.9:\n",
    "                isLoop = False\n",
    "                break\n",
    "    if not isLoop:\n",
    "        break\n",
    "    print(\"Length of train_loader is: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSED_LAYERS = ['classifier.1.weight', 'classifier.3.weight']\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "BRANCH_ACC = \"0.922\"\n",
    "\n",
    "original = LeNet()\n",
    "model_original = LeNet()\n",
    "\n",
    "# Load from \"branch point\"\n",
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "\n",
    "w, b = getBase(model_original)\n",
    "model = LeNet_LowRank(w, b, rank = RANK)\n",
    "load_sd_decomp(torch.load(BRANCH_LOC), model, DECOMPOSED_LAYERS)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "optimizer_full = torch.optim.SGD(model_original.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/lobranch/set_0\\base_model.pt\n",
      "Training Accuracy | Decomposed: 0.90625, Full : 0.90625\n",
      "Epoch: 0, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/lobranch/set_0\n",
      "Compressing delta for old_lc\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float32' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 94\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompressing delta for old_lc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# compressed_delta = olc.compress_delta(old_lc_delta, num_bits = 3)\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m olc_compressed_delta, update_prev \u001b[38;5;241m=\u001b[39m \u001b[43molc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompress_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mold_lc_delta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m olc\u001b[38;5;241m.\u001b[39msave_checkpoint(SAVE_LOC_OLC \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/set_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(current_set_old_lc), olc_compressed_delta, \n\u001b[0;32m     96\u001b[0m                     old_lc_bias, epch, current_iter_old_lc)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# prev_state = np.add(prev_state,)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Personal\\Singapour\\PFE\\code_of_shu-heng_with_models\\pfe_lc_lora\\old_lc\\main.py:477\u001b[0m, in \u001b[0;36mcompress_data\u001b[1;34m(Î´t, num_bits, threshold)\u001b[0m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;66;03m# Average the deltas within each group.\u001b[39;00m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m mp:\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# mp[k] = np.mean(mp[k])\u001b[39;00m\n\u001b[1;32m--> 477\u001b[0m     mp[k] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmp\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;66;03m# If threshold is enabled, select the most significant changes.\u001b[39;00m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m threshold:\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3502\u001b[0m, in \u001b[0;36mmean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m   3500\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _methods\u001b[38;5;241m.\u001b[39m_mean(a, axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   3505\u001b[0m                       out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\numpy\\core\\_methods.py:106\u001b[0m, in \u001b[0;36m_mean\u001b[1;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[0;32m    102\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[0;32m    104\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m rcount \u001b[38;5;241m=\u001b[39m \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    108\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\numpy\\core\\_methods.py:77\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[1;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[0;32m     75\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ax \u001b[38;5;129;01min\u001b[39;00m axis:\n\u001b[1;32m---> 77\u001b[0m         items \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mshape[\u001b[43mmu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize_axis_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m     78\u001b[0m     items \u001b[38;5;241m=\u001b[39m nt\u001b[38;5;241m.\u001b[39mintp(items)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;66;03m# TODO: Optimize case when `where` is broadcast along a non-reduction\u001b[39;00m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;66;03m# axis and full sum is more excessive than needed.\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m     \u001b[38;5;66;03m# guarded to protect circular imports\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.float32' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "delta_normal_max = []\n",
    "delta_normal_min = []\n",
    "delta_decomposed_max = []\n",
    "delta_decomposed_min = []\n",
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "current_iter = 0\n",
    "current_set = 0\n",
    "\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0\n",
    "\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n",
    "\n",
    "for epch in range(20):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"Epoch: {}, Iteration: {}\".format(epch, i))\n",
    "        \n",
    "        set_path = \"/set_{}\".format(current_set)\n",
    "        if not os.path.exists(SAVE_LOC + set_path):\n",
    "            os.makedirs(SAVE_LOC + set_path)\n",
    "\n",
    "        if i == 0 and epch == 0: # first iteration, create baseline model\n",
    "            base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "        else:\n",
    "            if i % 10 == 0: \n",
    "                # full snapshot!\n",
    "                new_model = lazy_restore(base, base_decomp, bias, LeNet(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "                original = new_model # Changing previous \"original model\" used to restore the loRA model.\n",
    "                \n",
    "                current_set += 1\n",
    "                current_iter = 0\n",
    "\n",
    "                set_path = \"/set_{}\".format(current_set)\n",
    "                if not os.path.exists(SAVE_LOC + set_path):\n",
    "                    os.makedirs(SAVE_LOC + set_path)\n",
    "                \n",
    "                # Rebuilding LoRA layers => reset model!\n",
    "                w, b = getBase(original)\n",
    "                model = LeNet_LowRank(w, b, rank = RANK)\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "                load_sd_decomp(original.state_dict(), model, DECOMPOSED_LAYERS)\n",
    "                base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "            else:\n",
    "                # Delta-compression\n",
    "                delta, decomp_delta, bias = lc.generate_delta(base, \n",
    "                                                                base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                \n",
    "                # Saving checkpoint\n",
    "                lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                                \"/set_{}\".format(current_set))\n",
    "    \n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                current_iter += 1\n",
    "            \n",
    "        # ==========================\n",
    "        # Saving using LC-Checkpoint\n",
    "        # ==========================\n",
    "                \n",
    "        if i == 0 and epch == 0:\n",
    "            cstate = model_original.state_dict()\n",
    "            set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "            if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "            torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "            prev_state = olc.extract_weights(cstate)\n",
    "            # prev_state = olc.extract_weights(model_original)\n",
    "        else:\n",
    "            if i % 10 == 0:\n",
    "                cstate = model_original.state_dict()\n",
    "                current_set_old_lc += 1\n",
    "                current_iter_old_lc = 0\n",
    "                set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                    os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "                # prev_state = olc.extract_weights(model_original)\n",
    "                prev_state = olc.extract_weights(cstate)\n",
    "            else:\n",
    "                cstate = model_original.state_dict()\n",
    "                old_lc_delta, old_lc_bias = olc.generate_delta(prev_state, cstate)\n",
    "                print(\"Compressing delta for old_lc\")\n",
    "                # compressed_delta = olc.compress_delta(old_lc_delta, num_bits = 3)\n",
    "                olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                                    old_lc_bias, epch, current_iter_old_lc)\n",
    "                # prev_state = np.add(prev_state,)\n",
    "                prev_state = np.add(prev_state, update_prev)\n",
    "                current_iter_old_lc += 1\n",
    "        \n",
    "        # ==========================\n",
    "        # Training on Low-Rank Model\n",
    "        # ==========================\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # ======================\n",
    "        # Training on Full Model\n",
    "        # ======================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_full.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs_full = model_original(inputs)\n",
    "        loss_full = torch.nn.functional.cross_entropy(outputs_full,labels)\n",
    "        loss_full.backward()\n",
    "        optimizer_full.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"Training Accuracy | Decomposed: {}, Full : {}\".format(acc(outputs, labels), \n",
    "                                                                         acc(outputs_full, labels)))\n",
    "\n",
    "        if i != 0  and i % 5 == 0: # Evaluation on testing set\n",
    "            full_accuracy.append(evaluate_accuracy(model_original, test_loader))\n",
    "            decomposed_full_accuracy.append(evaluate_accuracy(model, test_loader))\n",
    "            restored_model = lazy_restore(base, base_decomp, bias, LeNet(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                          rank = RANK, scaling = SCALING)\n",
    "            restored_accuracy.append(evaluate_accuracy(restored_model, test_loader))\n",
    "            restored_lc_model = LeNet()\n",
    "            restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "                                                                  restored_model.state_dict()))\n",
    "            lc_accuracy.append(evaluate_accuracy(restored_lc_model, test_loader))\n",
    "            print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\n",
    "                full_accuracy[-1], lc_accuracy[-1], decomposed_full_accuracy[-1], restored_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "full_accuracy = data['full_acc']\n",
    "lc_accuracy = data[\"lc_restored_accuracy\"]\n",
    "restored_accuracy = data[\"decomposed_restored_accuracy\"]\n",
    "decomposed_full_accuracy = data[\"decomposed_full_accuracy\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"LeNet, Accuracy, Branched @ {} Accuracy\".format(BRANCH_ACC))\n",
    "plt.plot(full_accuracy, label = \"Default LeNet\")\n",
    "# plt.plot(lc_accuracy, label = \"LC LeNet\")\n",
    "plt.plot(decomposed_full_accuracy, label = \"dLoRA LeNet\")\n",
    "plt.plot(restored_accuracy, label = \"dLoRA + LC LeNet\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"LeNet, Accuracy, Branched @ {} Accuracy\".format(BRANCH_ACC))\n",
    "# plt.plot(full_accuracy, label = \"Default LeNet\")\n",
    "plt.plot(lc_accuracy, label = \"LC LeNet\")\n",
    "# plt.plot(decomposed_full_accuracy, label = \"dLoRA LeNet\")\n",
    "# plt.plot(restored_accuracy, label = \"dLoRA + LC LeNet\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['feature.3.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangex = [x for x in range(0, 120) if x % 6 == 0]\n",
    "rangey = [x for x in range(0, 20)]\n",
    "plt.figure(figsize = (40, 10))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"LeNet Absolute Accuracy Loss (Default LeNet vs LC + dLoRA LeNet / LC LeNet)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(restored_accuracy))), label = \"LC + dLoRA LeNet\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC LeNet\")\n",
    "plt.legend()\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.ylim(0, 0.5)\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title(\"LeNet Absolute Restoration Accuracy Loss (LC + dLoRA LeNet & LC LeNet)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(restored_accuracy), \n",
    "                     np.array(decomposed_full_accuracy))), label = \"LC + dLoRA LeNet\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC LeNet\")\n",
    "plt.legend()\n",
    "plt.ylim(0, 0.5)\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getsize(sl):\n",
    "    dir = [x for x in os.listdir(sl)]\n",
    "    csize, usize = 0, 0\n",
    "    for set in dir:\n",
    "        for f in os.listdir(sl + \"/\" + set):\n",
    "            fp = sl + \"/{}/{}\".format(set, f)\n",
    "            csize += os.path.getsize(fp)\n",
    "            usize += 250 * math.pow(2, 10) # torch checkpoint same size\n",
    "    return csize, usize,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_size, uncompressed_size = getsize(SAVE_LOC)\n",
    "a, b = evaluate_compression(uncompressed_size, compressed_size)\n",
    "compressed_size, uncompressed_size = getsize(SAVE_LOC_OLC)\n",
    "a1, b1 = evaluate_compression(uncompressed_size, compressed_size)\n",
    "\n",
    "print(\"LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a1, b1))\n",
    "print(\"LoRA + LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"full_acc\" : full_accuracy,\n",
    "    \"decomposed_restored_accuracy\" : restored_accuracy,\n",
    "    \"decomposed_full_accuracy\" : decomposed_full_accuracy,\n",
    "    \"lc_restored_accuracy\" : lc_accuracy\n",
    "}\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/data.json\", 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
