{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Efficient Checkpointing on LeNet** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these 3 approaches : \n",
    "1. **Initial accuracy measurement:** Train LeNet on MNIST and achieve a baseline accuracy of around 99.9% without considering poisoned models.\n",
    "2. **Incremental learning:** Implement incremental learning on the divided MNIST subsets and measure the accuracy drop due to this method.\n",
    "3. **LC-checkpoint and Delta LoRA:** Apply LC-checkpoint and Delta LoRA on top of incremental learning and observe the resulting accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\utils\\generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "import old_lc.main as olc\n",
    "from src.models.LeNet import LeNet\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.LeNet_LowRank import getBase, LeNet_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, evaluate_accuracy_gpu, lazy_restore,lazy_restore_gpu, evaluate_compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Connexion to wandb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbryanchen1105\u001b[0m (\u001b[33mbryanbradfo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Bradf\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# Connect to W&B\n",
    "wandb.login(key=\"beb938fdf67db528128a4298e19b9997afd83dfd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variables and Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "num_work = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(0.1307, 0.3081)\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "\n",
    "    trainset.data = trainset.data.clone()[:]\n",
    "    trainset.targets = trainset.targets.clone()[:]\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = train_batch_size,\n",
    "                                              shuffle=True, num_workers=num_work)\n",
    "\n",
    "    testset = datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    testset.data = testset.data.clone()[:]\n",
    "    testset.targets = testset.targets.clone()[:]\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = test_batch_size,\n",
    "                                             shuffle=False, num_workers=num_work)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bypass the matplotlib error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verify if data loaded correctly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify the 10 classes of training dataset and 10 classes of testing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the dataloader to extract an image per class\n",
    "def get_images_by_class(dataloader):\n",
    "    images_by_class = {i: None for i in range(10)}\n",
    "    for images, labels in dataloader:\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i].item()\n",
    "            if images_by_class[label] is None:\n",
    "                images_by_class[label] = images[i]\n",
    "            if all(v is not None for v in images_by_class.values()):\n",
    "                return images_by_class\n",
    "    return images_by_class\n",
    "\n",
    "def plot_images(images_by_class, title):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 1.5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    for i in range(10):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images_by_class[i].squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_images = get_images_by_class(train_loader)\n",
    "test_images = get_images_by_class(test_loader)\n",
    "\n",
    "plot_images(train_images, \"Trainset Images by Class\")\n",
    "plot_images(test_images, \"Testset Images by Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify that the 10 first images are visually different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_images(dataloader, title, num_images=10):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 1.5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    images_shown = 0\n",
    "    for images, labels in dataloader:\n",
    "        for i in range(len(images)):\n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "            ax = axes[images_shown]\n",
    "            ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            images_shown += 1\n",
    "        if images_shown >= num_images:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_first_images(train_loader, \"First 10 Trainset Images\")\n",
    "plot_first_images(test_loader, \"First 10 Testset Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the dataset into three subsets having each all classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_loader1: 5996\n",
      "Size of train_loader2: 6000\n",
      "Size of train_loader3: 5999\n",
      "Size of train_loader4: 6001\n",
      "Size of train_loader5: 6001\n",
      "Size of train_loader6: 5998\n",
      "Size of train_loader7: 6000\n",
      "Size of train_loader8: 6000\n",
      "Size of train_loader9: 5999\n",
      "Size of train_loader10: 5996\n",
      "Size of train_loader: 59990\n",
      "Size of test_loader: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "def stratified_split(dataset, proportions):\n",
    "    class_indices = [np.where(np.array(dataset.targets) == i)[0] for i in range(10)]\n",
    "    \n",
    "    split_indices = []\n",
    "    for proportion in proportions:\n",
    "        class_split_indices = [np.split(indices, [int(proportion[0]*len(indices)), int((proportion[0]+proportion[1])*len(indices)), int((proportion[0]+proportion[1]+proportion[2])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3]+proportion[4])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3]+proportion[4]+proportion[5])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3]+proportion[4]+proportion[5]+proportion[6])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3]+proportion[4]+proportion[5]+proportion[6]+proportion[7])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3]+proportion[4]+proportion[5]+proportion[6]+proportion[7]+proportion[8])*len(indices)), int((proportion[0]+proportion[1]+proportion[2]+proportion[3]+proportion[4]+proportion[5]+proportion[6]+proportion[7]+proportion[8]+proportion[9])*len(indices))]) for indices in class_indices]\n",
    "        split_indices.append([np.concatenate([split[i] for split in class_split_indices]) for i in range(10)])\n",
    "    \n",
    "    return split_indices\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load the whole MNIST dataset\n",
    "    full_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # Proportions pour les splits\n",
    "    proportion = (0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1)\n",
    "    proportions = [proportion] * 10\n",
    "    \n",
    "    # Obtenir les indices pour chaque split\n",
    "    split_indices = stratified_split(full_trainset, proportions)\n",
    "    \n",
    "    # Créer des Subsets\n",
    "    trainset1 = Subset(full_trainset, split_indices[0][0])\n",
    "    trainset2 = Subset(full_trainset, split_indices[0][1])\n",
    "    trainset3 = Subset(full_trainset, split_indices[0][2])\n",
    "    trainset4 = Subset(full_trainset, split_indices[0][3])\n",
    "    trainset5 = Subset(full_trainset, split_indices[0][4])\n",
    "    trainset6 = Subset(full_trainset, split_indices[0][5])\n",
    "    trainset7 = Subset(full_trainset, split_indices[0][6])\n",
    "    trainset8 = Subset(full_trainset, split_indices[0][7])\n",
    "    trainset9 = Subset(full_trainset, split_indices[0][8])\n",
    "    trainset10 = Subset(full_trainset, split_indices[0][9])\n",
    "\n",
    "\n",
    "    # Créer des DataLoaders pour chacun des sous-ensembles\n",
    "    train_loader1 = DataLoader(trainset1, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader2 = DataLoader(trainset2, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader3 = DataLoader(trainset3, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader4 = DataLoader(trainset4, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader5 = DataLoader(trainset5, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader6 = DataLoader(trainset6, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader7 = DataLoader(trainset7, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader8 = DataLoader(trainset8, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader9 = DataLoader(trainset9, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader10 = DataLoader(trainset10, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Charger le jeu de données de test complet\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(testset, batch_size=test_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader1, train_loader2, train_loader3, train_loader4, train_loader5, train_loader6, train_loader7, train_loader8, train_loader9, train_loader10, test_loader\n",
    "\n",
    "# Load DataLoaders\n",
    "train_loader1, train_loader2, train_loader3, train_loader4, train_loader5, train_loader6, train_loader7, train_loader8, train_loader9, train_loader10, test_loader = data_loader()\n",
    "\n",
    "# Vérification des tailles des DataLoaders\n",
    "print(f'Size of train_loader1: {len(train_loader1.dataset)}')\n",
    "print(f'Size of train_loader2: {len(train_loader2.dataset)}')\n",
    "print(f'Size of train_loader3: {len(train_loader3.dataset)}')\n",
    "print(f'Size of train_loader4: {len(train_loader4.dataset)}')\n",
    "print(f'Size of train_loader5: {len(train_loader5.dataset)}')\n",
    "print(f'Size of train_loader6: {len(train_loader6.dataset)}')\n",
    "print(f'Size of train_loader7: {len(train_loader7.dataset)}')\n",
    "print(f'Size of train_loader8: {len(train_loader8.dataset)}')\n",
    "print(f'Size of train_loader9: {len(train_loader9.dataset)}')\n",
    "print(f'Size of train_loader10: {len(train_loader10.dataset)}')\n",
    "print(f'Size of train_loader: {len(train_loader1.dataset) + len(train_loader2.dataset) + len(train_loader3.dataset) + len(train_loader4.dataset) + len(train_loader5.dataset) + len(train_loader6.dataset) + len(train_loader7.dataset) + len(train_loader8.dataset) + len(train_loader9.dataset) + len(train_loader10.dataset)}')\n",
    "print(f'Size of test_loader: {len(test_loader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verify the content of train_loader subset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify the 10 classes of training dataset and 10 classes of testing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the dataloader to extract an image per class\n",
    "\n",
    "train_images1 = get_images_by_class(train_loader1)\n",
    "train_images2 = get_images_by_class(train_loader2)\n",
    "train_images3 = get_images_by_class(train_loader3)\n",
    "train_images4 = get_images_by_class(train_loader4)\n",
    "train_images5 = get_images_by_class(train_loader5)\n",
    "train_images6 = get_images_by_class(train_loader6)\n",
    "train_images7 = get_images_by_class(train_loader7)\n",
    "train_images8 = get_images_by_class(train_loader8)\n",
    "train_images9 = get_images_by_class(train_loader9)\n",
    "train_images10 = get_images_by_class(train_loader10)\n",
    "test_images = get_images_by_class(test_loader)\n",
    "\n",
    "plot_images(train_images1, \"Trainset Subset 1 Images by Class\")\n",
    "plot_images(train_images2, \"Trainset Subset 2 Images by Class\")\n",
    "plot_images(train_images3, \"Trainset Subset 3 Images by Class\")\n",
    "plot_images(train_images4, \"Trainset Subset 4 Images by Class\")\n",
    "plot_images(train_images5, \"Trainset Subset 5 Images by Class\")\n",
    "plot_images(train_images6, \"Trainset Subset 6 Images by Class\")\n",
    "plot_images(train_images7, \"Trainset Subset 7 Images by Class\")\n",
    "plot_images(train_images8, \"Trainset Subset 8 Images by Class\")\n",
    "plot_images(train_images9, \"Trainset Subset 9 Images by Class\")\n",
    "plot_images(train_images10, \"Trainset Subset 10 Images by Class\")\n",
    "plot_images(test_images, \"Testset Images by Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify that the 10 first images are visually different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_images(train_loader1, \"First 10 Trainset Subset 1 Images\")\n",
    "plot_first_images(train_loader2, \"First 10 Trainset Subset 2 Images\")\n",
    "plot_first_images(train_loader3, \"First 10 Trainset Subset 3 Images\")\n",
    "plot_first_images(train_loader4, \"First 10 Trainset Subset 4 Images\")\n",
    "plot_first_images(train_loader5, \"First 10 Trainset Subset 5 Images\")\n",
    "plot_first_images(train_loader6, \"First 10 Trainset Subset 6 Images\")\n",
    "plot_first_images(train_loader7, \"First 10 Trainset Subset 7 Images\")\n",
    "plot_first_images(train_loader8, \"First 10 Trainset Subset 8 Images\")\n",
    "plot_first_images(train_loader9, \"First 10 Trainset Subset 9 Images\")\n",
    "plot_first_images(train_loader10, \"First 10 Trainset Subset 10 Images\")\n",
    "plot_first_images(test_loader, \"First 10 Testset Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining some variables and creating files & folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "SAVE_LOC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/lobranch\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "\n",
    "SAVE_LOC_OLC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/old-lc\"\n",
    "if not os.path.exists(SAVE_LOC_OLC):\n",
    "    os.makedirs(SAVE_LOC_OLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First version : LeNet without Incremental Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_for_checkpoint = LeNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_for_checkpoint = nn.DataParallel(model_for_checkpoint)\n",
    "    model_for_checkpoint.to(device)\n",
    "else:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU!\")\n",
    "    model_for_checkpoint.to(device)\n",
    "\n",
    "# Training code\n",
    "NUM_EPOCHES = 50\n",
    "learning_rate = 0.02\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_for_checkpoint.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"LeNet\", \n",
    "           name=\"LeNet-Without-Incremental-Learning\", \n",
    "           tags=[\"LeNet\", \"Without-Incremental-Learning\", \"MNIST\"],\n",
    "           config={\"num_epoches\": NUM_EPOCHES,\n",
    "                    \"model\": \"LeNet\",\n",
    "                    \"train dataset\": \"CIFAR10 train dataset[:]\",\n",
    "                    \"test dataset\": \"CIFAR10 test dataset[:]\",\n",
    "                    \"batch_size on training\": train_batch_size,\n",
    "                    \"batch_size on testing\": test_batch_size,\n",
    "                    \"num_workers\": num_work,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"optimizer\": \"SGD\",\n",
    "                }\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start of model training...\")\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_for_checkpoint.train()\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_for_checkpoint(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_for_checkpoint.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_for_checkpoint(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training...\")\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Second version : LeNet with Incremental Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_incremental_learning = LeNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_with_incremental_learning = nn.DataParallel(model_with_incremental_learning)\n",
    "    model_with_incremental_learning.to(device)\n",
    "else:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU!\")\n",
    "    model_with_incremental_learning.to(device)\n",
    "\n",
    "# Training code\n",
    "NUM_EPOCHES = 45\n",
    "learning_rate = 0.01\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_with_incremental_learning.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"LeNet\", \n",
    "           name=\"LeNet-With-Incremental-Learning\", \n",
    "           tags=[\"LeNet\", \"With-Incremental-Learning\", \"MNIST\"],\n",
    "           config={\"num_epoches\": NUM_EPOCHES,\n",
    "                    \"model\": \"LeNet\",\n",
    "                    \"train dataset\": \"CIFAR10 train dataloader1\",\n",
    "                    \"test dataset\": \"CIFAR10 test dataset[:]\",\n",
    "                    \"batch_size on training\": train_batch_size,\n",
    "                    \"batch_size on testing\": test_batch_size,\n",
    "                    \"num_workers\": num_work,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"optimizer\": \"SGD\",\n",
    "                }\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start of model training on dataloader1...\")\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning.train()\n",
    "        for iter, data in enumerate(train_loader1):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader1.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader1...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# Training code on dataloader2\n",
    "print(\"Start of model training on dataloader2...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning.train()\n",
    "        for iter, data in enumerate(train_loader2):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader2.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader2...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# Training code on dataloader2\n",
    "print(\"Start of model training on dataloader3...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning.train()\n",
    "        for iter, data in enumerate(train_loader3):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader3.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader3...\")\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Third version : LeNet with Incremental Learning, LC-checkpoint, and Delta-LoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Personal\\Singapour\\PFE\\code_of_shu-heng_with_models\\pfe_lc_lora\\wandb\\run-20240705_160139-jdbx0b1u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bryanbradfo/LeNet/runs/jdbx0b1u/workspace' target=\"_blank\">LeNet-With-Incremental-Learning_LC_DLORA-Without-Restore_10x10</a></strong> to <a href='https://wandb.ai/bryanbradfo/LeNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bryanbradfo/LeNet' target=\"_blank\">https://wandb.ai/bryanbradfo/LeNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bryanbradfo/LeNet/runs/jdbx0b1u/workspace' target=\"_blank\">https://wandb.ai/bryanbradfo/LeNet/runs/jdbx0b1u/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of model training on dataloader1...\n",
      "Epoch: [0/19], Training Loss: 2.285443, Validation Loss: 2.271121, Training Accuracy: 0.111074, Validation Accuracy: 0.146400\n",
      "Epoch: [1/19], Training Loss: 2.252956, Validation Loss: 2.232981, Training Accuracy: 0.227318, Validation Accuracy: 0.334400\n",
      "Epoch: [2/19], Training Loss: 2.204758, Validation Loss: 2.172450, Training Accuracy: 0.410607, Validation Accuracy: 0.482300\n",
      "Epoch: [3/19], Training Loss: 2.125271, Validation Loss: 2.071563, Training Accuracy: 0.527685, Validation Accuracy: 0.569800\n",
      "Epoch: [4/19], Training Loss: 1.997414, Validation Loss: 1.917468, Training Accuracy: 0.596898, Validation Accuracy: 0.601600\n",
      "Epoch: [5/19], Training Loss: 1.820089, Validation Loss: 1.724531, Training Accuracy: 0.611741, Validation Accuracy: 0.614300\n",
      "Epoch: [6/19], Training Loss: 1.620833, Validation Loss: 1.527612, Training Accuracy: 0.626918, Validation Accuracy: 0.636900\n",
      "Epoch: [7/19], Training Loss: 1.431183, Validation Loss: 1.350411, Training Accuracy: 0.654603, Validation Accuracy: 0.668200\n",
      "Model saved at accuracy: 0.7058\n",
      "End of model training on dataloader1...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_incremental_learning_lc_dlora = LeNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_with_incremental_learning_lc_dlora = nn.DataParallel(model_with_incremental_learning_lc_dlora)\n",
    "    model_with_incremental_learning_lc_dlora.to(device)\n",
    "else:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU!\")\n",
    "    model_with_incremental_learning_lc_dlora.to(device)\n",
    "\n",
    "# Training code\n",
    "NUM_EPOCHES = 20\n",
    "learning_rate = 0.1\n",
    "learning_rate_dloralc = 0.1\n",
    "learning_rate1 = 0.005\n",
    "# super_step = len(train_loader2)\n",
    "# super_step = 20\n",
    "isLoop = True\n",
    "\n",
    "optimizer = torch.optim.SGD(model_with_incremental_learning_lc_dlora.parameters(), lr=learning_rate1)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"LeNet\", \n",
    "           name=\"LeNet-With-Incremental-Learning_LC_DLORA-Without-Restore_10x10\", \n",
    "           tags=[\"LeNet\", \"With-Incremental-Learning_LC_DLORA\", \"MNIST\"],\n",
    "           config={\"num_epoches\": NUM_EPOCHES,\n",
    "                    \"model\": \"LeNet\",\n",
    "                    \"splitting\": \"10-10-10-10-10-10-10-10-10-10\",\n",
    "                    \"train dataset 1\": \"MNIST train dataloader1\",\n",
    "                    \"test dataset\": \"MNIST test dataset[:]\",\n",
    "                    \"batch_size on training\": train_batch_size,\n",
    "                    \"batch_size on testing\": test_batch_size,\n",
    "                    \"num_workers\": num_work,\n",
    "                    \"learning_rate_nothing\": learning_rate,\n",
    "                    \"learning_rate_dloralc\": learning_rate_dloralc,\n",
    "                    \"optimizer\": \"SGD\"\n",
    "                }\n",
    "           )\n",
    "\n",
    "print(\"Start of model training on dataloader1...\")\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning_lc_dlora.train()\n",
    "        for iter, data in enumerate(train_loader1):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning_lc_dlora(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning_lc_dlora.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning_lc_dlora(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader1.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    if valid_accuracy > 0.7:\n",
    "        rounded_valid_acc = round(valid_accuracy, 4)\n",
    "        # torch.save(model_for_checkpoint.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/vit/branch_{}.pt\".format(rounded_valid_acc))\n",
    "        torch.save(model_with_incremental_learning_lc_dlora.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(rounded_valid_acc))\n",
    "        print(\"Model saved at accuracy: {:.4f}\".format(rounded_valid_acc))\n",
    "        isLoop = False\n",
    "        break\n",
    "    # if valid_accuracy > 0.90:\n",
    "    #     isLoop = False\n",
    "    #     break\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss_dloralc\": train_loss,\n",
    "        \"valid_loss_dloralc\": valid_loss,\n",
    "        \"train_accuracy_dloralc\": train_accuarcy,\n",
    "        \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "        \"train_loss_lc\": train_loss,\n",
    "        \"valid_loss_lc\": valid_loss,\n",
    "        \"train_accuracy_lc\": train_accuarcy,\n",
    "        \"valid_accuracy_lc\": valid_accuracy, \n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "    # wandb.log({\n",
    "    #     \"train_loss_dloralc\": train_loss,\n",
    "    #     \"valid_loss_dloralc\": valid_loss,\n",
    "    #     \"train_accuracy_dloralc\": train_accuarcy,\n",
    "    #     \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "    #     \"train_loss_lc\": train_loss,\n",
    "    #     \"valid_loss_lc\": valid_loss,\n",
    "    #     \"train_accuracy_lc\": train_accuarcy,\n",
    "    #     \"valid_accuracy_lc\": valid_accuracy, \n",
    "    #     \"train_loss\": train_loss,\n",
    "    #     \"valid_loss\": valid_loss,\n",
    "    #     \"train_accuracy\": train_accuarcy,\n",
    "    #     \"valid_accuracy\": valid_accuracy,\n",
    "    #     \"valid_loss_dloralc_restored\": valid_loss,\n",
    "    #     \"valid_accuracy_dloralc_restored\": valid_accuracy,\n",
    "    #     \"epoch\": epoch,\n",
    "    # })\n",
    "\n",
    "print(\"End of model training on dataloader1...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working iteratively on training with delta-LoRA and LC-checkpoint on each train dataloader** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader2...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.843772, Validation Loss DLoRALC: 0.620693, Training Accuracy DLoRALC: 0.778333, Validation Accuracy DLoRALC: 0.843400 \n",
      "             \t Training Loss LC: 0.650667, Validation Loss LC: 0.413343, Training Accuracy LC: 0.827667, Validation Accuracy LC: 0.885100, \n",
      "             \t Training Loss: 0.650668, Validation Loss: 0.413343, Training Accuracy: 0.827667, Validation Accuracy: 0.885100\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.525748, Validation Loss DLoRALC: 0.443181, Training Accuracy DLoRALC: 0.867500, Validation Accuracy DLoRALC: 0.883400 \n",
      "             \t Training Loss LC: 0.362199, Validation Loss LC: 0.322850, Training Accuracy LC: 0.897500, Validation Accuracy LC: 0.905800, \n",
      "             \t Training Loss: 0.362199, Validation Loss: 0.322850, Training Accuracy: 0.897500, Validation Accuracy: 0.905800\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.405344, Validation Loss DLoRALC: 0.366527, Training Accuracy DLoRALC: 0.888833, Validation Accuracy DLoRALC: 0.898900 \n",
      "             \t Training Loss LC: 0.291661, Validation Loss LC: 0.274993, Training Accuracy LC: 0.913833, Validation Accuracy LC: 0.917100, \n",
      "             \t Training Loss: 0.291661, Validation Loss: 0.274994, Training Accuracy: 0.913833, Validation Accuracy: 0.917100\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.341973, Validation Loss DLoRALC: 0.324568, Training Accuracy DLoRALC: 0.903667, Validation Accuracy DLoRALC: 0.906000 \n",
      "             \t Training Loss LC: 0.242727, Validation Loss LC: 0.237374, Training Accuracy LC: 0.929000, Validation Accuracy LC: 0.928400, \n",
      "             \t Training Loss: 0.242727, Validation Loss: 0.237374, Training Accuracy: 0.929000, Validation Accuracy: 0.928400\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.302598, Validation Loss DLoRALC: 0.289814, Training Accuracy DLoRALC: 0.913333, Validation Accuracy DLoRALC: 0.915800 \n",
      "             \t Training Loss LC: 0.210601, Validation Loss LC: 0.219362, Training Accuracy LC: 0.937167, Validation Accuracy LC: 0.933000, \n",
      "             \t Training Loss: 0.210601, Validation Loss: 0.219362, Training Accuracy: 0.937167, Validation Accuracy: 0.933000\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.269233, Validation Loss DLoRALC: 0.268647, Training Accuracy DLoRALC: 0.922167, Validation Accuracy DLoRALC: 0.919800 \n",
      "             \t Training Loss LC: 0.181343, Validation Loss LC: 0.189795, Training Accuracy LC: 0.949500, Validation Accuracy LC: 0.941900, \n",
      "             \t Training Loss: 0.181343, Validation Loss: 0.189796, Training Accuracy: 0.949500, Validation Accuracy: 0.941900\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.237958, Validation Loss DLoRALC: 0.249190, Training Accuracy DLoRALC: 0.930167, Validation Accuracy DLoRALC: 0.926100 \n",
      "             \t Training Loss LC: 0.157258, Validation Loss LC: 0.168495, Training Accuracy LC: 0.954333, Validation Accuracy LC: 0.948400, \n",
      "             \t Training Loss: 0.157258, Validation Loss: 0.168493, Training Accuracy: 0.954333, Validation Accuracy: 0.948400\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.214789, Validation Loss DLoRALC: 0.249304, Training Accuracy DLoRALC: 0.938667, Validation Accuracy DLoRALC: 0.922500 \n",
      "             \t Training Loss LC: 0.137935, Validation Loss LC: 0.166130, Training Accuracy LC: 0.957333, Validation Accuracy LC: 0.946100, \n",
      "             \t Training Loss: 0.137935, Validation Loss: 0.166132, Training Accuracy: 0.957333, Validation Accuracy: 0.946100\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.195708, Validation Loss DLoRALC: 0.251534, Training Accuracy DLoRALC: 0.940333, Validation Accuracy DLoRALC: 0.919000 \n",
      "             \t Training Loss LC: 0.122250, Validation Loss LC: 0.141212, Training Accuracy LC: 0.962167, Validation Accuracy LC: 0.955500, \n",
      "             \t Training Loss: 0.122249, Validation Loss: 0.141211, Training Accuracy: 0.962167, Validation Accuracy: 0.955500\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.174777, Validation Loss DLoRALC: 0.193504, Training Accuracy DLoRALC: 0.948167, Validation Accuracy DLoRALC: 0.942700 \n",
      "             \t Training Loss LC: 0.106766, Validation Loss LC: 0.143196, Training Accuracy LC: 0.969333, Validation Accuracy LC: 0.957500, \n",
      "             \t Training Loss: 0.106767, Validation Loss: 0.143197, Training Accuracy: 0.969333, Validation Accuracy: 0.957500\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.157266, Validation Loss DLoRALC: 0.211241, Training Accuracy DLoRALC: 0.953333, Validation Accuracy DLoRALC: 0.937500 \n",
      "             \t Training Loss LC: 0.095137, Validation Loss LC: 0.138108, Training Accuracy LC: 0.973167, Validation Accuracy LC: 0.959100, \n",
      "             \t Training Loss: 0.095137, Validation Loss: 0.138107, Training Accuracy: 0.973167, Validation Accuracy: 0.959100\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.148883, Validation Loss DLoRALC: 0.164376, Training Accuracy DLoRALC: 0.955833, Validation Accuracy DLoRALC: 0.951100 \n",
      "             \t Training Loss LC: 0.084797, Validation Loss LC: 0.134101, Training Accuracy LC: 0.976167, Validation Accuracy LC: 0.957600, \n",
      "             \t Training Loss: 0.084797, Validation Loss: 0.134103, Training Accuracy: 0.976167, Validation Accuracy: 0.957600\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.130890, Validation Loss DLoRALC: 0.142880, Training Accuracy DLoRALC: 0.959167, Validation Accuracy DLoRALC: 0.956200 \n",
      "             \t Training Loss LC: 0.076295, Validation Loss LC: 0.110051, Training Accuracy LC: 0.979500, Validation Accuracy LC: 0.967700, \n",
      "             \t Training Loss: 0.076295, Validation Loss: 0.110051, Training Accuracy: 0.979500, Validation Accuracy: 0.967700\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.121581, Validation Loss DLoRALC: 0.146454, Training Accuracy DLoRALC: 0.964000, Validation Accuracy DLoRALC: 0.956800 \n",
      "             \t Training Loss LC: 0.067261, Validation Loss LC: 0.110036, Training Accuracy LC: 0.980667, Validation Accuracy LC: 0.965800, \n",
      "             \t Training Loss: 0.067261, Validation Loss: 0.110036, Training Accuracy: 0.980667, Validation Accuracy: 0.965800\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.109794, Validation Loss DLoRALC: 0.139298, Training Accuracy DLoRALC: 0.967167, Validation Accuracy DLoRALC: 0.958400 \n",
      "             \t Training Loss LC: 0.060787, Validation Loss LC: 0.103586, Training Accuracy LC: 0.984500, Validation Accuracy LC: 0.968800, \n",
      "             \t Training Loss: 0.060787, Validation Loss: 0.103586, Training Accuracy: 0.984667, Validation Accuracy: 0.968800\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.104494, Validation Loss DLoRALC: 0.124450, Training Accuracy DLoRALC: 0.969167, Validation Accuracy DLoRALC: 0.961200 \n",
      "             \t Training Loss LC: 0.054898, Validation Loss LC: 0.102745, Training Accuracy LC: 0.985167, Validation Accuracy LC: 0.968300, \n",
      "             \t Training Loss: 0.054899, Validation Loss: 0.102745, Training Accuracy: 0.985167, Validation Accuracy: 0.968300\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.094066, Validation Loss DLoRALC: 0.183580, Training Accuracy DLoRALC: 0.971333, Validation Accuracy DLoRALC: 0.940800 \n",
      "             \t Training Loss LC: 0.050380, Validation Loss LC: 0.121363, Training Accuracy LC: 0.987167, Validation Accuracy LC: 0.961400, \n",
      "             \t Training Loss: 0.050380, Validation Loss: 0.121359, Training Accuracy: 0.987167, Validation Accuracy: 0.961400\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.087241, Validation Loss DLoRALC: 0.122345, Training Accuracy DLoRALC: 0.972500, Validation Accuracy DLoRALC: 0.961800 \n",
      "             \t Training Loss LC: 0.044671, Validation Loss LC: 0.107428, Training Accuracy LC: 0.989500, Validation Accuracy LC: 0.967700, \n",
      "             \t Training Loss: 0.044671, Validation Loss: 0.107429, Training Accuracy: 0.989500, Validation Accuracy: 0.967700\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.078585, Validation Loss DLoRALC: 0.123593, Training Accuracy DLoRALC: 0.977500, Validation Accuracy DLoRALC: 0.963200 \n",
      "             \t Training Loss LC: 0.040229, Validation Loss LC: 0.112591, Training Accuracy LC: 0.990667, Validation Accuracy LC: 0.963100, \n",
      "             \t Training Loss: 0.040229, Validation Loss: 0.112591, Training Accuracy: 0.990667, Validation Accuracy: 0.963100\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.074841, Validation Loss DLoRALC: 0.113049, Training Accuracy DLoRALC: 0.978000, Validation Accuracy DLoRALC: 0.966500 \n",
      "             \t Training Loss LC: 0.037421, Validation Loss LC: 0.095124, Training Accuracy LC: 0.991333, Validation Accuracy LC: 0.970400, \n",
      "             \t Training Loss: 0.037421, Validation Loss: 0.095125, Training Accuracy: 0.991333, Validation Accuracy: 0.970400\n",
      "End of model training on dataloader2...\n",
      "Model saved at accuracy: 0.9665\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader3...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.133956, Validation Loss DLoRALC: 0.102994, Training Accuracy DLoRALC: 0.958493, Validation Accuracy DLoRALC: 0.969800 \n",
      "             \t Training Loss LC: 0.114099, Validation Loss LC: 0.086698, Training Accuracy LC: 0.967328, Validation Accuracy LC: 0.973100, \n",
      "             \t Training Loss: 0.114100, Validation Loss: 0.086696, Training Accuracy: 0.967328, Validation Accuracy: 0.973100\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.105104, Validation Loss DLoRALC: 0.101904, Training Accuracy DLoRALC: 0.967661, Validation Accuracy DLoRALC: 0.968600 \n",
      "             \t Training Loss LC: 0.081913, Validation Loss LC: 0.102932, Training Accuracy LC: 0.973329, Validation Accuracy LC: 0.967800, \n",
      "             \t Training Loss: 0.081915, Validation Loss: 0.102933, Training Accuracy: 0.973329, Validation Accuracy: 0.967800\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.090730, Validation Loss DLoRALC: 0.095869, Training Accuracy DLoRALC: 0.971329, Validation Accuracy DLoRALC: 0.968100 \n",
      "             \t Training Loss LC: 0.063783, Validation Loss LC: 0.106163, Training Accuracy LC: 0.980163, Validation Accuracy LC: 0.965000, \n",
      "             \t Training Loss: 0.063783, Validation Loss: 0.106167, Training Accuracy: 0.980163, Validation Accuracy: 0.965000\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.076116, Validation Loss DLoRALC: 0.115318, Training Accuracy DLoRALC: 0.976663, Validation Accuracy DLoRALC: 0.963200 \n",
      "             \t Training Loss LC: 0.054519, Validation Loss LC: 0.072549, Training Accuracy LC: 0.982997, Validation Accuracy LC: 0.977500, \n",
      "             \t Training Loss: 0.054519, Validation Loss: 0.072551, Training Accuracy: 0.982997, Validation Accuracy: 0.977500\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.068041, Validation Loss DLoRALC: 0.094876, Training Accuracy DLoRALC: 0.978996, Validation Accuracy DLoRALC: 0.971300 \n",
      "             \t Training Loss LC: 0.044405, Validation Loss LC: 0.089852, Training Accuracy LC: 0.987331, Validation Accuracy LC: 0.970100, \n",
      "             \t Training Loss: 0.044406, Validation Loss: 0.089840, Training Accuracy: 0.987331, Validation Accuracy: 0.970200\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.060223, Validation Loss DLoRALC: 0.080502, Training Accuracy DLoRALC: 0.982330, Validation Accuracy DLoRALC: 0.973900 \n",
      "             \t Training Loss LC: 0.038635, Validation Loss LC: 0.069835, Training Accuracy LC: 0.988331, Validation Accuracy LC: 0.977900, \n",
      "             \t Training Loss: 0.038634, Validation Loss: 0.069834, Training Accuracy: 0.988331, Validation Accuracy: 0.977900\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.055492, Validation Loss DLoRALC: 0.095413, Training Accuracy DLoRALC: 0.983164, Validation Accuracy DLoRALC: 0.971200 \n",
      "             \t Training Loss LC: 0.032827, Validation Loss LC: 0.069177, Training Accuracy LC: 0.992332, Validation Accuracy LC: 0.977800, \n",
      "             \t Training Loss: 0.032828, Validation Loss: 0.069179, Training Accuracy: 0.992332, Validation Accuracy: 0.977800\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.048314, Validation Loss DLoRALC: 0.078691, Training Accuracy DLoRALC: 0.985164, Validation Accuracy DLoRALC: 0.976000 \n",
      "             \t Training Loss LC: 0.026965, Validation Loss LC: 0.066478, Training Accuracy LC: 0.993832, Validation Accuracy LC: 0.978400, \n",
      "             \t Training Loss: 0.026965, Validation Loss: 0.066479, Training Accuracy: 0.993832, Validation Accuracy: 0.978400\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.044427, Validation Loss DLoRALC: 0.081162, Training Accuracy DLoRALC: 0.986998, Validation Accuracy DLoRALC: 0.975500 \n",
      "             \t Training Loss LC: 0.025424, Validation Loss LC: 0.066939, Training Accuracy LC: 0.994332, Validation Accuracy LC: 0.978600, \n",
      "             \t Training Loss: 0.025424, Validation Loss: 0.066940, Training Accuracy: 0.994332, Validation Accuracy: 0.978600\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.039840, Validation Loss DLoRALC: 0.073639, Training Accuracy DLoRALC: 0.988998, Validation Accuracy DLoRALC: 0.976600 \n",
      "             \t Training Loss LC: 0.021561, Validation Loss LC: 0.065921, Training Accuracy LC: 0.996166, Validation Accuracy LC: 0.978600, \n",
      "             \t Training Loss: 0.021561, Validation Loss: 0.065921, Training Accuracy: 0.996166, Validation Accuracy: 0.978600\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.035547, Validation Loss DLoRALC: 0.081852, Training Accuracy DLoRALC: 0.989665, Validation Accuracy DLoRALC: 0.974900 \n",
      "             \t Training Loss LC: 0.018631, Validation Loss LC: 0.066808, Training Accuracy LC: 0.996499, Validation Accuracy LC: 0.979100, \n",
      "             \t Training Loss: 0.018631, Validation Loss: 0.066807, Training Accuracy: 0.996499, Validation Accuracy: 0.979100\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.035658, Validation Loss DLoRALC: 0.082807, Training Accuracy DLoRALC: 0.989498, Validation Accuracy DLoRALC: 0.974500 \n",
      "             \t Training Loss LC: 0.017597, Validation Loss LC: 0.065304, Training Accuracy LC: 0.997333, Validation Accuracy LC: 0.979900, \n",
      "             \t Training Loss: 0.017597, Validation Loss: 0.065303, Training Accuracy: 0.997333, Validation Accuracy: 0.979900\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.031020, Validation Loss DLoRALC: 0.073938, Training Accuracy DLoRALC: 0.991832, Validation Accuracy DLoRALC: 0.977400 \n",
      "             \t Training Loss LC: 0.015512, Validation Loss LC: 0.064668, Training Accuracy LC: 0.998166, Validation Accuracy LC: 0.980500, \n",
      "             \t Training Loss: 0.015512, Validation Loss: 0.064669, Training Accuracy: 0.998166, Validation Accuracy: 0.980500\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.029409, Validation Loss DLoRALC: 0.083035, Training Accuracy DLoRALC: 0.992999, Validation Accuracy DLoRALC: 0.974800 \n",
      "             \t Training Loss LC: 0.013748, Validation Loss LC: 0.064816, Training Accuracy LC: 0.998500, Validation Accuracy LC: 0.979700, \n",
      "             \t Training Loss: 0.013749, Validation Loss: 0.064815, Training Accuracy: 0.998500, Validation Accuracy: 0.979700\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.025083, Validation Loss DLoRALC: 0.092131, Training Accuracy DLoRALC: 0.993832, Validation Accuracy DLoRALC: 0.972700 \n",
      "             \t Training Loss LC: 0.012333, Validation Loss LC: 0.068696, Training Accuracy LC: 0.999167, Validation Accuracy LC: 0.979100, \n",
      "             \t Training Loss: 0.012333, Validation Loss: 0.068698, Training Accuracy: 0.999167, Validation Accuracy: 0.979100\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.022428, Validation Loss DLoRALC: 0.075754, Training Accuracy DLoRALC: 0.995166, Validation Accuracy DLoRALC: 0.977300 \n",
      "             \t Training Loss LC: 0.011104, Validation Loss LC: 0.065301, Training Accuracy LC: 0.999500, Validation Accuracy LC: 0.979900, \n",
      "             \t Training Loss: 0.011104, Validation Loss: 0.065301, Training Accuracy: 0.999500, Validation Accuracy: 0.979900\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.021795, Validation Loss DLoRALC: 0.079028, Training Accuracy DLoRALC: 0.994666, Validation Accuracy DLoRALC: 0.976100 \n",
      "             \t Training Loss LC: 0.010815, Validation Loss LC: 0.063500, Training Accuracy LC: 0.999333, Validation Accuracy LC: 0.979700, \n",
      "             \t Training Loss: 0.010815, Validation Loss: 0.063501, Training Accuracy: 0.999333, Validation Accuracy: 0.979700\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.018118, Validation Loss DLoRALC: 0.069154, Training Accuracy DLoRALC: 0.997166, Validation Accuracy DLoRALC: 0.979100 \n",
      "             \t Training Loss LC: 0.009508, Validation Loss LC: 0.063543, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.981400, \n",
      "             \t Training Loss: 0.009508, Validation Loss: 0.063544, Training Accuracy: 1.000000, Validation Accuracy: 0.981400\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.015507, Validation Loss DLoRALC: 0.071384, Training Accuracy DLoRALC: 0.997666, Validation Accuracy DLoRALC: 0.978500 \n",
      "             \t Training Loss LC: 0.008551, Validation Loss LC: 0.064100, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.980600, \n",
      "             \t Training Loss: 0.008551, Validation Loss: 0.064101, Training Accuracy: 0.999667, Validation Accuracy: 0.980600\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.015423, Validation Loss DLoRALC: 0.069566, Training Accuracy DLoRALC: 0.997333, Validation Accuracy DLoRALC: 0.978900 \n",
      "             \t Training Loss LC: 0.007929, Validation Loss LC: 0.063215, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.980800, \n",
      "             \t Training Loss: 0.007930, Validation Loss: 0.063215, Training Accuracy: 1.000000, Validation Accuracy: 0.980800\n",
      "End of model training on dataloader3...\n",
      "Model saved at accuracy: 0.9789\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader4...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.085110, Validation Loss DLoRALC: 0.076405, Training Accuracy DLoRALC: 0.973004, Validation Accuracy DLoRALC: 0.976400 \n",
      "             \t Training Loss LC: 0.078284, Validation Loss LC: 0.069398, Training Accuracy LC: 0.973504, Validation Accuracy LC: 0.979000, \n",
      "             \t Training Loss: 0.078286, Validation Loss: 0.069397, Training Accuracy: 0.973504, Validation Accuracy: 0.979000\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.055138, Validation Loss DLoRALC: 0.075056, Training Accuracy DLoRALC: 0.981836, Validation Accuracy DLoRALC: 0.977600 \n",
      "             \t Training Loss LC: 0.046041, Validation Loss LC: 0.064998, Training Accuracy LC: 0.986002, Validation Accuracy LC: 0.981000, \n",
      "             \t Training Loss: 0.046042, Validation Loss: 0.064998, Training Accuracy: 0.986002, Validation Accuracy: 0.981000\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.043681, Validation Loss DLoRALC: 0.078293, Training Accuracy DLoRALC: 0.984336, Validation Accuracy DLoRALC: 0.977000 \n",
      "             \t Training Loss LC: 0.034575, Validation Loss LC: 0.070385, Training Accuracy LC: 0.989502, Validation Accuracy LC: 0.978300, \n",
      "             \t Training Loss: 0.034574, Validation Loss: 0.070381, Training Accuracy: 0.989502, Validation Accuracy: 0.978300\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.035026, Validation Loss DLoRALC: 0.066952, Training Accuracy DLoRALC: 0.989168, Validation Accuracy DLoRALC: 0.979500 \n",
      "             \t Training Loss LC: 0.026160, Validation Loss LC: 0.061374, Training Accuracy LC: 0.992335, Validation Accuracy LC: 0.982000, \n",
      "             \t Training Loss: 0.026160, Validation Loss: 0.061372, Training Accuracy: 0.992335, Validation Accuracy: 0.982000\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.028620, Validation Loss DLoRALC: 0.066127, Training Accuracy DLoRALC: 0.992335, Validation Accuracy DLoRALC: 0.980700 \n",
      "             \t Training Loss LC: 0.020332, Validation Loss LC: 0.065741, Training Accuracy LC: 0.994668, Validation Accuracy LC: 0.979400, \n",
      "             \t Training Loss: 0.020333, Validation Loss: 0.065742, Training Accuracy: 0.994668, Validation Accuracy: 0.979400\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.026084, Validation Loss DLoRALC: 0.071051, Training Accuracy DLoRALC: 0.992501, Validation Accuracy DLoRALC: 0.979800 \n",
      "             \t Training Loss LC: 0.016061, Validation Loss LC: 0.061713, Training Accuracy LC: 0.996834, Validation Accuracy LC: 0.981300, \n",
      "             \t Training Loss: 0.016061, Validation Loss: 0.061712, Training Accuracy: 0.996834, Validation Accuracy: 0.981300\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.021497, Validation Loss DLoRALC: 0.065413, Training Accuracy DLoRALC: 0.994168, Validation Accuracy DLoRALC: 0.981100 \n",
      "             \t Training Loss LC: 0.014156, Validation Loss LC: 0.056359, Training Accuracy LC: 0.997334, Validation Accuracy LC: 0.982900, \n",
      "             \t Training Loss: 0.014155, Validation Loss: 0.056359, Training Accuracy: 0.997334, Validation Accuracy: 0.982900\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.018593, Validation Loss DLoRALC: 0.111355, Training Accuracy DLoRALC: 0.995501, Validation Accuracy DLoRALC: 0.967400 \n",
      "             \t Training Loss LC: 0.011739, Validation Loss LC: 0.059332, Training Accuracy LC: 0.998834, Validation Accuracy LC: 0.982900, \n",
      "             \t Training Loss: 0.011739, Validation Loss: 0.059328, Training Accuracy: 0.998834, Validation Accuracy: 0.982900\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.016935, Validation Loss DLoRALC: 0.063680, Training Accuracy DLoRALC: 0.995834, Validation Accuracy DLoRALC: 0.981300 \n",
      "             \t Training Loss LC: 0.010500, Validation Loss LC: 0.056800, Training Accuracy LC: 0.998500, Validation Accuracy LC: 0.983400, \n",
      "             \t Training Loss: 0.010499, Validation Loss: 0.056800, Training Accuracy: 0.998500, Validation Accuracy: 0.983400\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.015112, Validation Loss DLoRALC: 0.060167, Training Accuracy DLoRALC: 0.996834, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.009159, Validation Loss LC: 0.060006, Training Accuracy LC: 0.999000, Validation Accuracy LC: 0.982900, \n",
      "             \t Training Loss: 0.009158, Validation Loss: 0.060008, Training Accuracy: 0.999000, Validation Accuracy: 0.982900\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.012621, Validation Loss DLoRALC: 0.061446, Training Accuracy DLoRALC: 0.997834, Validation Accuracy DLoRALC: 0.982300 \n",
      "             \t Training Loss LC: 0.007661, Validation Loss LC: 0.056890, Training Accuracy LC: 0.999500, Validation Accuracy LC: 0.983600, \n",
      "             \t Training Loss: 0.007661, Validation Loss: 0.056889, Training Accuracy: 0.999500, Validation Accuracy: 0.983600\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.010321, Validation Loss DLoRALC: 0.062362, Training Accuracy DLoRALC: 0.999333, Validation Accuracy DLoRALC: 0.980900 \n",
      "             \t Training Loss LC: 0.006738, Validation Loss LC: 0.056534, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984100, \n",
      "             \t Training Loss: 0.006738, Validation Loss: 0.056533, Training Accuracy: 0.999833, Validation Accuracy: 0.984100\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.009026, Validation Loss DLoRALC: 0.063805, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.982200 \n",
      "             \t Training Loss LC: 0.006339, Validation Loss LC: 0.056348, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984100, \n",
      "             \t Training Loss: 0.006340, Validation Loss: 0.056349, Training Accuracy: 1.000000, Validation Accuracy: 0.984100\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.008297, Validation Loss DLoRALC: 0.062880, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.005738, Validation Loss LC: 0.058473, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.983400, \n",
      "             \t Training Loss: 0.005738, Validation Loss: 0.058472, Training Accuracy: 1.000000, Validation Accuracy: 0.983400\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.007820, Validation Loss DLoRALC: 0.064491, Training Accuracy DLoRALC: 0.999333, Validation Accuracy DLoRALC: 0.981900 \n",
      "             \t Training Loss LC: 0.005255, Validation Loss LC: 0.056328, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984400, \n",
      "             \t Training Loss: 0.005255, Validation Loss: 0.056329, Training Accuracy: 1.000000, Validation Accuracy: 0.984400\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.006665, Validation Loss DLoRALC: 0.061225, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.983000 \n",
      "             \t Training Loss LC: 0.004844, Validation Loss LC: 0.055972, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.983800, \n",
      "             \t Training Loss: 0.004844, Validation Loss: 0.055971, Training Accuracy: 1.000000, Validation Accuracy: 0.983800\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.006672, Validation Loss DLoRALC: 0.062576, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.981100 \n",
      "             \t Training Loss LC: 0.004597, Validation Loss LC: 0.056285, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.004597, Validation Loss: 0.056285, Training Accuracy: 1.000000, Validation Accuracy: 0.984500\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.006166, Validation Loss DLoRALC: 0.062410, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.981500 \n",
      "             \t Training Loss LC: 0.004291, Validation Loss LC: 0.055367, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984200, \n",
      "             \t Training Loss: 0.004291, Validation Loss: 0.055368, Training Accuracy: 1.000000, Validation Accuracy: 0.984200\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.006050, Validation Loss DLoRALC: 0.062556, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.982000 \n",
      "             \t Training Loss LC: 0.004035, Validation Loss LC: 0.056417, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.004035, Validation Loss: 0.056418, Training Accuracy: 1.000000, Validation Accuracy: 0.984600\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.005405, Validation Loss DLoRALC: 0.060801, Training Accuracy DLoRALC: 1.000000, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.003886, Validation Loss LC: 0.056242, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.983900, \n",
      "             \t Training Loss: 0.003886, Validation Loss: 0.056243, Training Accuracy: 1.000000, Validation Accuracy: 0.983900\n",
      "End of model training on dataloader4...\n",
      "Model saved at accuracy: 0.9829\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader5...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.106516, Validation Loss DLoRALC: 0.119342, Training Accuracy DLoRALC: 0.969172, Validation Accuracy DLoRALC: 0.968300 \n",
      "             \t Training Loss LC: 0.093886, Validation Loss LC: 0.084503, Training Accuracy LC: 0.974504, Validation Accuracy LC: 0.976000, \n",
      "             \t Training Loss: 0.093887, Validation Loss: 0.084502, Training Accuracy: 0.974504, Validation Accuracy: 0.976000\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.072155, Validation Loss DLoRALC: 0.108547, Training Accuracy DLoRALC: 0.979003, Validation Accuracy DLoRALC: 0.970700 \n",
      "             \t Training Loss LC: 0.057308, Validation Loss LC: 0.065598, Training Accuracy LC: 0.981170, Validation Accuracy LC: 0.980200, \n",
      "             \t Training Loss: 0.057308, Validation Loss: 0.065595, Training Accuracy: 0.981170, Validation Accuracy: 0.980300\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.056349, Validation Loss DLoRALC: 0.072279, Training Accuracy DLoRALC: 0.983169, Validation Accuracy DLoRALC: 0.979300 \n",
      "             \t Training Loss LC: 0.039087, Validation Loss LC: 0.052945, Training Accuracy LC: 0.986836, Validation Accuracy LC: 0.984000, \n",
      "             \t Training Loss: 0.039087, Validation Loss: 0.052943, Training Accuracy: 0.986836, Validation Accuracy: 0.984000\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.046778, Validation Loss DLoRALC: 0.075115, Training Accuracy DLoRALC: 0.985169, Validation Accuracy DLoRALC: 0.978100 \n",
      "             \t Training Loss LC: 0.030472, Validation Loss LC: 0.063163, Training Accuracy LC: 0.989835, Validation Accuracy LC: 0.981300, \n",
      "             \t Training Loss: 0.030473, Validation Loss: 0.063165, Training Accuracy: 0.989835, Validation Accuracy: 0.981300\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.039583, Validation Loss DLoRALC: 0.082064, Training Accuracy DLoRALC: 0.986169, Validation Accuracy DLoRALC: 0.977500 \n",
      "             \t Training Loss LC: 0.023643, Validation Loss LC: 0.059413, Training Accuracy LC: 0.992835, Validation Accuracy LC: 0.982100, \n",
      "             \t Training Loss: 0.023642, Validation Loss: 0.059411, Training Accuracy: 0.992835, Validation Accuracy: 0.982100\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.032918, Validation Loss DLoRALC: 0.065783, Training Accuracy DLoRALC: 0.990335, Validation Accuracy DLoRALC: 0.978700 \n",
      "             \t Training Loss LC: 0.018600, Validation Loss LC: 0.052879, Training Accuracy LC: 0.995501, Validation Accuracy LC: 0.983100, \n",
      "             \t Training Loss: 0.018599, Validation Loss: 0.052878, Training Accuracy: 0.995501, Validation Accuracy: 0.983100\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.029537, Validation Loss DLoRALC: 0.063172, Training Accuracy DLoRALC: 0.990668, Validation Accuracy DLoRALC: 0.981400 \n",
      "             \t Training Loss LC: 0.015947, Validation Loss LC: 0.054511, Training Accuracy LC: 0.996667, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.015946, Validation Loss: 0.054510, Training Accuracy: 0.996667, Validation Accuracy: 0.984300\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.025855, Validation Loss DLoRALC: 0.073604, Training Accuracy DLoRALC: 0.992335, Validation Accuracy DLoRALC: 0.979100 \n",
      "             \t Training Loss LC: 0.013350, Validation Loss LC: 0.055163, Training Accuracy LC: 0.997000, Validation Accuracy LC: 0.983700, \n",
      "             \t Training Loss: 0.013350, Validation Loss: 0.055162, Training Accuracy: 0.997000, Validation Accuracy: 0.983700\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.022289, Validation Loss DLoRALC: 0.064985, Training Accuracy DLoRALC: 0.993501, Validation Accuracy DLoRALC: 0.981900 \n",
      "             \t Training Loss LC: 0.010928, Validation Loss LC: 0.054172, Training Accuracy LC: 0.998167, Validation Accuracy LC: 0.983900, \n",
      "             \t Training Loss: 0.010928, Validation Loss: 0.054172, Training Accuracy: 0.998167, Validation Accuracy: 0.983900\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.022654, Validation Loss DLoRALC: 0.078233, Training Accuracy DLoRALC: 0.993668, Validation Accuracy DLoRALC: 0.976600 \n",
      "             \t Training Loss LC: 0.010297, Validation Loss LC: 0.059320, Training Accuracy LC: 0.998167, Validation Accuracy LC: 0.981500, \n",
      "             \t Training Loss: 0.010297, Validation Loss: 0.059323, Training Accuracy: 0.998167, Validation Accuracy: 0.981500\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.017359, Validation Loss DLoRALC: 0.067251, Training Accuracy DLoRALC: 0.994334, Validation Accuracy DLoRALC: 0.981100 \n",
      "             \t Training Loss LC: 0.008429, Validation Loss LC: 0.056398, Training Accuracy LC: 0.999167, Validation Accuracy LC: 0.983400, \n",
      "             \t Training Loss: 0.008430, Validation Loss: 0.056398, Training Accuracy: 0.999167, Validation Accuracy: 0.983400\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.016312, Validation Loss DLoRALC: 0.060776, Training Accuracy DLoRALC: 0.996167, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.007931, Validation Loss LC: 0.056272, Training Accuracy LC: 0.999167, Validation Accuracy LC: 0.982500, \n",
      "             \t Training Loss: 0.007931, Validation Loss: 0.056272, Training Accuracy: 0.999167, Validation Accuracy: 0.982500\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.015203, Validation Loss DLoRALC: 0.057117, Training Accuracy DLoRALC: 0.997167, Validation Accuracy DLoRALC: 0.983200 \n",
      "             \t Training Loss LC: 0.007799, Validation Loss LC: 0.053950, Training Accuracy LC: 0.998667, Validation Accuracy LC: 0.983800, \n",
      "             \t Training Loss: 0.007800, Validation Loss: 0.053950, Training Accuracy: 0.998667, Validation Accuracy: 0.983800\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.012452, Validation Loss DLoRALC: 0.064743, Training Accuracy DLoRALC: 0.997834, Validation Accuracy DLoRALC: 0.981400 \n",
      "             \t Training Loss LC: 0.006450, Validation Loss LC: 0.054646, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.983300, \n",
      "             \t Training Loss: 0.006450, Validation Loss: 0.054646, Training Accuracy: 0.999667, Validation Accuracy: 0.983300\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.010739, Validation Loss DLoRALC: 0.062936, Training Accuracy DLoRALC: 0.998000, Validation Accuracy DLoRALC: 0.981500 \n",
      "             \t Training Loss LC: 0.005959, Validation Loss LC: 0.055373, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.005959, Validation Loss: 0.055374, Training Accuracy: 0.999667, Validation Accuracy: 0.984300\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.009406, Validation Loss DLoRALC: 0.068722, Training Accuracy DLoRALC: 0.998500, Validation Accuracy DLoRALC: 0.980000 \n",
      "             \t Training Loss LC: 0.005215, Validation Loss LC: 0.057239, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.983300, \n",
      "             \t Training Loss: 0.005215, Validation Loss: 0.057241, Training Accuracy: 0.999667, Validation Accuracy: 0.983300\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.008538, Validation Loss DLoRALC: 0.060863, Training Accuracy DLoRALC: 0.998834, Validation Accuracy DLoRALC: 0.982200 \n",
      "             \t Training Loss LC: 0.004697, Validation Loss LC: 0.054645, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984200, \n",
      "             \t Training Loss: 0.004697, Validation Loss: 0.054646, Training Accuracy: 0.999833, Validation Accuracy: 0.984200\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.009525, Validation Loss DLoRALC: 0.063213, Training Accuracy DLoRALC: 0.998167, Validation Accuracy DLoRALC: 0.981600 \n",
      "             \t Training Loss LC: 0.004603, Validation Loss LC: 0.054912, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.983500, \n",
      "             \t Training Loss: 0.004603, Validation Loss: 0.054913, Training Accuracy: 0.999833, Validation Accuracy: 0.983500\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.008922, Validation Loss DLoRALC: 0.061024, Training Accuracy DLoRALC: 0.997667, Validation Accuracy DLoRALC: 0.983400 \n",
      "             \t Training Loss LC: 0.004450, Validation Loss LC: 0.054515, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.983800, \n",
      "             \t Training Loss: 0.004451, Validation Loss: 0.054515, Training Accuracy: 0.999667, Validation Accuracy: 0.983900\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.006883, Validation Loss DLoRALC: 0.064213, Training Accuracy DLoRALC: 0.999167, Validation Accuracy DLoRALC: 0.981000 \n",
      "             \t Training Loss LC: 0.003865, Validation Loss LC: 0.054314, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984200, \n",
      "             \t Training Loss: 0.003865, Validation Loss: 0.054314, Training Accuracy: 1.000000, Validation Accuracy: 0.984200\n",
      "End of model training on dataloader5...\n",
      "Model saved at accuracy: 0.9810\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader6...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.100167, Validation Loss DLoRALC: 0.096053, Training Accuracy DLoRALC: 0.974325, Validation Accuracy DLoRALC: 0.971000 \n",
      "             \t Training Loss LC: 0.075276, Validation Loss LC: 0.059601, Training Accuracy LC: 0.978993, Validation Accuracy LC: 0.982500, \n",
      "             \t Training Loss: 0.075276, Validation Loss: 0.059606, Training Accuracy: 0.978993, Validation Accuracy: 0.982500\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.068277, Validation Loss DLoRALC: 0.062036, Training Accuracy DLoRALC: 0.977326, Validation Accuracy DLoRALC: 0.980500 \n",
      "             \t Training Loss LC: 0.045718, Validation Loss LC: 0.054761, Training Accuracy LC: 0.986495, Validation Accuracy LC: 0.982400, \n",
      "             \t Training Loss: 0.045716, Validation Loss: 0.054763, Training Accuracy: 0.986495, Validation Accuracy: 0.982400\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.053470, Validation Loss DLoRALC: 0.067123, Training Accuracy DLoRALC: 0.982494, Validation Accuracy DLoRALC: 0.980200 \n",
      "             \t Training Loss LC: 0.029569, Validation Loss LC: 0.057202, Training Accuracy LC: 0.990997, Validation Accuracy LC: 0.981900, \n",
      "             \t Training Loss: 0.029571, Validation Loss: 0.057207, Training Accuracy: 0.990997, Validation Accuracy: 0.981900\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.038022, Validation Loss DLoRALC: 0.062667, Training Accuracy DLoRALC: 0.988496, Validation Accuracy DLoRALC: 0.980300 \n",
      "             \t Training Loss LC: 0.020428, Validation Loss LC: 0.055301, Training Accuracy LC: 0.994665, Validation Accuracy LC: 0.983000, \n",
      "             \t Training Loss: 0.020428, Validation Loss: 0.055303, Training Accuracy: 0.994665, Validation Accuracy: 0.983000\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.030036, Validation Loss DLoRALC: 0.057506, Training Accuracy DLoRALC: 0.990163, Validation Accuracy DLoRALC: 0.982400 \n",
      "             \t Training Loss LC: 0.014683, Validation Loss LC: 0.055055, Training Accuracy LC: 0.995999, Validation Accuracy LC: 0.983500, \n",
      "             \t Training Loss: 0.014683, Validation Loss: 0.055053, Training Accuracy: 0.995999, Validation Accuracy: 0.983500\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.022306, Validation Loss DLoRALC: 0.056890, Training Accuracy DLoRALC: 0.993831, Validation Accuracy DLoRALC: 0.982000 \n",
      "             \t Training Loss LC: 0.011392, Validation Loss LC: 0.055378, Training Accuracy LC: 0.997666, Validation Accuracy LC: 0.983200, \n",
      "             \t Training Loss: 0.011392, Validation Loss: 0.055377, Training Accuracy: 0.997666, Validation Accuracy: 0.983200\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.019407, Validation Loss DLoRALC: 0.056222, Training Accuracy DLoRALC: 0.995165, Validation Accuracy DLoRALC: 0.982800 \n",
      "             \t Training Loss LC: 0.009188, Validation Loss LC: 0.053149, Training Accuracy LC: 0.998833, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.009187, Validation Loss: 0.053149, Training Accuracy: 0.998833, Validation Accuracy: 0.984600\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.017588, Validation Loss DLoRALC: 0.059325, Training Accuracy DLoRALC: 0.995498, Validation Accuracy DLoRALC: 0.980700 \n",
      "             \t Training Loss LC: 0.007903, Validation Loss LC: 0.053757, Training Accuracy LC: 0.999166, Validation Accuracy LC: 0.982900, \n",
      "             \t Training Loss: 0.007903, Validation Loss: 0.053758, Training Accuracy: 0.999166, Validation Accuracy: 0.982900\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.015952, Validation Loss DLoRALC: 0.056401, Training Accuracy DLoRALC: 0.996332, Validation Accuracy DLoRALC: 0.982200 \n",
      "             \t Training Loss LC: 0.006496, Validation Loss LC: 0.053204, Training Accuracy LC: 0.999500, Validation Accuracy LC: 0.983600, \n",
      "             \t Training Loss: 0.006497, Validation Loss: 0.053205, Training Accuracy: 0.999500, Validation Accuracy: 0.983600\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.012937, Validation Loss DLoRALC: 0.058284, Training Accuracy DLoRALC: 0.997332, Validation Accuracy DLoRALC: 0.981800 \n",
      "             \t Training Loss LC: 0.005537, Validation Loss LC: 0.053259, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.005537, Validation Loss: 0.053260, Training Accuracy: 0.999667, Validation Accuracy: 0.984300\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.011055, Validation Loss DLoRALC: 0.056697, Training Accuracy DLoRALC: 0.997833, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.004776, Validation Loss LC: 0.052428, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.983800, \n",
      "             \t Training Loss: 0.004777, Validation Loss: 0.052429, Training Accuracy: 0.999833, Validation Accuracy: 0.983800\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.009512, Validation Loss DLoRALC: 0.054466, Training Accuracy DLoRALC: 0.998333, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.004550, Validation Loss LC: 0.052000, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984400, \n",
      "             \t Training Loss: 0.004550, Validation Loss: 0.052000, Training Accuracy: 0.999833, Validation Accuracy: 0.984400\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.008806, Validation Loss DLoRALC: 0.055139, Training Accuracy DLoRALC: 0.998166, Validation Accuracy DLoRALC: 0.983200 \n",
      "             \t Training Loss LC: 0.004122, Validation Loss LC: 0.052913, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.004122, Validation Loss: 0.052916, Training Accuracy: 1.000000, Validation Accuracy: 0.984600\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.007535, Validation Loss DLoRALC: 0.055485, Training Accuracy DLoRALC: 0.998666, Validation Accuracy DLoRALC: 0.982800 \n",
      "             \t Training Loss LC: 0.003819, Validation Loss LC: 0.052513, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984400, \n",
      "             \t Training Loss: 0.003819, Validation Loss: 0.052516, Training Accuracy: 1.000000, Validation Accuracy: 0.984400\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.006334, Validation Loss DLoRALC: 0.055148, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.982400 \n",
      "             \t Training Loss LC: 0.003484, Validation Loss LC: 0.052270, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.003484, Validation Loss: 0.052269, Training Accuracy: 1.000000, Validation Accuracy: 0.984500\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.005845, Validation Loss DLoRALC: 0.052212, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.984100 \n",
      "             \t Training Loss LC: 0.003356, Validation Loss LC: 0.051716, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984000, \n",
      "             \t Training Loss: 0.003356, Validation Loss: 0.051716, Training Accuracy: 1.000000, Validation Accuracy: 0.984000\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.005049, Validation Loss DLoRALC: 0.051472, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.984000 \n",
      "             \t Training Loss LC: 0.003093, Validation Loss LC: 0.051591, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.003094, Validation Loss: 0.051593, Training Accuracy: 1.000000, Validation Accuracy: 0.984600\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.004760, Validation Loss DLoRALC: 0.052533, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.982800 \n",
      "             \t Training Loss LC: 0.002980, Validation Loss LC: 0.051506, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.002981, Validation Loss: 0.051506, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.004579, Validation Loss DLoRALC: 0.053712, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.002848, Validation Loss LC: 0.051862, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.002848, Validation Loss: 0.051862, Training Accuracy: 1.000000, Validation Accuracy: 0.984600\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.004132, Validation Loss DLoRALC: 0.053710, Training Accuracy DLoRALC: 1.000000, Validation Accuracy DLoRALC: 0.983200 \n",
      "             \t Training Loss LC: 0.002655, Validation Loss LC: 0.051665, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.002655, Validation Loss: 0.051666, Training Accuracy: 1.000000, Validation Accuracy: 0.984500\n",
      "End of model training on dataloader6...\n",
      "Model saved at accuracy: 0.9832\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader7...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.103722, Validation Loss DLoRALC: 0.070008, Training Accuracy DLoRALC: 0.972500, Validation Accuracy DLoRALC: 0.978600 \n",
      "             \t Training Loss LC: 0.082696, Validation Loss LC: 0.071291, Training Accuracy LC: 0.976000, Validation Accuracy LC: 0.979000, \n",
      "             \t Training Loss: 0.082698, Validation Loss: 0.071297, Training Accuracy: 0.976000, Validation Accuracy: 0.979000\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.067135, Validation Loss DLoRALC: 0.054126, Training Accuracy DLoRALC: 0.980000, Validation Accuracy DLoRALC: 0.982500 \n",
      "             \t Training Loss LC: 0.047456, Validation Loss LC: 0.076557, Training Accuracy LC: 0.985333, Validation Accuracy LC: 0.977600, \n",
      "             \t Training Loss: 0.047453, Validation Loss: 0.076564, Training Accuracy: 0.985333, Validation Accuracy: 0.977600\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.052434, Validation Loss DLoRALC: 0.060667, Training Accuracy DLoRALC: 0.985333, Validation Accuracy DLoRALC: 0.981200 \n",
      "             \t Training Loss LC: 0.030000, Validation Loss LC: 0.061243, Training Accuracy LC: 0.991000, Validation Accuracy LC: 0.982800, \n",
      "             \t Training Loss: 0.030000, Validation Loss: 0.061237, Training Accuracy: 0.991000, Validation Accuracy: 0.982800\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.042625, Validation Loss DLoRALC: 0.056235, Training Accuracy DLoRALC: 0.987500, Validation Accuracy DLoRALC: 0.982500 \n",
      "             \t Training Loss LC: 0.022896, Validation Loss LC: 0.052621, Training Accuracy LC: 0.993000, Validation Accuracy LC: 0.984100, \n",
      "             \t Training Loss: 0.022894, Validation Loss: 0.052617, Training Accuracy: 0.993000, Validation Accuracy: 0.984100\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.033064, Validation Loss DLoRALC: 0.059689, Training Accuracy DLoRALC: 0.990333, Validation Accuracy DLoRALC: 0.981600 \n",
      "             \t Training Loss LC: 0.016440, Validation Loss LC: 0.054612, Training Accuracy LC: 0.996333, Validation Accuracy LC: 0.983600, \n",
      "             \t Training Loss: 0.016441, Validation Loss: 0.054612, Training Accuracy: 0.996333, Validation Accuracy: 0.983600\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.027277, Validation Loss DLoRALC: 0.057898, Training Accuracy DLoRALC: 0.992667, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.011660, Validation Loss LC: 0.056682, Training Accuracy LC: 0.997667, Validation Accuracy LC: 0.983300, \n",
      "             \t Training Loss: 0.011659, Validation Loss: 0.056685, Training Accuracy: 0.997667, Validation Accuracy: 0.983400\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.021994, Validation Loss DLoRALC: 0.055778, Training Accuracy DLoRALC: 0.994500, Validation Accuracy DLoRALC: 0.983500 \n",
      "             \t Training Loss LC: 0.010553, Validation Loss LC: 0.054822, Training Accuracy LC: 0.997833, Validation Accuracy LC: 0.982900, \n",
      "             \t Training Loss: 0.010552, Validation Loss: 0.054825, Training Accuracy: 0.997833, Validation Accuracy: 0.982900\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.021056, Validation Loss DLoRALC: 0.057398, Training Accuracy DLoRALC: 0.994167, Validation Accuracy DLoRALC: 0.983000 \n",
      "             \t Training Loss LC: 0.007626, Validation Loss LC: 0.054115, Training Accuracy LC: 0.999167, Validation Accuracy LC: 0.983400, \n",
      "             \t Training Loss: 0.007626, Validation Loss: 0.054114, Training Accuracy: 0.999167, Validation Accuracy: 0.983400\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.018358, Validation Loss DLoRALC: 0.079134, Training Accuracy DLoRALC: 0.994167, Validation Accuracy DLoRALC: 0.975300 \n",
      "             \t Training Loss LC: 0.006113, Validation Loss LC: 0.057860, Training Accuracy LC: 0.999333, Validation Accuracy LC: 0.982200, \n",
      "             \t Training Loss: 0.006113, Validation Loss: 0.057861, Training Accuracy: 0.999333, Validation Accuracy: 0.982200\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.015354, Validation Loss DLoRALC: 0.055478, Training Accuracy DLoRALC: 0.996667, Validation Accuracy DLoRALC: 0.983700 \n",
      "             \t Training Loss LC: 0.005772, Validation Loss LC: 0.052125, Training Accuracy LC: 0.999167, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.005772, Validation Loss: 0.052126, Training Accuracy: 0.999167, Validation Accuracy: 0.984500\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.013566, Validation Loss DLoRALC: 0.058651, Training Accuracy DLoRALC: 0.996667, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.004692, Validation Loss LC: 0.053134, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984200, \n",
      "             \t Training Loss: 0.004692, Validation Loss: 0.053135, Training Accuracy: 0.999833, Validation Accuracy: 0.984200\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.011709, Validation Loss DLoRALC: 0.052885, Training Accuracy DLoRALC: 0.997500, Validation Accuracy DLoRALC: 0.984300 \n",
      "             \t Training Loss LC: 0.004477, Validation Loss LC: 0.053294, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.004477, Validation Loss: 0.053296, Training Accuracy: 0.999833, Validation Accuracy: 0.984300\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.009866, Validation Loss DLoRALC: 0.053955, Training Accuracy DLoRALC: 0.998167, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.004029, Validation Loss LC: 0.052843, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.004029, Validation Loss: 0.052845, Training Accuracy: 0.999833, Validation Accuracy: 0.984600\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.008208, Validation Loss DLoRALC: 0.061049, Training Accuracy DLoRALC: 0.999000, Validation Accuracy DLoRALC: 0.982600 \n",
      "             \t Training Loss LC: 0.003672, Validation Loss LC: 0.054702, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.983500, \n",
      "             \t Training Loss: 0.003672, Validation Loss: 0.054703, Training Accuracy: 1.000000, Validation Accuracy: 0.983500\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.008235, Validation Loss DLoRALC: 0.063218, Training Accuracy DLoRALC: 0.998500, Validation Accuracy DLoRALC: 0.981500 \n",
      "             \t Training Loss LC: 0.003520, Validation Loss LC: 0.053473, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.983700, \n",
      "             \t Training Loss: 0.003520, Validation Loss: 0.053473, Training Accuracy: 0.999833, Validation Accuracy: 0.983700\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.007031, Validation Loss DLoRALC: 0.053329, Training Accuracy DLoRALC: 0.999167, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.003180, Validation Loss LC: 0.052475, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.003180, Validation Loss: 0.052476, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.006261, Validation Loss DLoRALC: 0.055571, Training Accuracy DLoRALC: 0.999333, Validation Accuracy DLoRALC: 0.984600 \n",
      "             \t Training Loss LC: 0.002988, Validation Loss LC: 0.052947, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.002988, Validation Loss: 0.052948, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.006019, Validation Loss DLoRALC: 0.057994, Training Accuracy DLoRALC: 0.999167, Validation Accuracy DLoRALC: 0.983900 \n",
      "             \t Training Loss LC: 0.002866, Validation Loss LC: 0.052780, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.983800, \n",
      "             \t Training Loss: 0.002866, Validation Loss: 0.052781, Training Accuracy: 0.999833, Validation Accuracy: 0.983800\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.005364, Validation Loss DLoRALC: 0.054948, Training Accuracy DLoRALC: 0.999500, Validation Accuracy DLoRALC: 0.984500 \n",
      "             \t Training Loss LC: 0.002690, Validation Loss LC: 0.052679, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984100, \n",
      "             \t Training Loss: 0.002690, Validation Loss: 0.052682, Training Accuracy: 1.000000, Validation Accuracy: 0.984100\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.004711, Validation Loss DLoRALC: 0.056682, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.002563, Validation Loss LC: 0.053683, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.983800, \n",
      "             \t Training Loss: 0.002563, Validation Loss: 0.053683, Training Accuracy: 1.000000, Validation Accuracy: 0.983800\n",
      "End of model training on dataloader7...\n",
      "Model saved at accuracy: 0.9844\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader8...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.091798, Validation Loss DLoRALC: 0.068874, Training Accuracy DLoRALC: 0.974000, Validation Accuracy DLoRALC: 0.979300 \n",
      "             \t Training Loss LC: 0.074652, Validation Loss LC: 0.067263, Training Accuracy LC: 0.976833, Validation Accuracy LC: 0.980200, \n",
      "             \t Training Loss: 0.074655, Validation Loss: 0.067259, Training Accuracy: 0.976833, Validation Accuracy: 0.980200\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.057441, Validation Loss DLoRALC: 0.083043, Training Accuracy DLoRALC: 0.982000, Validation Accuracy DLoRALC: 0.976500 \n",
      "             \t Training Loss LC: 0.040768, Validation Loss LC: 0.101758, Training Accuracy LC: 0.986833, Validation Accuracy LC: 0.971300, \n",
      "             \t Training Loss: 0.040764, Validation Loss: 0.101722, Training Accuracy: 0.986833, Validation Accuracy: 0.971300\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.045824, Validation Loss DLoRALC: 0.113904, Training Accuracy DLoRALC: 0.985667, Validation Accuracy DLoRALC: 0.965200 \n",
      "             \t Training Loss LC: 0.027193, Validation Loss LC: 0.071554, Training Accuracy LC: 0.990000, Validation Accuracy LC: 0.976800, \n",
      "             \t Training Loss: 0.027194, Validation Loss: 0.071570, Training Accuracy: 0.990000, Validation Accuracy: 0.976800\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.037110, Validation Loss DLoRALC: 0.074232, Training Accuracy DLoRALC: 0.987667, Validation Accuracy DLoRALC: 0.977200 \n",
      "             \t Training Loss LC: 0.019599, Validation Loss LC: 0.065509, Training Accuracy LC: 0.995000, Validation Accuracy LC: 0.980800, \n",
      "             \t Training Loss: 0.019602, Validation Loss: 0.065515, Training Accuracy: 0.995000, Validation Accuracy: 0.980800\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.026649, Validation Loss DLoRALC: 0.061031, Training Accuracy DLoRALC: 0.992167, Validation Accuracy DLoRALC: 0.980100 \n",
      "             \t Training Loss LC: 0.013278, Validation Loss LC: 0.054057, Training Accuracy LC: 0.996500, Validation Accuracy LC: 0.983500, \n",
      "             \t Training Loss: 0.013278, Validation Loss: 0.054060, Training Accuracy: 0.996500, Validation Accuracy: 0.983500\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.023082, Validation Loss DLoRALC: 0.053881, Training Accuracy DLoRALC: 0.993667, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.009571, Validation Loss LC: 0.055851, Training Accuracy LC: 0.998500, Validation Accuracy LC: 0.983300, \n",
      "             \t Training Loss: 0.009571, Validation Loss: 0.055851, Training Accuracy: 0.998500, Validation Accuracy: 0.983300\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.020831, Validation Loss DLoRALC: 0.058755, Training Accuracy DLoRALC: 0.994667, Validation Accuracy DLoRALC: 0.981100 \n",
      "             \t Training Loss LC: 0.007692, Validation Loss LC: 0.054556, Training Accuracy LC: 0.999000, Validation Accuracy LC: 0.983600, \n",
      "             \t Training Loss: 0.007691, Validation Loss: 0.054557, Training Accuracy: 0.999000, Validation Accuracy: 0.983600\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.018260, Validation Loss DLoRALC: 0.052751, Training Accuracy DLoRALC: 0.995167, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.008009, Validation Loss LC: 0.050955, Training Accuracy LC: 0.997667, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.008011, Validation Loss: 0.050955, Training Accuracy: 0.997667, Validation Accuracy: 0.984300\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.015014, Validation Loss DLoRALC: 0.050826, Training Accuracy DLoRALC: 0.996000, Validation Accuracy DLoRALC: 0.983800 \n",
      "             \t Training Loss LC: 0.005538, Validation Loss LC: 0.050324, Training Accuracy LC: 0.999167, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.005538, Validation Loss: 0.050322, Training Accuracy: 0.999167, Validation Accuracy: 0.984500\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.014954, Validation Loss DLoRALC: 0.056346, Training Accuracy DLoRALC: 0.996000, Validation Accuracy DLoRALC: 0.982000 \n",
      "             \t Training Loss LC: 0.004647, Validation Loss LC: 0.052041, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.983200, \n",
      "             \t Training Loss: 0.004647, Validation Loss: 0.052039, Training Accuracy: 0.999667, Validation Accuracy: 0.983200\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.011796, Validation Loss DLoRALC: 0.057739, Training Accuracy DLoRALC: 0.996333, Validation Accuracy DLoRALC: 0.982400 \n",
      "             \t Training Loss LC: 0.004172, Validation Loss LC: 0.050686, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.004172, Validation Loss: 0.050686, Training Accuracy: 0.999667, Validation Accuracy: 0.984300\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.009807, Validation Loss DLoRALC: 0.052660, Training Accuracy DLoRALC: 0.997500, Validation Accuracy DLoRALC: 0.983600 \n",
      "             \t Training Loss LC: 0.003555, Validation Loss LC: 0.050294, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.003554, Validation Loss: 0.050292, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.008421, Validation Loss DLoRALC: 0.051853, Training Accuracy DLoRALC: 0.998833, Validation Accuracy DLoRALC: 0.983800 \n",
      "             \t Training Loss LC: 0.003216, Validation Loss LC: 0.049301, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984900, \n",
      "             \t Training Loss: 0.003216, Validation Loss: 0.049303, Training Accuracy: 0.999833, Validation Accuracy: 0.984900\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.007473, Validation Loss DLoRALC: 0.056709, Training Accuracy DLoRALC: 0.998833, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.002888, Validation Loss LC: 0.051086, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984900, \n",
      "             \t Training Loss: 0.002888, Validation Loss: 0.051086, Training Accuracy: 1.000000, Validation Accuracy: 0.984900\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.006820, Validation Loss DLoRALC: 0.051796, Training Accuracy DLoRALC: 0.998667, Validation Accuracy DLoRALC: 0.983000 \n",
      "             \t Training Loss LC: 0.002829, Validation Loss LC: 0.050661, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984000, \n",
      "             \t Training Loss: 0.002829, Validation Loss: 0.050661, Training Accuracy: 1.000000, Validation Accuracy: 0.984000\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.005621, Validation Loss DLoRALC: 0.053603, Training Accuracy DLoRALC: 0.999000, Validation Accuracy DLoRALC: 0.984000 \n",
      "             \t Training Loss LC: 0.002526, Validation Loss LC: 0.049764, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.002526, Validation Loss: 0.049766, Training Accuracy: 1.000000, Validation Accuracy: 0.984500\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.005632, Validation Loss DLoRALC: 0.054235, Training Accuracy DLoRALC: 0.999000, Validation Accuracy DLoRALC: 0.983100 \n",
      "             \t Training Loss LC: 0.002419, Validation Loss LC: 0.049851, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984200, \n",
      "             \t Training Loss: 0.002419, Validation Loss: 0.049851, Training Accuracy: 1.000000, Validation Accuracy: 0.984200\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.004807, Validation Loss DLoRALC: 0.057863, Training Accuracy DLoRALC: 0.998833, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.002274, Validation Loss LC: 0.050069, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984800, \n",
      "             \t Training Loss: 0.002274, Validation Loss: 0.050067, Training Accuracy: 1.000000, Validation Accuracy: 0.984800\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.004437, Validation Loss DLoRALC: 0.052239, Training Accuracy DLoRALC: 0.999333, Validation Accuracy DLoRALC: 0.984200 \n",
      "             \t Training Loss LC: 0.002121, Validation Loss LC: 0.050104, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.002121, Validation Loss: 0.050104, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.004052, Validation Loss DLoRALC: 0.054230, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.983100 \n",
      "             \t Training Loss LC: 0.002079, Validation Loss LC: 0.050746, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984200, \n",
      "             \t Training Loss: 0.002079, Validation Loss: 0.050746, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "End of model training on dataloader8...\n",
      "Model saved at accuracy: 0.9831\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader9...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.087549, Validation Loss DLoRALC: 0.071605, Training Accuracy DLoRALC: 0.976329, Validation Accuracy DLoRALC: 0.979700 \n",
      "             \t Training Loss LC: 0.070055, Validation Loss LC: 0.069401, Training Accuracy LC: 0.981330, Validation Accuracy LC: 0.980600, \n",
      "             \t Training Loss: 0.070051, Validation Loss: 0.069405, Training Accuracy: 0.981330, Validation Accuracy: 0.980600\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.063341, Validation Loss DLoRALC: 0.066099, Training Accuracy DLoRALC: 0.978996, Validation Accuracy DLoRALC: 0.980700 \n",
      "             \t Training Loss LC: 0.040830, Validation Loss LC: 0.053284, Training Accuracy LC: 0.986331, Validation Accuracy LC: 0.983900, \n",
      "             \t Training Loss: 0.040827, Validation Loss: 0.053283, Training Accuracy: 0.986331, Validation Accuracy: 0.983900\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.043500, Validation Loss DLoRALC: 0.072313, Training Accuracy DLoRALC: 0.986331, Validation Accuracy DLoRALC: 0.977600 \n",
      "             \t Training Loss LC: 0.025017, Validation Loss LC: 0.060851, Training Accuracy LC: 0.991832, Validation Accuracy LC: 0.981800, \n",
      "             \t Training Loss: 0.025016, Validation Loss: 0.060855, Training Accuracy: 0.991832, Validation Accuracy: 0.981800\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.032890, Validation Loss DLoRALC: 0.056394, Training Accuracy DLoRALC: 0.988665, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.014095, Validation Loss LC: 0.049214, Training Accuracy LC: 0.996666, Validation Accuracy LC: 0.984000, \n",
      "             \t Training Loss: 0.014097, Validation Loss: 0.049217, Training Accuracy: 0.996666, Validation Accuracy: 0.984000\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.019135, Validation Loss DLoRALC: 0.052259, Training Accuracy DLoRALC: 0.993666, Validation Accuracy DLoRALC: 0.983800 \n",
      "             \t Training Loss LC: 0.011280, Validation Loss LC: 0.050882, Training Accuracy LC: 0.997333, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.011282, Validation Loss: 0.050888, Training Accuracy: 0.997333, Validation Accuracy: 0.984600\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.017632, Validation Loss DLoRALC: 0.053747, Training Accuracy DLoRALC: 0.995333, Validation Accuracy DLoRALC: 0.982700 \n",
      "             \t Training Loss LC: 0.007870, Validation Loss LC: 0.050083, Training Accuracy LC: 0.998333, Validation Accuracy LC: 0.984400, \n",
      "             \t Training Loss: 0.007874, Validation Loss: 0.050085, Training Accuracy: 0.998333, Validation Accuracy: 0.984400\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.013043, Validation Loss DLoRALC: 0.059730, Training Accuracy DLoRALC: 0.996666, Validation Accuracy DLoRALC: 0.981900 \n",
      "             \t Training Loss LC: 0.007807, Validation Loss LC: 0.054478, Training Accuracy LC: 0.998666, Validation Accuracy LC: 0.983100, \n",
      "             \t Training Loss: 0.007812, Validation Loss: 0.054469, Training Accuracy: 0.998666, Validation Accuracy: 0.983100\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.010208, Validation Loss DLoRALC: 0.055460, Training Accuracy DLoRALC: 0.998500, Validation Accuracy DLoRALC: 0.982500 \n",
      "             \t Training Loss LC: 0.006154, Validation Loss LC: 0.050995, Training Accuracy LC: 0.998833, Validation Accuracy LC: 0.985400, \n",
      "             \t Training Loss: 0.006158, Validation Loss: 0.050995, Training Accuracy: 0.998833, Validation Accuracy: 0.985400\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.007777, Validation Loss DLoRALC: 0.053448, Training Accuracy DLoRALC: 0.999000, Validation Accuracy DLoRALC: 0.983300 \n",
      "             \t Training Loss LC: 0.004817, Validation Loss LC: 0.051243, Training Accuracy LC: 0.999333, Validation Accuracy LC: 0.983700, \n",
      "             \t Training Loss: 0.004819, Validation Loss: 0.051241, Training Accuracy: 0.999333, Validation Accuracy: 0.983700\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.007391, Validation Loss DLoRALC: 0.056476, Training Accuracy DLoRALC: 0.998333, Validation Accuracy DLoRALC: 0.981300 \n",
      "             \t Training Loss LC: 0.004321, Validation Loss LC: 0.049765, Training Accuracy LC: 0.999500, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.004322, Validation Loss: 0.049767, Training Accuracy: 0.999500, Validation Accuracy: 0.984600\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.006541, Validation Loss DLoRALC: 0.052703, Training Accuracy DLoRALC: 0.999167, Validation Accuracy DLoRALC: 0.983600 \n",
      "             \t Training Loss LC: 0.003466, Validation Loss LC: 0.048702, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.984700, \n",
      "             \t Training Loss: 0.003468, Validation Loss: 0.048701, Training Accuracy: 0.999667, Validation Accuracy: 0.984700\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.006082, Validation Loss DLoRALC: 0.056256, Training Accuracy DLoRALC: 0.999167, Validation Accuracy DLoRALC: 0.982200 \n",
      "             \t Training Loss LC: 0.003429, Validation Loss LC: 0.050663, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.003430, Validation Loss: 0.050663, Training Accuracy: 0.999667, Validation Accuracy: 0.984500\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.005480, Validation Loss DLoRALC: 0.052573, Training Accuracy DLoRALC: 0.999500, Validation Accuracy DLoRALC: 0.983200 \n",
      "             \t Training Loss LC: 0.003615, Validation Loss LC: 0.049857, Training Accuracy LC: 0.999500, Validation Accuracy LC: 0.986000, \n",
      "             \t Training Loss: 0.003616, Validation Loss: 0.049857, Training Accuracy: 0.999500, Validation Accuracy: 0.986000\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.005537, Validation Loss DLoRALC: 0.053096, Training Accuracy DLoRALC: 0.999333, Validation Accuracy DLoRALC: 0.984100 \n",
      "             \t Training Loss LC: 0.003023, Validation Loss LC: 0.049867, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.003023, Validation Loss: 0.049863, Training Accuracy: 0.999833, Validation Accuracy: 0.984500\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.005056, Validation Loss DLoRALC: 0.057068, Training Accuracy DLoRALC: 0.999167, Validation Accuracy DLoRALC: 0.982900 \n",
      "             \t Training Loss LC: 0.003003, Validation Loss LC: 0.052408, Training Accuracy LC: 0.999667, Validation Accuracy LC: 0.985200, \n",
      "             \t Training Loss: 0.003003, Validation Loss: 0.052405, Training Accuracy: 0.999500, Validation Accuracy: 0.985200\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.004316, Validation Loss DLoRALC: 0.053265, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.983300 \n",
      "             \t Training Loss LC: 0.002751, Validation Loss LC: 0.049965, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.985100, \n",
      "             \t Training Loss: 0.002751, Validation Loss: 0.049964, Training Accuracy: 0.999833, Validation Accuracy: 0.985100\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.003606, Validation Loss DLoRALC: 0.054658, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.982500 \n",
      "             \t Training Loss LC: 0.002213, Validation Loss LC: 0.050079, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984300, \n",
      "             \t Training Loss: 0.002213, Validation Loss: 0.050080, Training Accuracy: 1.000000, Validation Accuracy: 0.984300\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.003761, Validation Loss DLoRALC: 0.053491, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.983000 \n",
      "             \t Training Loss LC: 0.002330, Validation Loss LC: 0.050358, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984600, \n",
      "             \t Training Loss: 0.002330, Validation Loss: 0.050357, Training Accuracy: 0.999833, Validation Accuracy: 0.984600\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.003517, Validation Loss DLoRALC: 0.054577, Training Accuracy DLoRALC: 0.999667, Validation Accuracy DLoRALC: 0.983400 \n",
      "             \t Training Loss LC: 0.002091, Validation Loss LC: 0.050537, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984800, \n",
      "             \t Training Loss: 0.002091, Validation Loss: 0.050537, Training Accuracy: 1.000000, Validation Accuracy: 0.984800\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.003121, Validation Loss DLoRALC: 0.053613, Training Accuracy DLoRALC: 1.000000, Validation Accuracy DLoRALC: 0.983900 \n",
      "             \t Training Loss LC: 0.001952, Validation Loss LC: 0.050111, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984800, \n",
      "             \t Training Loss: 0.001952, Validation Loss: 0.050109, Training Accuracy: 1.000000, Validation Accuracy: 0.984800\n",
      "End of model training on dataloader9...\n",
      "Model saved at accuracy: 0.9839\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n",
      "-----------------------------------\n",
      "Start of model training on dataloader10...\n",
      "Epoch: [0/19], Training Loss DLoRALC: 0.060324, Validation Loss DLoRALC: 0.070438, Training Accuracy DLoRALC: 0.983823, Validation Accuracy DLoRALC: 0.979600 \n",
      "             \t Training Loss LC: 0.049822, Validation Loss LC: 0.062160, Training Accuracy LC: 0.987492, Validation Accuracy LC: 0.981600, \n",
      "             \t Training Loss: 0.049823, Validation Loss: 0.062161, Training Accuracy: 0.987492, Validation Accuracy: 0.981600\n",
      "Epoch: [1/19], Training Loss DLoRALC: 0.035964, Validation Loss DLoRALC: 0.060231, Training Accuracy DLoRALC: 0.988326, Validation Accuracy DLoRALC: 0.983900 \n",
      "             \t Training Loss LC: 0.026194, Validation Loss LC: 0.055033, Training Accuracy LC: 0.992328, Validation Accuracy LC: 0.983300, \n",
      "             \t Training Loss: 0.026189, Validation Loss: 0.055025, Training Accuracy: 0.992328, Validation Accuracy: 0.983300\n",
      "Epoch: [2/19], Training Loss DLoRALC: 0.022631, Validation Loss DLoRALC: 0.073843, Training Accuracy DLoRALC: 0.993662, Validation Accuracy DLoRALC: 0.979700 \n",
      "             \t Training Loss LC: 0.014865, Validation Loss LC: 0.060651, Training Accuracy LC: 0.995163, Validation Accuracy LC: 0.982500, \n",
      "             \t Training Loss: 0.014861, Validation Loss: 0.060661, Training Accuracy: 0.995163, Validation Accuracy: 0.982500\n",
      "Epoch: [3/19], Training Loss DLoRALC: 0.019659, Validation Loss DLoRALC: 0.052895, Training Accuracy DLoRALC: 0.993329, Validation Accuracy DLoRALC: 0.984600 \n",
      "             \t Training Loss LC: 0.011413, Validation Loss LC: 0.052173, Training Accuracy LC: 0.997165, Validation Accuracy LC: 0.984400, \n",
      "             \t Training Loss: 0.011416, Validation Loss: 0.052177, Training Accuracy: 0.997165, Validation Accuracy: 0.984400\n",
      "Epoch: [4/19], Training Loss DLoRALC: 0.012862, Validation Loss DLoRALC: 0.055080, Training Accuracy DLoRALC: 0.996664, Validation Accuracy DLoRALC: 0.984300 \n",
      "             \t Training Loss LC: 0.006167, Validation Loss LC: 0.055442, Training Accuracy LC: 0.999166, Validation Accuracy LC: 0.983900, \n",
      "             \t Training Loss: 0.006166, Validation Loss: 0.055433, Training Accuracy: 0.999166, Validation Accuracy: 0.983900\n",
      "Epoch: [5/19], Training Loss DLoRALC: 0.009958, Validation Loss DLoRALC: 0.057594, Training Accuracy DLoRALC: 0.998165, Validation Accuracy DLoRALC: 0.984600 \n",
      "             \t Training Loss LC: 0.004541, Validation Loss LC: 0.052163, Training Accuracy LC: 0.999500, Validation Accuracy LC: 0.984800, \n",
      "             \t Training Loss: 0.004541, Validation Loss: 0.052165, Training Accuracy: 0.999500, Validation Accuracy: 0.984800\n",
      "Epoch: [6/19], Training Loss DLoRALC: 0.007614, Validation Loss DLoRALC: 0.052903, Training Accuracy DLoRALC: 0.998165, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.003569, Validation Loss LC: 0.051833, Training Accuracy LC: 0.999666, Validation Accuracy LC: 0.985200, \n",
      "             \t Training Loss: 0.003569, Validation Loss: 0.051833, Training Accuracy: 0.999666, Validation Accuracy: 0.985200\n",
      "Epoch: [7/19], Training Loss DLoRALC: 0.006396, Validation Loss DLoRALC: 0.052563, Training Accuracy DLoRALC: 0.998833, Validation Accuracy DLoRALC: 0.984900 \n",
      "             \t Training Loss LC: 0.002912, Validation Loss LC: 0.051910, Training Accuracy LC: 0.999666, Validation Accuracy LC: 0.985000, \n",
      "             \t Training Loss: 0.002912, Validation Loss: 0.051909, Training Accuracy: 0.999666, Validation Accuracy: 0.985000\n",
      "Epoch: [8/19], Training Loss DLoRALC: 0.005216, Validation Loss DLoRALC: 0.054976, Training Accuracy DLoRALC: 0.999666, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.002357, Validation Loss LC: 0.054139, Training Accuracy LC: 0.999833, Validation Accuracy LC: 0.984700, \n",
      "             \t Training Loss: 0.002357, Validation Loss: 0.054142, Training Accuracy: 0.999833, Validation Accuracy: 0.984700\n",
      "Epoch: [9/19], Training Loss DLoRALC: 0.004779, Validation Loss DLoRALC: 0.052000, Training Accuracy DLoRALC: 0.999500, Validation Accuracy DLoRALC: 0.985000 \n",
      "             \t Training Loss LC: 0.002161, Validation Loss LC: 0.053425, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984500, \n",
      "             \t Training Loss: 0.002161, Validation Loss: 0.053426, Training Accuracy: 1.000000, Validation Accuracy: 0.984500\n",
      "Epoch: [10/19], Training Loss DLoRALC: 0.004595, Validation Loss DLoRALC: 0.052537, Training Accuracy DLoRALC: 0.999666, Validation Accuracy DLoRALC: 0.985000 \n",
      "             \t Training Loss LC: 0.001908, Validation Loss LC: 0.053479, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984700, \n",
      "             \t Training Loss: 0.001908, Validation Loss: 0.053479, Training Accuracy: 1.000000, Validation Accuracy: 0.984700\n",
      "Epoch: [11/19], Training Loss DLoRALC: 0.003859, Validation Loss DLoRALC: 0.055692, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.984400 \n",
      "             \t Training Loss LC: 0.001759, Validation Loss LC: 0.052828, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.985000, \n",
      "             \t Training Loss: 0.001759, Validation Loss: 0.052824, Training Accuracy: 1.000000, Validation Accuracy: 0.985000\n",
      "Epoch: [12/19], Training Loss DLoRALC: 0.003764, Validation Loss DLoRALC: 0.051661, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.985300 \n",
      "             \t Training Loss LC: 0.001651, Validation Loss LC: 0.053259, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984800, \n",
      "             \t Training Loss: 0.001652, Validation Loss: 0.053259, Training Accuracy: 1.000000, Validation Accuracy: 0.984800\n",
      "Epoch: [13/19], Training Loss DLoRALC: 0.003224, Validation Loss DLoRALC: 0.053604, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.984600 \n",
      "             \t Training Loss LC: 0.001495, Validation Loss LC: 0.053970, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984000, \n",
      "             \t Training Loss: 0.001495, Validation Loss: 0.053970, Training Accuracy: 1.000000, Validation Accuracy: 0.984100\n",
      "Epoch: [14/19], Training Loss DLoRALC: 0.003234, Validation Loss DLoRALC: 0.052687, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.985300 \n",
      "             \t Training Loss LC: 0.001457, Validation Loss LC: 0.053955, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984400, \n",
      "             \t Training Loss: 0.001458, Validation Loss: 0.053954, Training Accuracy: 1.000000, Validation Accuracy: 0.984400\n",
      "Epoch: [15/19], Training Loss DLoRALC: 0.002984, Validation Loss DLoRALC: 0.053554, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.985400 \n",
      "             \t Training Loss LC: 0.001374, Validation Loss LC: 0.053571, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984700, \n",
      "             \t Training Loss: 0.001374, Validation Loss: 0.053572, Training Accuracy: 1.000000, Validation Accuracy: 0.984700\n",
      "Epoch: [16/19], Training Loss DLoRALC: 0.002774, Validation Loss DLoRALC: 0.054868, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.983500 \n",
      "             \t Training Loss LC: 0.001294, Validation Loss LC: 0.053515, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.984800, \n",
      "             \t Training Loss: 0.001294, Validation Loss: 0.053514, Training Accuracy: 1.000000, Validation Accuracy: 0.984800\n",
      "Epoch: [17/19], Training Loss DLoRALC: 0.002590, Validation Loss DLoRALC: 0.052140, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.985600 \n",
      "             \t Training Loss LC: 0.001236, Validation Loss LC: 0.054035, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.985000, \n",
      "             \t Training Loss: 0.001236, Validation Loss: 0.054032, Training Accuracy: 1.000000, Validation Accuracy: 0.985000\n",
      "Epoch: [18/19], Training Loss DLoRALC: 0.002454, Validation Loss DLoRALC: 0.053197, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.985000 \n",
      "             \t Training Loss LC: 0.001174, Validation Loss LC: 0.054491, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.985100, \n",
      "             \t Training Loss: 0.001174, Validation Loss: 0.054489, Training Accuracy: 1.000000, Validation Accuracy: 0.985100\n",
      "Epoch: [19/19], Training Loss DLoRALC: 0.002299, Validation Loss DLoRALC: 0.053410, Training Accuracy DLoRALC: 0.999833, Validation Accuracy DLoRALC: 0.985100 \n",
      "             \t Training Loss LC: 0.001145, Validation Loss LC: 0.054162, Training Accuracy LC: 1.000000, Validation Accuracy LC: 0.985100, \n",
      "             \t Training Loss: 0.001145, Validation Loss: 0.054160, Training Accuracy: 1.000000, Validation Accuracy: 0.985100\n",
      "End of model training on dataloader10...\n",
      "Model saved at accuracy: 0.9851\n",
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n"
     ]
    }
   ],
   "source": [
    "DECOMPOSED_LAYERS = [\"classifier.1.weight\", \"classifier.3.weight\"]\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "BRANCH_ACC = \"0.7058\"\n",
    "\n",
    "original = LeNet().to(device)\n",
    "model_original = LeNet().to(device)\n",
    "model_no_touch = LeNet().to(device)\n",
    "\n",
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_no_touch.load_state_dict(torch.load(BRANCH_LOC))\n",
    "\n",
    "w, b = getBase(original)\n",
    "model = LeNet_LowRank(w, b, rank = RANK).to(device)\n",
    "\n",
    "print(model.state_dict().keys())\n",
    "\n",
    "load_sd_decomp(torch.load(BRANCH_LOC, map_location=device), model, DECOMPOSED_LAYERS)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_dloralc)\n",
    "optimizer_lc_only = torch.optim.SGD(model_original.parameters(), lr=learning_rate)\n",
    "optimizer_no_touch = torch.optim.SGD(model_no_touch.parameters(), lr=learning_rate)\n",
    "\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n",
    "\n",
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "\n",
    "train_loader_list = [train_loader2, train_loader3, train_loader4, train_loader5, train_loader6, train_loader7, train_loader8, train_loader9, train_loader10]\n",
    "\n",
    "for i in range(len(train_loader_list)):\n",
    "\n",
    "    # Initialize the current iteration and set to 0\n",
    "    current_iter = 0\n",
    "    current_set = 0\n",
    "\n",
    "    # Initialize the current iteration and set to 0 for the old LC method\n",
    "    current_iter_old_lc = 0\n",
    "    current_set_old_lc = 0\n",
    "\n",
    "    print(\"-----------------------------------\")\n",
    "\n",
    "    txt_MNIST_train = \"MNIST train dataloader{}\".format(i+2)\n",
    "    txt_train_dataset = \"train dataset {}\".format(i+2)\n",
    "\n",
    "    wandb.config[\"branch_accuracy\"] = BRANCH_ACC\n",
    "    wandb.config[txt_train_dataset] = txt_MNIST_train\n",
    "\n",
    "    # Training code on dataloader2\n",
    "    print(\"Start of model training on dataloader{}...\".format(i+2))\n",
    "\n",
    "    isLoop = True\n",
    "\n",
    "    valid_accuracy_list = []\n",
    "\n",
    "    base = None\n",
    "\n",
    "    for epoch in range(NUM_EPOCHES):\n",
    "        if not isLoop:\n",
    "            break\n",
    "        else:\n",
    "            train_loss = 0.0\n",
    "            train_loss_lc = 0.0\n",
    "            train_loss_no_touch = 0.0\n",
    "            valid_loss = 0.0\n",
    "            valid_loss_lc = 0.0\n",
    "            valid_loss_no_touch = 0.0\n",
    "\n",
    "            train_correct = 0\n",
    "            train_correct_lc = 0\n",
    "            train_correct_no_touch = 0\n",
    "            train_total = 0\n",
    "            train_total_lc = 0\n",
    "            train_total_no_touch = 0\n",
    "\n",
    "            valid_correct = 0\n",
    "            valid_correct_lc = 0\n",
    "            valid_correct_no_touch = 0\n",
    "            valid_total = 0\n",
    "            valid_total_lc = 0\n",
    "            valid_total_no_touch = 0\n",
    "            \n",
    "            model.train()\n",
    "            model_original.train()\n",
    "            model_no_touch.train()\n",
    "\n",
    "            for iter, data in enumerate(train_loader_list[i]):\n",
    "\n",
    "                # Check if it is the first iteration of the first epoch\n",
    "                if iter == 0: # first iteration, create baseline model\n",
    "                # if iter == 0 and epoch == 0: # first iteration, create baseline model\n",
    "                    ########################################################\n",
    "                    ### DELTA-LORA + LC\n",
    "                    ########################################################\n",
    "                    \n",
    "                    base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "                                                            \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "                    ########################################################\n",
    "                    ### LC\n",
    "                    ########################################################\n",
    "\n",
    "                    cstate = model_original.state_dict()\n",
    "                    set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                    if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                        os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                    prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path,DECOMPOSED_LAYERS)\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    ########################################################\n",
    "                    ### DELTA-LORA + LC \n",
    "                    ########################################################\n",
    "\n",
    "                    # Delta-compression: The delta for the weights of the normal and decomposed layers.\n",
    "                    # Also returns the full dictionary, which holds the bias.\n",
    "\n",
    "                    #Calculate the time before generate_delta_gpu function\n",
    "                    delta, decomp_delta, bias = lc.generate_delta_gpu(base, \n",
    "                                                                    base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                    # Compressing the delta and decomposed delta\n",
    "                    compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                                decomp_delta)\n",
    "                    # Saving checkpoint\n",
    "                    # lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                    #                 \"/set_{}\".format(current_set))\n",
    "                \n",
    "                    # Update base and base_decomp\n",
    "                    base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                    base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                    # Update current iteration\n",
    "                    current_iter += 1\n",
    "\n",
    "                    ########################################################\n",
    "                    ### LC\n",
    "                    ########################################################\n",
    "\n",
    "                    cstate = model_original.state_dict()\n",
    "                    old_lc_delta, old_lc_bias = olc.generate_delta_gpu(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "                    olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                    # olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                    #                     old_lc_bias, current_iter_old_lc)\n",
    "                    prev_state = np.add(prev_state, update_prev)\n",
    "                    current_iter_old_lc += 1\n",
    "\n",
    "                inputs, labels = data\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(inputs)\n",
    "\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                train_correct += (predicted == labels).sum().item()\n",
    "                train_total += labels.size(0)\n",
    "\n",
    "                train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                optimizer_lc_only.zero_grad()\n",
    "                output_lc = model_original(inputs)\n",
    "\n",
    "                loss_lc = torch.nn.CrossEntropyLoss()(output_lc, labels)\n",
    "                loss_lc.backward()\n",
    "                optimizer_lc_only.step()\n",
    "                train_loss_lc += loss_lc.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted_lc = torch.max(output_lc, 1)\n",
    "                train_correct_lc += (predicted_lc == labels).sum().item()\n",
    "                train_total_lc += labels.size(0)\n",
    "\n",
    "                train_acc_lc = torch.eq(output_lc.argmax(-1), labels).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### NO TOUCH\n",
    "                ########################################################\n",
    "\n",
    "                optimizer_no_touch.zero_grad()\n",
    "                output_no_touch = model_no_touch(inputs)\n",
    "\n",
    "                loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, labels)\n",
    "                loss_no_touch.backward()\n",
    "                optimizer_no_touch.step()\n",
    "                train_loss_no_touch += loss_no_touch.item() * inputs.size(0)\n",
    "\n",
    "                _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "                train_correct_no_touch += (predicted_no_touch == labels).sum().item()\n",
    "                train_total_no_touch += labels.size(0)\n",
    "\n",
    "                train_acc_no_touch = torch.eq(output_no_touch.argmax(-1), labels).float().mean()\n",
    "\n",
    "            model.eval()\n",
    "            model_original.eval()\n",
    "            model_no_touch.eval()\n",
    "            with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "                for data, target in test_loader:\n",
    "                    # Move data and target to the correct device\n",
    "                    data, target = data.to(device), target.to(device)\n",
    "\n",
    "                    ########################################################\n",
    "                    ### DELTA-LORA + LC\n",
    "                    ########################################################\n",
    "\n",
    "                    output = model(data)\n",
    "                    loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                    valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                    _, predicted = torch.max(output, 1)\n",
    "                    valid_correct += (predicted == target).sum().item()\n",
    "                    valid_total += target.size(0)\n",
    "\n",
    "                    valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "                    ########################################################\n",
    "                    ### LC\n",
    "                    ########################################################\n",
    "\n",
    "                    output_lc = model_original(data)\n",
    "                    loss_lc = torch.nn.CrossEntropyLoss()(output_lc, target)\n",
    "                    valid_loss_lc += loss_lc.item() * data.size(0)\n",
    "\n",
    "                    _, predicted_lc = torch.max(output_lc, 1)\n",
    "                    valid_correct_lc += (predicted_lc == target).sum().item()\n",
    "                    valid_total_lc += target.size(0)\n",
    "\n",
    "                    valid_acc_lc = torch.eq(output_lc.argmax(-1), target).float().mean()\n",
    "\n",
    "                    ########################################################\n",
    "                    ### NO TOUCH\n",
    "                    ########################################################\n",
    "\n",
    "                    output_no_touch = model_no_touch(data)\n",
    "                    loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, target)\n",
    "                    valid_loss_no_touch += loss_no_touch.item() * data.size(0)\n",
    "\n",
    "                    _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "                    valid_correct_no_touch += (predicted_no_touch == target).sum().item()\n",
    "                    valid_total_no_touch += target.size(0)\n",
    "\n",
    "                    valid_acc_no_touch = torch.eq(output_no_touch.argmax(-1), target).float().mean()\n",
    "\n",
    "                    \n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader_list[i].dataset)\n",
    "        train_loss_lc /= len(train_loader_list[i].dataset)\n",
    "        train_loss_no_touch /= len(train_loader_list[i].dataset)\n",
    "        valid_loss /= len(test_loader.dataset)\n",
    "        valid_loss_lc /= len(test_loader.dataset)\n",
    "        valid_loss_no_touch /= len(test_loader.dataset)\n",
    "\n",
    "        train_accuarcy = train_correct / train_total\n",
    "        train_accuarcy_lc = train_correct_lc / train_total_lc\n",
    "        train_accuarcy_no_touch = train_correct_no_touch / train_total_no_touch\n",
    "        valid_accuracy = valid_correct / valid_total\n",
    "        valid_accuracy_lc = valid_correct_lc / valid_total_lc\n",
    "        valid_accuracy_no_touch = valid_correct_no_touch / valid_total_no_touch\n",
    "\n",
    "        valid_accuracy_list.append(valid_accuracy)\n",
    "        \n",
    "        print(\"Epoch: [{}/{}], Training Loss DLoRALC: {:.6f}, Validation Loss DLoRALC: {:.6f}, Training Accuracy DLoRALC: {:.6f}, Validation Accuracy DLoRALC: {:.6f} \\n \\\n",
    "            \\t Training Loss LC: {:.6f}, Validation Loss LC: {:.6f}, Training Accuracy LC: {:.6f}, Validation Accuracy LC: {:.6f}, \\n \\\n",
    "            \\t Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, \n",
    "            train_loss, valid_loss, train_accuarcy, valid_accuracy, \n",
    "            train_loss_lc, valid_loss_lc, train_accuarcy_lc, valid_accuracy_lc, \n",
    "            train_loss_no_touch, valid_loss_no_touch, train_accuarcy_no_touch, valid_accuracy_no_touch)) \n",
    "\n",
    "        wandb.log({\n",
    "            \"train_loss_dloralc\": train_loss,\n",
    "            \"valid_loss_dloralc\": valid_loss,\n",
    "            \"train_accuracy_dloralc\": train_accuarcy,\n",
    "            \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "            \"train_loss_lc\": train_loss_lc,\n",
    "            \"valid_loss_lc\": valid_loss_lc,\n",
    "            \"train_accuracy_lc\": train_accuarcy_lc,\n",
    "            \"valid_accuracy_lc\": valid_accuracy_lc,\n",
    "            \"train_loss\": train_loss_no_touch,\n",
    "            \"valid_loss\": valid_loss_no_touch,\n",
    "            \"train_accuracy\": train_accuarcy_no_touch,\n",
    "            \"valid_accuracy\": valid_accuracy_no_touch,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "\n",
    "\n",
    "    print(\"End of model training on dataloader{}...\".format(i+2))\n",
    "\n",
    "    # Store accuracy\n",
    "\n",
    "    BRANCH_ACC = valid_accuracy\n",
    "    print(\"Model saved at accuracy: {:.4f}\".format(BRANCH_ACC))\n",
    "    torch.save(model.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}_{}.pt\".format(i+2,BRANCH_ACC))\n",
    "\n",
    "    w, b = getBase(original)\n",
    "    model = LeNet_LowRank(w, b, rank = RANK).to(device)\n",
    "\n",
    "    model.load_state_dict(torch.load(HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}_{}.pt\".format(i+2,BRANCH_ACC)))\n",
    "\n",
    "    print(model.state_dict().keys())\n",
    "\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_dloralc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a79686a5ee540ec8ace4938a9de22a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.091 MB uploaded\\r'), FloatProgress(value=0.015130804172274562, max=1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▁▃▅▆▁▃▅▆█▃▄▆█▂▄▆▇▂▃▆▇▁▃▅▇▁▃▅▆█▃▄▆█▂▄▆█</td></tr><tr><td>train_accuracy</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>train_accuracy_dloralc</td><td>▁▄▇▇████████████████████████████████████</td></tr><tr><td>train_accuracy_lc</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_dloralc</td><td>█▇▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss_lc</td><td>█▇▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_accuracy</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>valid_accuracy_dloralc</td><td>▁▄▇▇████████████████████████████████████</td></tr><tr><td>valid_accuracy_lc</td><td>▁▄▇█████████████████████████████████████</td></tr><tr><td>valid_loss</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss_dloralc</td><td>█▆▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_loss_lc</td><td>█▆▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>1.0</td></tr><tr><td>train_accuracy_dloralc</td><td>0.99983</td></tr><tr><td>train_accuracy_lc</td><td>1.0</td></tr><tr><td>train_loss</td><td>0.00115</td></tr><tr><td>train_loss_dloralc</td><td>0.0023</td></tr><tr><td>train_loss_lc</td><td>0.00114</td></tr><tr><td>valid_accuracy</td><td>0.9851</td></tr><tr><td>valid_accuracy_dloralc</td><td>0.9851</td></tr><tr><td>valid_accuracy_lc</td><td>0.9851</td></tr><tr><td>valid_loss</td><td>0.05416</td></tr><tr><td>valid_loss_dloralc</td><td>0.05341</td></tr><tr><td>valid_loss_lc</td><td>0.05416</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LeNet-With-Incremental-Learning_LC_DLORA-Without-Restore_10x10</strong> at: <a href='https://wandb.ai/bryanbradfo/LeNet/runs/jdbx0b1u/workspace' target=\"_blank\">https://wandb.ai/bryanbradfo/LeNet/runs/jdbx0b1u/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240705_160139-jdbx0b1u\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
