{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ViT Implementation with Old LC on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\utils\\generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "import old_lc.main as olc\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.ViT import ViT\n",
    "from src.models.ViT_LowRank import getBase, ViT_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, evaluate_accuracy_gpu, lazy_restore,lazy_restore_gpu, evaluate_compression\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHES = 9000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Data Loader function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "def data_loader():\n",
    "    # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "    ])\n",
    "    trainset = datasets.CIFAR10(root='./data_CIFAR10', download=True, train=True, transform=transform)\n",
    "\n",
    "    # trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                        #   download=True, transform=transform)\n",
    "    # Reintroduce the 2000 datapoints model has not seen before.\n",
    "    trainset.data = trainset.data[-2000:-1000]\n",
    "    trainset.targets = trainset.targets[-2000:-1000]\n",
    "    # trainset.data = trainset.data[-56000:]\n",
    "    # trainset.targets = trainset.targets[-56000:]\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "    testset = datasets.CIFAR10('./data_CIFAR10', download=True, train=False, transform=transform)\n",
    "\n",
    "    # testset = datasets.MNIST(root='./data', train=False,\n",
    "    #                                      download=True, transform=transform)\n",
    "\n",
    "    testset.data = trainset.data[-1000:]\n",
    "    testset.targets = trainset.targets[-1000:]\n",
    "    # testset.data = trainset.data[:1000]\n",
    "    # testset.targets = trainset.targets[:1000]\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "\n",
    "# HDFP = \"./volumes/Ultra Touch\"  # Placeholder for HDD path\n",
    "\n",
    "# def data_loader():\n",
    "#     # Définir les transformations\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Resize((28, 28)),\n",
    "#         transforms.Normalize((0.5,), (0.5,))\n",
    "#     ])\n",
    "\n",
    "#     # Charger le dataset d'entraînement MNIST\n",
    "#     trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "#     # Utiliser les dernières 1000 images pour l'entraînement\n",
    "#     # trainset.data = trainset.data.clone()[-40000:]\n",
    "#     # trainset.targets = trainset.targets.clone()[-40000:]\n",
    "#     trainset.data = trainset.data.clone()[-2000:-1000]\n",
    "#     trainset.targets = trainset.targets.clone()[-2000:-1000]\n",
    "#     train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "#     # Charger le dataset de test MNIST\n",
    "#     testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "#     # Utiliser les premières 300 images du dataset de test\n",
    "#     testset.data = trainset.data.clone()[-1000:]\n",
    "#     testset.targets = trainset.targets.clone()[-1000:]\n",
    "#     # testset.data = trainset.data.clone()[:3000]\n",
    "#     # testset.targets = trainset.targets.clone()[:3000]\n",
    "#     test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "#     return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bypass the matplotlib error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Showing some images of the dataset we use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "automobile | airplane |  bird\n"
     ]
    }
   ],
   "source": [
    "# # Adjust these values to match the normalization values used during the loading of your dataset\n",
    "# mean = 0.1307\n",
    "# std = 0.3081\n",
    "\n",
    "# # Function to show an image\n",
    "# def imshow(img):\n",
    "#     # Adjusting unnormalization for potentially 3-channel images\n",
    "#     img = img * std + mean  # Unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "# # Assuming train_loader is defined and loaded as before\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = next(dataiter)\n",
    "\n",
    "# # Show images\n",
    "# imshow(torchvision.utils.make_grid(images[:3]))\n",
    "# # Print labels\n",
    "# print(' '.join('%5s' % labels[j] for j in range(3)))\n",
    "\n",
    "\n",
    "# Mapping des labels aux noms de classe\n",
    "classes = ('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# Affichage des images avec les noms de classe\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)), interpolation='nearest')\n",
    "    plt.show()\n",
    "\n",
    "# Chargement des données\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Affichage des images\n",
    "imshow(torchvision.utils.make_grid(images[:3]))\n",
    "\n",
    "# Impression des noms de classe\n",
    "print(' | '.join('%5s' % classes[labels[j]] for j in range(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of folder to save the results (for plots and compression rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "\n",
    "SAVE_LOC_OLC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc\"\n",
    "if not os.path.exists(SAVE_LOC_OLC):\n",
    "    os.makedirs(SAVE_LOC_OLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the accuracy functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binary(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = torch.sigmoid(model(input))  # Apply sigmoid to model output to get probabilities.\n",
    "            output = torch.where(output > 0.5, 1, 0)  # Threshold probabilities at 0.5 to decide between classes 0 and 1.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += (output == label).sum().item()  # Increment correct predictions by the number of matches in the batch.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n",
    "\n",
    "def accuracy_multiclass(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = model(input)  # Get the raw logits from the model.\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max logit which represents the predicted class.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += pred.eq(label.view_as(pred)).sum().item()  # Compare predictions with true labels and sum up correct predictions.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special function for the accuracy of the model on GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_multiclass_gpu(model, evaluation_set):\n",
    "    device = next(model.parameters()).device  # Get the device of the model\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for inputs, labels in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device of the model\n",
    "            output = model(inputs)  # Get the raw logits from the model.\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max logit which represents the predicted class.\n",
    "            no_seen += labels.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += pred.eq(labels.view_as(pred)).sum().item()  # Compare predictions with true labels and sum up correct predictions.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usual training on GPU (Creating branchpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0 (NVIDIA GeForce RTX 4060 Laptop GPU)\n",
      "Epoch : [0/8999], Training Loss: 2.304662, Validation Loss: 2.304465\n",
      "Epoch : [1/8999], Training Loss: 2.304345, Validation Loss: 2.304143\n",
      "Epoch : [2/8999], Training Loss: 2.304032, Validation Loss: 2.303844\n",
      "Epoch : [3/8999], Training Loss: 2.303742, Validation Loss: 2.303566\n",
      "Epoch : [4/8999], Training Loss: 2.303472, Validation Loss: 2.303306\n",
      "Epoch : [5/8999], Training Loss: 2.303218, Validation Loss: 2.303061\n",
      "Epoch : [6/8999], Training Loss: 2.302978, Validation Loss: 2.302828\n",
      "Epoch : [7/8999], Training Loss: 2.302751, Validation Loss: 2.302607\n",
      "Epoch : [8/8999], Training Loss: 2.302533, Validation Loss: 2.302395\n",
      "Epoch : [9/8999], Training Loss: 2.302324, Validation Loss: 2.302191\n",
      "Epoch : [10/8999], Training Loss: 2.302123, Validation Loss: 2.301994\n",
      "Epoch : [11/8999], Training Loss: 2.301928, Validation Loss: 2.301802\n",
      "Epoch : [12/8999], Training Loss: 2.301738, Validation Loss: 2.301615\n",
      "Epoch : [13/8999], Training Loss: 2.301552, Validation Loss: 2.301431\n",
      "Epoch : [14/8999], Training Loss: 2.301370, Validation Loss: 2.301251\n",
      "Epoch : [15/8999], Training Loss: 2.301190, Validation Loss: 2.301074\n",
      "Epoch : [16/8999], Training Loss: 2.301013, Validation Loss: 2.300898\n",
      "Epoch : [17/8999], Training Loss: 2.300838, Validation Loss: 2.300724\n",
      "Epoch : [18/8999], Training Loss: 2.300664, Validation Loss: 2.300551\n",
      "Epoch : [19/8999], Training Loss: 2.300491, Validation Loss: 2.300379\n",
      "Epoch : [20/8999], Training Loss: 2.300319, Validation Loss: 2.300206\n",
      "Epoch : [21/8999], Training Loss: 2.300146, Validation Loss: 2.300034\n",
      "Epoch : [22/8999], Training Loss: 2.299974, Validation Loss: 2.299862\n",
      "Epoch : [23/8999], Training Loss: 2.299801, Validation Loss: 2.299688\n",
      "Epoch : [24/8999], Training Loss: 2.299627, Validation Loss: 2.299514\n",
      "Epoch : [25/8999], Training Loss: 2.299452, Validation Loss: 2.299339\n",
      "Epoch : [26/8999], Training Loss: 2.299276, Validation Loss: 2.299162\n",
      "Epoch : [27/8999], Training Loss: 2.299098, Validation Loss: 2.298984\n",
      "Epoch : [28/8999], Training Loss: 2.298919, Validation Loss: 2.298804\n",
      "Epoch : [29/8999], Training Loss: 2.298738, Validation Loss: 2.298622\n",
      "Epoch : [30/8999], Training Loss: 2.298555, Validation Loss: 2.298438\n",
      "Epoch : [31/8999], Training Loss: 2.298370, Validation Loss: 2.298252\n",
      "Epoch : [32/8999], Training Loss: 2.298183, Validation Loss: 2.298064\n",
      "Epoch : [33/8999], Training Loss: 2.297993, Validation Loss: 2.297873\n",
      "Epoch : [34/8999], Training Loss: 2.297801, Validation Loss: 2.297679\n",
      "Epoch : [35/8999], Training Loss: 2.297606, Validation Loss: 2.297483\n",
      "Epoch : [36/8999], Training Loss: 2.297408, Validation Loss: 2.297284\n",
      "Epoch : [37/8999], Training Loss: 2.297207, Validation Loss: 2.297082\n",
      "Epoch : [38/8999], Training Loss: 2.297004, Validation Loss: 2.296877\n",
      "Epoch : [39/8999], Training Loss: 2.296797, Validation Loss: 2.296668\n",
      "Epoch : [40/8999], Training Loss: 2.296588, Validation Loss: 2.296457\n",
      "Epoch : [41/8999], Training Loss: 2.296375, Validation Loss: 2.296243\n",
      "Epoch : [42/8999], Training Loss: 2.296159, Validation Loss: 2.296026\n",
      "Epoch : [43/8999], Training Loss: 2.295940, Validation Loss: 2.295805\n",
      "Epoch : [44/8999], Training Loss: 2.295718, Validation Loss: 2.295581\n",
      "Epoch : [45/8999], Training Loss: 2.295492, Validation Loss: 2.295354\n",
      "Epoch : [46/8999], Training Loss: 2.295263, Validation Loss: 2.295123\n",
      "Epoch : [47/8999], Training Loss: 2.295031, Validation Loss: 2.294889\n",
      "Epoch : [48/8999], Training Loss: 2.294795, Validation Loss: 2.294651\n",
      "Epoch : [49/8999], Training Loss: 2.294556, Validation Loss: 2.294411\n",
      "Epoch : [50/8999], Training Loss: 2.294313, Validation Loss: 2.294166\n",
      "Epoch : [51/8999], Training Loss: 2.294067, Validation Loss: 2.293918\n",
      "Epoch : [52/8999], Training Loss: 2.293818, Validation Loss: 2.293667\n",
      "Epoch : [53/8999], Training Loss: 2.293565, Validation Loss: 2.293412\n",
      "Epoch : [54/8999], Training Loss: 2.293308, Validation Loss: 2.293154\n",
      "Epoch : [55/8999], Training Loss: 2.293048, Validation Loss: 2.292892\n",
      "Epoch : [56/8999], Training Loss: 2.292784, Validation Loss: 2.292626\n",
      "Epoch : [57/8999], Training Loss: 2.292517, Validation Loss: 2.292357\n",
      "Epoch : [58/8999], Training Loss: 2.292246, Validation Loss: 2.292085\n",
      "Epoch : [59/8999], Training Loss: 2.291972, Validation Loss: 2.291809\n",
      "Epoch : [60/8999], Training Loss: 2.291694, Validation Loss: 2.291529\n",
      "Epoch : [61/8999], Training Loss: 2.291413, Validation Loss: 2.291246\n",
      "Epoch : [62/8999], Training Loss: 2.291128, Validation Loss: 2.290960\n",
      "Epoch : [63/8999], Training Loss: 2.290840, Validation Loss: 2.290670\n",
      "Epoch : [64/8999], Training Loss: 2.290549, Validation Loss: 2.290376\n",
      "Epoch : [65/8999], Training Loss: 2.290254, Validation Loss: 2.290080\n",
      "Epoch : [66/8999], Training Loss: 2.289955, Validation Loss: 2.289780\n",
      "Epoch : [67/8999], Training Loss: 2.289654, Validation Loss: 2.289476\n",
      "Epoch : [68/8999], Training Loss: 2.289349, Validation Loss: 2.289170\n",
      "Epoch : [69/8999], Training Loss: 2.289041, Validation Loss: 2.288860\n",
      "Epoch : [70/8999], Training Loss: 2.288730, Validation Loss: 2.288547\n",
      "Epoch : [71/8999], Training Loss: 2.288416, Validation Loss: 2.288232\n",
      "Epoch : [72/8999], Training Loss: 2.288099, Validation Loss: 2.287913\n",
      "Epoch : [73/8999], Training Loss: 2.287779, Validation Loss: 2.287592\n",
      "Epoch : [74/8999], Training Loss: 2.287456, Validation Loss: 2.287268\n",
      "Epoch : [75/8999], Training Loss: 2.287131, Validation Loss: 2.286942\n",
      "Epoch : [76/8999], Training Loss: 2.286804, Validation Loss: 2.286613\n",
      "Epoch : [77/8999], Training Loss: 2.286474, Validation Loss: 2.286282\n",
      "Epoch : [78/8999], Training Loss: 2.286142, Validation Loss: 2.285949\n",
      "Epoch : [79/8999], Training Loss: 2.285809, Validation Loss: 2.285614\n",
      "Epoch : [80/8999], Training Loss: 2.285473, Validation Loss: 2.285277\n",
      "Epoch : [81/8999], Training Loss: 2.285136, Validation Loss: 2.284939\n",
      "Epoch : [82/8999], Training Loss: 2.284797, Validation Loss: 2.284599\n",
      "Epoch : [83/8999], Training Loss: 2.284457, Validation Loss: 2.284259\n",
      "Epoch : [84/8999], Training Loss: 2.284116, Validation Loss: 2.283917\n",
      "Epoch : [85/8999], Training Loss: 2.283774, Validation Loss: 2.283574\n",
      "Epoch : [86/8999], Training Loss: 2.283431, Validation Loss: 2.283231\n",
      "Epoch : [87/8999], Training Loss: 2.283088, Validation Loss: 2.282887\n",
      "Epoch : [88/8999], Training Loss: 2.282745, Validation Loss: 2.282544\n",
      "Epoch : [89/8999], Training Loss: 2.282401, Validation Loss: 2.282200\n",
      "Epoch : [90/8999], Training Loss: 2.282057, Validation Loss: 2.281856\n",
      "Epoch : [91/8999], Training Loss: 2.281714, Validation Loss: 2.281512\n",
      "Epoch : [92/8999], Training Loss: 2.281371, Validation Loss: 2.281169\n",
      "Epoch : [93/8999], Training Loss: 2.281028, Validation Loss: 2.280827\n",
      "Epoch : [94/8999], Training Loss: 2.280686, Validation Loss: 2.280485\n",
      "Epoch : [95/8999], Training Loss: 2.280345, Validation Loss: 2.280144\n",
      "Epoch : [96/8999], Training Loss: 2.280006, Validation Loss: 2.279805\n",
      "Epoch : [97/8999], Training Loss: 2.279667, Validation Loss: 2.279466\n",
      "Epoch : [98/8999], Training Loss: 2.279329, Validation Loss: 2.279129\n",
      "Epoch : [99/8999], Training Loss: 2.278993, Validation Loss: 2.278794\n",
      "Epoch : [100/8999], Training Loss: 2.278659, Validation Loss: 2.278460\n",
      "Epoch : [101/8999], Training Loss: 2.278326, Validation Loss: 2.278127\n",
      "Epoch : [102/8999], Training Loss: 2.277994, Validation Loss: 2.277796\n",
      "Epoch : [103/8999], Training Loss: 2.277665, Validation Loss: 2.277468\n",
      "Epoch : [104/8999], Training Loss: 2.277337, Validation Loss: 2.277141\n",
      "Epoch : [105/8999], Training Loss: 2.277011, Validation Loss: 2.276816\n",
      "Epoch : [106/8999], Training Loss: 2.276688, Validation Loss: 2.276493\n",
      "Epoch : [107/8999], Training Loss: 2.276366, Validation Loss: 2.276172\n",
      "Epoch : [108/8999], Training Loss: 2.276046, Validation Loss: 2.275853\n",
      "Epoch : [109/8999], Training Loss: 2.275729, Validation Loss: 2.275536\n",
      "Epoch : [110/8999], Training Loss: 2.275413, Validation Loss: 2.275221\n",
      "Epoch : [111/8999], Training Loss: 2.275100, Validation Loss: 2.274908\n",
      "Epoch : [112/8999], Training Loss: 2.274788, Validation Loss: 2.274598\n",
      "Epoch : [113/8999], Training Loss: 2.274479, Validation Loss: 2.274289\n",
      "Epoch : [114/8999], Training Loss: 2.274172, Validation Loss: 2.273983\n",
      "Epoch : [115/8999], Training Loss: 2.273867, Validation Loss: 2.273678\n",
      "Epoch : [116/8999], Training Loss: 2.273563, Validation Loss: 2.273376\n",
      "Epoch : [117/8999], Training Loss: 2.273262, Validation Loss: 2.273075\n",
      "Epoch : [118/8999], Training Loss: 2.272963, Validation Loss: 2.272776\n",
      "Epoch : [119/8999], Training Loss: 2.272665, Validation Loss: 2.272479\n",
      "Epoch : [120/8999], Training Loss: 2.272369, Validation Loss: 2.272184\n",
      "Epoch : [121/8999], Training Loss: 2.272075, Validation Loss: 2.271891\n",
      "Epoch : [122/8999], Training Loss: 2.271783, Validation Loss: 2.271599\n",
      "Epoch : [123/8999], Training Loss: 2.271492, Validation Loss: 2.271308\n",
      "Epoch : [124/8999], Training Loss: 2.271202, Validation Loss: 2.271019\n",
      "Epoch : [125/8999], Training Loss: 2.270915, Validation Loss: 2.270732\n",
      "Epoch : [126/8999], Training Loss: 2.270628, Validation Loss: 2.270446\n",
      "Epoch : [127/8999], Training Loss: 2.270343, Validation Loss: 2.270161\n",
      "Epoch : [128/8999], Training Loss: 2.270059, Validation Loss: 2.269877\n",
      "Epoch : [129/8999], Training Loss: 2.269776, Validation Loss: 2.269594\n",
      "Epoch : [130/8999], Training Loss: 2.269494, Validation Loss: 2.269313\n",
      "Epoch : [131/8999], Training Loss: 2.269213, Validation Loss: 2.269032\n",
      "Epoch : [132/8999], Training Loss: 2.268933, Validation Loss: 2.268752\n",
      "Epoch : [133/8999], Training Loss: 2.268654, Validation Loss: 2.268473\n",
      "Epoch : [134/8999], Training Loss: 2.268376, Validation Loss: 2.268195\n",
      "Epoch : [135/8999], Training Loss: 2.268098, Validation Loss: 2.267917\n",
      "Epoch : [136/8999], Training Loss: 2.267821, Validation Loss: 2.267640\n",
      "Epoch : [137/8999], Training Loss: 2.267545, Validation Loss: 2.267364\n",
      "Epoch : [138/8999], Training Loss: 2.267269, Validation Loss: 2.267088\n",
      "Epoch : [139/8999], Training Loss: 2.266994, Validation Loss: 2.266813\n",
      "Epoch : [140/8999], Training Loss: 2.266719, Validation Loss: 2.266538\n",
      "Epoch : [141/8999], Training Loss: 2.266445, Validation Loss: 2.266264\n",
      "Epoch : [142/8999], Training Loss: 2.266172, Validation Loss: 2.265990\n",
      "Epoch : [143/8999], Training Loss: 2.265899, Validation Loss: 2.265717\n",
      "Epoch : [144/8999], Training Loss: 2.265626, Validation Loss: 2.265445\n",
      "Epoch : [145/8999], Training Loss: 2.265354, Validation Loss: 2.265173\n",
      "Epoch : [146/8999], Training Loss: 2.265083, Validation Loss: 2.264901\n",
      "Epoch : [147/8999], Training Loss: 2.264812, Validation Loss: 2.264630\n",
      "Epoch : [148/8999], Training Loss: 2.264542, Validation Loss: 2.264360\n",
      "Epoch : [149/8999], Training Loss: 2.264272, Validation Loss: 2.264090\n",
      "Epoch : [150/8999], Training Loss: 2.264003, Validation Loss: 2.263821\n",
      "Epoch : [151/8999], Training Loss: 2.263735, Validation Loss: 2.263553\n",
      "Epoch : [152/8999], Training Loss: 2.263468, Validation Loss: 2.263285\n",
      "Epoch : [153/8999], Training Loss: 2.263201, Validation Loss: 2.263019\n",
      "Epoch : [154/8999], Training Loss: 2.262935, Validation Loss: 2.262753\n",
      "Epoch : [155/8999], Training Loss: 2.262670, Validation Loss: 2.262488\n",
      "Epoch : [156/8999], Training Loss: 2.262406, Validation Loss: 2.262223\n",
      "Epoch : [157/8999], Training Loss: 2.262143, Validation Loss: 2.261960\n",
      "Epoch : [158/8999], Training Loss: 2.261880, Validation Loss: 2.261698\n",
      "Epoch : [159/8999], Training Loss: 2.261619, Validation Loss: 2.261436\n",
      "Epoch : [160/8999], Training Loss: 2.261358, Validation Loss: 2.261176\n",
      "Epoch : [161/8999], Training Loss: 2.261099, Validation Loss: 2.260916\n",
      "Epoch : [162/8999], Training Loss: 2.260840, Validation Loss: 2.260657\n",
      "Epoch : [163/8999], Training Loss: 2.260583, Validation Loss: 2.260400\n",
      "Epoch : [164/8999], Training Loss: 2.260326, Validation Loss: 2.260143\n",
      "Epoch : [165/8999], Training Loss: 2.260070, Validation Loss: 2.259887\n",
      "Epoch : [166/8999], Training Loss: 2.259815, Validation Loss: 2.259632\n",
      "Epoch : [167/8999], Training Loss: 2.259561, Validation Loss: 2.259378\n",
      "Epoch : [168/8999], Training Loss: 2.259308, Validation Loss: 2.259125\n",
      "Epoch : [169/8999], Training Loss: 2.259056, Validation Loss: 2.258872\n",
      "Epoch : [170/8999], Training Loss: 2.258804, Validation Loss: 2.258621\n",
      "Epoch : [171/8999], Training Loss: 2.258554, Validation Loss: 2.258370\n",
      "Epoch : [172/8999], Training Loss: 2.258304, Validation Loss: 2.258120\n",
      "Epoch : [173/8999], Training Loss: 2.258055, Validation Loss: 2.257871\n",
      "Epoch : [174/8999], Training Loss: 2.257807, Validation Loss: 2.257623\n",
      "Epoch : [175/8999], Training Loss: 2.257559, Validation Loss: 2.257375\n",
      "Epoch : [176/8999], Training Loss: 2.257312, Validation Loss: 2.257127\n",
      "Epoch : [177/8999], Training Loss: 2.257066, Validation Loss: 2.256881\n",
      "Epoch : [178/8999], Training Loss: 2.256820, Validation Loss: 2.256635\n",
      "Epoch : [179/8999], Training Loss: 2.256574, Validation Loss: 2.256389\n",
      "Epoch : [180/8999], Training Loss: 2.256330, Validation Loss: 2.256144\n",
      "Epoch : [181/8999], Training Loss: 2.256086, Validation Loss: 2.255900\n",
      "Epoch : [182/8999], Training Loss: 2.255842, Validation Loss: 2.255656\n",
      "Epoch : [183/8999], Training Loss: 2.255599, Validation Loss: 2.255413\n",
      "Epoch : [184/8999], Training Loss: 2.255356, Validation Loss: 2.255170\n",
      "Epoch : [185/8999], Training Loss: 2.255114, Validation Loss: 2.254927\n",
      "Epoch : [186/8999], Training Loss: 2.254872, Validation Loss: 2.254685\n",
      "Epoch : [187/8999], Training Loss: 2.254630, Validation Loss: 2.254443\n",
      "Epoch : [188/8999], Training Loss: 2.254389, Validation Loss: 2.254201\n",
      "Epoch : [189/8999], Training Loss: 2.254148, Validation Loss: 2.253960\n",
      "Epoch : [190/8999], Training Loss: 2.253908, Validation Loss: 2.253720\n",
      "Epoch : [191/8999], Training Loss: 2.253668, Validation Loss: 2.253479\n",
      "Epoch : [192/8999], Training Loss: 2.253428, Validation Loss: 2.253239\n",
      "Epoch : [193/8999], Training Loss: 2.253189, Validation Loss: 2.252999\n",
      "Epoch : [194/8999], Training Loss: 2.252950, Validation Loss: 2.252760\n",
      "Epoch : [195/8999], Training Loss: 2.252711, Validation Loss: 2.252521\n",
      "Epoch : [196/8999], Training Loss: 2.252472, Validation Loss: 2.252282\n",
      "Epoch : [197/8999], Training Loss: 2.252234, Validation Loss: 2.252043\n",
      "Epoch : [198/8999], Training Loss: 2.251996, Validation Loss: 2.251805\n",
      "Epoch : [199/8999], Training Loss: 2.251759, Validation Loss: 2.251567\n",
      "Epoch : [200/8999], Training Loss: 2.251521, Validation Loss: 2.251329\n",
      "Epoch : [201/8999], Training Loss: 2.251284, Validation Loss: 2.251092\n",
      "Epoch : [202/8999], Training Loss: 2.251048, Validation Loss: 2.250855\n",
      "Epoch : [203/8999], Training Loss: 2.250811, Validation Loss: 2.250618\n",
      "Epoch : [204/8999], Training Loss: 2.250575, Validation Loss: 2.250381\n",
      "Epoch : [205/8999], Training Loss: 2.250338, Validation Loss: 2.250144\n",
      "Epoch : [206/8999], Training Loss: 2.250102, Validation Loss: 2.249908\n",
      "Epoch : [207/8999], Training Loss: 2.249867, Validation Loss: 2.249672\n",
      "Epoch : [208/8999], Training Loss: 2.249631, Validation Loss: 2.249435\n",
      "Epoch : [209/8999], Training Loss: 2.249395, Validation Loss: 2.249199\n",
      "Epoch : [210/8999], Training Loss: 2.249160, Validation Loss: 2.248963\n",
      "Epoch : [211/8999], Training Loss: 2.248925, Validation Loss: 2.248728\n",
      "Epoch : [212/8999], Training Loss: 2.248689, Validation Loss: 2.248492\n",
      "Epoch : [213/8999], Training Loss: 2.248454, Validation Loss: 2.248256\n",
      "Epoch : [214/8999], Training Loss: 2.248219, Validation Loss: 2.248020\n",
      "Epoch : [215/8999], Training Loss: 2.247983, Validation Loss: 2.247784\n",
      "Epoch : [216/8999], Training Loss: 2.247748, Validation Loss: 2.247548\n",
      "Epoch : [217/8999], Training Loss: 2.247512, Validation Loss: 2.247312\n",
      "Epoch : [218/8999], Training Loss: 2.247276, Validation Loss: 2.247075\n",
      "Epoch : [219/8999], Training Loss: 2.247040, Validation Loss: 2.246839\n",
      "Epoch : [220/8999], Training Loss: 2.246804, Validation Loss: 2.246601\n",
      "Epoch : [221/8999], Training Loss: 2.246567, Validation Loss: 2.246364\n",
      "Epoch : [222/8999], Training Loss: 2.246330, Validation Loss: 2.246126\n",
      "Epoch : [223/8999], Training Loss: 2.246092, Validation Loss: 2.245887\n",
      "Epoch : [224/8999], Training Loss: 2.245854, Validation Loss: 2.245648\n",
      "Epoch : [225/8999], Training Loss: 2.245615, Validation Loss: 2.245408\n",
      "Epoch : [226/8999], Training Loss: 2.245376, Validation Loss: 2.245168\n",
      "Epoch : [227/8999], Training Loss: 2.245136, Validation Loss: 2.244927\n",
      "Epoch : [228/8999], Training Loss: 2.244894, Validation Loss: 2.244684\n",
      "Epoch : [229/8999], Training Loss: 2.244652, Validation Loss: 2.244441\n",
      "Epoch : [230/8999], Training Loss: 2.244409, Validation Loss: 2.244197\n",
      "Epoch : [231/8999], Training Loss: 2.244165, Validation Loss: 2.243952\n",
      "Epoch : [232/8999], Training Loss: 2.243920, Validation Loss: 2.243705\n",
      "Epoch : [233/8999], Training Loss: 2.243674, Validation Loss: 2.243458\n",
      "Epoch : [234/8999], Training Loss: 2.243426, Validation Loss: 2.243209\n",
      "Epoch : [235/8999], Training Loss: 2.243177, Validation Loss: 2.242958\n",
      "Epoch : [236/8999], Training Loss: 2.242926, Validation Loss: 2.242706\n",
      "Epoch : [237/8999], Training Loss: 2.242675, Validation Loss: 2.242453\n",
      "Epoch : [238/8999], Training Loss: 2.242421, Validation Loss: 2.242198\n",
      "Epoch : [239/8999], Training Loss: 2.242166, Validation Loss: 2.241941\n",
      "Epoch : [240/8999], Training Loss: 2.241910, Validation Loss: 2.241683\n",
      "Epoch : [241/8999], Training Loss: 2.241652, Validation Loss: 2.241424\n",
      "Epoch : [242/8999], Training Loss: 2.241392, Validation Loss: 2.241162\n",
      "Epoch : [243/8999], Training Loss: 2.241131, Validation Loss: 2.240900\n",
      "Epoch : [244/8999], Training Loss: 2.240868, Validation Loss: 2.240635\n",
      "Epoch : [245/8999], Training Loss: 2.240603, Validation Loss: 2.240369\n",
      "Epoch : [246/8999], Training Loss: 2.240337, Validation Loss: 2.240101\n",
      "Epoch : [247/8999], Training Loss: 2.240070, Validation Loss: 2.239832\n",
      "Epoch : [248/8999], Training Loss: 2.239801, Validation Loss: 2.239562\n",
      "Epoch : [249/8999], Training Loss: 2.239530, Validation Loss: 2.239290\n",
      "Epoch : [250/8999], Training Loss: 2.239259, Validation Loss: 2.239017\n",
      "Epoch : [251/8999], Training Loss: 2.238986, Validation Loss: 2.238742\n",
      "Epoch : [252/8999], Training Loss: 2.238711, Validation Loss: 2.238467\n",
      "Epoch : [253/8999], Training Loss: 2.238436, Validation Loss: 2.238191\n",
      "Epoch : [254/8999], Training Loss: 2.238160, Validation Loss: 2.237914\n",
      "Epoch : [255/8999], Training Loss: 2.237884, Validation Loss: 2.237636\n",
      "Epoch : [256/8999], Training Loss: 2.237607, Validation Loss: 2.237358\n",
      "Epoch : [257/8999], Training Loss: 2.237329, Validation Loss: 2.237079\n",
      "Epoch : [258/8999], Training Loss: 2.237051, Validation Loss: 2.236800\n",
      "Epoch : [259/8999], Training Loss: 2.236773, Validation Loss: 2.236521\n",
      "Epoch : [260/8999], Training Loss: 2.236494, Validation Loss: 2.236242\n",
      "Epoch : [261/8999], Training Loss: 2.236216, Validation Loss: 2.235964\n",
      "Epoch : [262/8999], Training Loss: 2.235938, Validation Loss: 2.235685\n",
      "Epoch : [263/8999], Training Loss: 2.235661, Validation Loss: 2.235407\n",
      "Epoch : [264/8999], Training Loss: 2.235384, Validation Loss: 2.235130\n",
      "Epoch : [265/8999], Training Loss: 2.235107, Validation Loss: 2.234853\n",
      "Epoch : [266/8999], Training Loss: 2.234832, Validation Loss: 2.234577\n",
      "Epoch : [267/8999], Training Loss: 2.234556, Validation Loss: 2.234302\n",
      "Epoch : [268/8999], Training Loss: 2.234282, Validation Loss: 2.234028\n",
      "Epoch : [269/8999], Training Loss: 2.234009, Validation Loss: 2.233754\n",
      "Epoch : [270/8999], Training Loss: 2.233736, Validation Loss: 2.233482\n",
      "Epoch : [271/8999], Training Loss: 2.233465, Validation Loss: 2.233210\n",
      "Epoch : [272/8999], Training Loss: 2.233194, Validation Loss: 2.232939\n",
      "Epoch : [273/8999], Training Loss: 2.232925, Validation Loss: 2.232669\n",
      "Epoch : [274/8999], Training Loss: 2.232656, Validation Loss: 2.232401\n",
      "Epoch : [275/8999], Training Loss: 2.232388, Validation Loss: 2.232133\n",
      "Epoch : [276/8999], Training Loss: 2.232121, Validation Loss: 2.231866\n",
      "Epoch : [277/8999], Training Loss: 2.231856, Validation Loss: 2.231600\n",
      "Epoch : [278/8999], Training Loss: 2.231591, Validation Loss: 2.231335\n",
      "Epoch : [279/8999], Training Loss: 2.231327, Validation Loss: 2.231070\n",
      "Epoch : [280/8999], Training Loss: 2.231063, Validation Loss: 2.230807\n",
      "Epoch : [281/8999], Training Loss: 2.230801, Validation Loss: 2.230544\n",
      "Epoch : [282/8999], Training Loss: 2.230540, Validation Loss: 2.230282\n",
      "Epoch : [283/8999], Training Loss: 2.230279, Validation Loss: 2.230021\n",
      "Epoch : [284/8999], Training Loss: 2.230019, Validation Loss: 2.229761\n",
      "Epoch : [285/8999], Training Loss: 2.229760, Validation Loss: 2.229501\n",
      "Epoch : [286/8999], Training Loss: 2.229501, Validation Loss: 2.229242\n",
      "Epoch : [287/8999], Training Loss: 2.229244, Validation Loss: 2.228984\n",
      "Epoch : [288/8999], Training Loss: 2.228987, Validation Loss: 2.228726\n",
      "Epoch : [289/8999], Training Loss: 2.228730, Validation Loss: 2.228469\n",
      "Epoch : [290/8999], Training Loss: 2.228474, Validation Loss: 2.228213\n",
      "Epoch : [291/8999], Training Loss: 2.228219, Validation Loss: 2.227957\n",
      "Epoch : [292/8999], Training Loss: 2.227965, Validation Loss: 2.227702\n",
      "Epoch : [293/8999], Training Loss: 2.227711, Validation Loss: 2.227447\n",
      "Epoch : [294/8999], Training Loss: 2.227458, Validation Loss: 2.227193\n",
      "Epoch : [295/8999], Training Loss: 2.227205, Validation Loss: 2.226940\n",
      "Epoch : [296/8999], Training Loss: 2.226953, Validation Loss: 2.226687\n",
      "Epoch : [297/8999], Training Loss: 2.226702, Validation Loss: 2.226435\n",
      "Epoch : [298/8999], Training Loss: 2.226451, Validation Loss: 2.226183\n",
      "Epoch : [299/8999], Training Loss: 2.226201, Validation Loss: 2.225932\n",
      "Epoch : [300/8999], Training Loss: 2.225952, Validation Loss: 2.225682\n",
      "Epoch : [301/8999], Training Loss: 2.225703, Validation Loss: 2.225432\n",
      "Epoch : [302/8999], Training Loss: 2.225454, Validation Loss: 2.225183\n",
      "Epoch : [303/8999], Training Loss: 2.225207, Validation Loss: 2.224935\n",
      "Epoch : [304/8999], Training Loss: 2.224960, Validation Loss: 2.224687\n",
      "Epoch : [305/8999], Training Loss: 2.224713, Validation Loss: 2.224439\n",
      "Epoch : [306/8999], Training Loss: 2.224468, Validation Loss: 2.224192\n",
      "Epoch : [307/8999], Training Loss: 2.224222, Validation Loss: 2.223946\n",
      "Epoch : [308/8999], Training Loss: 2.223978, Validation Loss: 2.223701\n",
      "Epoch : [309/8999], Training Loss: 2.223734, Validation Loss: 2.223455\n",
      "Epoch : [310/8999], Training Loss: 2.223490, Validation Loss: 2.223211\n",
      "Epoch : [311/8999], Training Loss: 2.223247, Validation Loss: 2.222967\n",
      "Epoch : [312/8999], Training Loss: 2.223005, Validation Loss: 2.222724\n",
      "Epoch : [313/8999], Training Loss: 2.222764, Validation Loss: 2.222481\n",
      "Epoch : [314/8999], Training Loss: 2.222523, Validation Loss: 2.222239\n",
      "Epoch : [315/8999], Training Loss: 2.222282, Validation Loss: 2.221997\n",
      "Epoch : [316/8999], Training Loss: 2.222042, Validation Loss: 2.221756\n",
      "Epoch : [317/8999], Training Loss: 2.221803, Validation Loss: 2.221515\n",
      "Epoch : [318/8999], Training Loss: 2.221564, Validation Loss: 2.221276\n",
      "Epoch : [319/8999], Training Loss: 2.221326, Validation Loss: 2.221036\n",
      "Epoch : [320/8999], Training Loss: 2.221088, Validation Loss: 2.220797\n",
      "Epoch : [321/8999], Training Loss: 2.220851, Validation Loss: 2.220559\n",
      "Epoch : [322/8999], Training Loss: 2.220614, Validation Loss: 2.220321\n",
      "Epoch : [323/8999], Training Loss: 2.220379, Validation Loss: 2.220084\n",
      "Epoch : [324/8999], Training Loss: 2.220143, Validation Loss: 2.219847\n",
      "Epoch : [325/8999], Training Loss: 2.219908, Validation Loss: 2.219611\n",
      "Epoch : [326/8999], Training Loss: 2.219674, Validation Loss: 2.219376\n",
      "Epoch : [327/8999], Training Loss: 2.219440, Validation Loss: 2.219140\n",
      "Epoch : [328/8999], Training Loss: 2.219207, Validation Loss: 2.218906\n",
      "Epoch : [329/8999], Training Loss: 2.218974, Validation Loss: 2.218672\n",
      "Epoch : [330/8999], Training Loss: 2.218742, Validation Loss: 2.218438\n",
      "Epoch : [331/8999], Training Loss: 2.218510, Validation Loss: 2.218205\n",
      "Epoch : [332/8999], Training Loss: 2.218279, Validation Loss: 2.217973\n",
      "Epoch : [333/8999], Training Loss: 2.218049, Validation Loss: 2.217741\n",
      "Epoch : [334/8999], Training Loss: 2.217818, Validation Loss: 2.217510\n",
      "Epoch : [335/8999], Training Loss: 2.217589, Validation Loss: 2.217279\n",
      "Epoch : [336/8999], Training Loss: 2.217360, Validation Loss: 2.217049\n",
      "Epoch : [337/8999], Training Loss: 2.217132, Validation Loss: 2.216819\n",
      "Epoch : [338/8999], Training Loss: 2.216904, Validation Loss: 2.216590\n",
      "Epoch : [339/8999], Training Loss: 2.216676, Validation Loss: 2.216361\n",
      "Epoch : [340/8999], Training Loss: 2.216450, Validation Loss: 2.216133\n",
      "Epoch : [341/8999], Training Loss: 2.216224, Validation Loss: 2.215906\n",
      "Epoch : [342/8999], Training Loss: 2.215998, Validation Loss: 2.215679\n",
      "Epoch : [343/8999], Training Loss: 2.215773, Validation Loss: 2.215452\n",
      "Epoch : [344/8999], Training Loss: 2.215548, Validation Loss: 2.215227\n",
      "Epoch : [345/8999], Training Loss: 2.215325, Validation Loss: 2.215002\n",
      "Epoch : [346/8999], Training Loss: 2.215101, Validation Loss: 2.214777\n",
      "Epoch : [347/8999], Training Loss: 2.214879, Validation Loss: 2.214553\n",
      "Epoch : [348/8999], Training Loss: 2.214656, Validation Loss: 2.214330\n",
      "Epoch : [349/8999], Training Loss: 2.214435, Validation Loss: 2.214107\n",
      "Epoch : [350/8999], Training Loss: 2.214214, Validation Loss: 2.213885\n",
      "Epoch : [351/8999], Training Loss: 2.213994, Validation Loss: 2.213663\n",
      "Epoch : [352/8999], Training Loss: 2.213774, Validation Loss: 2.213442\n",
      "Epoch : [353/8999], Training Loss: 2.213555, Validation Loss: 2.213222\n",
      "Epoch : [354/8999], Training Loss: 2.213336, Validation Loss: 2.213002\n",
      "Epoch : [355/8999], Training Loss: 2.213119, Validation Loss: 2.212783\n",
      "Epoch : [356/8999], Training Loss: 2.212901, Validation Loss: 2.212565\n",
      "Epoch : [357/8999], Training Loss: 2.212685, Validation Loss: 2.212347\n",
      "Epoch : [358/8999], Training Loss: 2.212469, Validation Loss: 2.212130\n",
      "Epoch : [359/8999], Training Loss: 2.212254, Validation Loss: 2.211913\n",
      "Epoch : [360/8999], Training Loss: 2.212039, Validation Loss: 2.211697\n",
      "Epoch : [361/8999], Training Loss: 2.211825, Validation Loss: 2.211482\n",
      "Epoch : [362/8999], Training Loss: 2.211611, Validation Loss: 2.211268\n",
      "Epoch : [363/8999], Training Loss: 2.211398, Validation Loss: 2.211054\n",
      "Epoch : [364/8999], Training Loss: 2.211186, Validation Loss: 2.210840\n",
      "Epoch : [365/8999], Training Loss: 2.210975, Validation Loss: 2.210628\n",
      "Epoch : [366/8999], Training Loss: 2.210764, Validation Loss: 2.210416\n",
      "Epoch : [367/8999], Training Loss: 2.210554, Validation Loss: 2.210204\n",
      "Epoch : [368/8999], Training Loss: 2.210344, Validation Loss: 2.209993\n",
      "Epoch : [369/8999], Training Loss: 2.210135, Validation Loss: 2.209783\n",
      "Epoch : [370/8999], Training Loss: 2.209926, Validation Loss: 2.209573\n",
      "Epoch : [371/8999], Training Loss: 2.209718, Validation Loss: 2.209364\n",
      "Epoch : [372/8999], Training Loss: 2.209511, Validation Loss: 2.209155\n",
      "Epoch : [373/8999], Training Loss: 2.209304, Validation Loss: 2.208947\n",
      "Epoch : [374/8999], Training Loss: 2.209097, Validation Loss: 2.208740\n",
      "Epoch : [375/8999], Training Loss: 2.208891, Validation Loss: 2.208532\n",
      "Epoch : [376/8999], Training Loss: 2.208685, Validation Loss: 2.208326\n",
      "Epoch : [377/8999], Training Loss: 2.208480, Validation Loss: 2.208119\n",
      "Epoch : [378/8999], Training Loss: 2.208275, Validation Loss: 2.207913\n",
      "Epoch : [379/8999], Training Loss: 2.208070, Validation Loss: 2.207707\n",
      "Epoch : [380/8999], Training Loss: 2.207866, Validation Loss: 2.207501\n",
      "Epoch : [381/8999], Training Loss: 2.207662, Validation Loss: 2.207295\n",
      "Epoch : [382/8999], Training Loss: 2.207457, Validation Loss: 2.207089\n",
      "Epoch : [383/8999], Training Loss: 2.207253, Validation Loss: 2.206883\n",
      "Epoch : [384/8999], Training Loss: 2.207049, Validation Loss: 2.206677\n",
      "Epoch : [385/8999], Training Loss: 2.206844, Validation Loss: 2.206471\n",
      "Epoch : [386/8999], Training Loss: 2.206639, Validation Loss: 2.206264\n",
      "Epoch : [387/8999], Training Loss: 2.206434, Validation Loss: 2.206057\n",
      "Epoch : [388/8999], Training Loss: 2.206228, Validation Loss: 2.205849\n",
      "Epoch : [389/8999], Training Loss: 2.206022, Validation Loss: 2.205641\n",
      "Epoch : [390/8999], Training Loss: 2.205814, Validation Loss: 2.205431\n",
      "Epoch : [391/8999], Training Loss: 2.205606, Validation Loss: 2.205220\n",
      "Epoch : [392/8999], Training Loss: 2.205397, Validation Loss: 2.205008\n",
      "Epoch : [393/8999], Training Loss: 2.205186, Validation Loss: 2.204795\n",
      "Epoch : [394/8999], Training Loss: 2.204974, Validation Loss: 2.204580\n",
      "Epoch : [395/8999], Training Loss: 2.204759, Validation Loss: 2.204362\n",
      "Epoch : [396/8999], Training Loss: 2.204543, Validation Loss: 2.204143\n",
      "Epoch : [397/8999], Training Loss: 2.204325, Validation Loss: 2.203921\n",
      "Epoch : [398/8999], Training Loss: 2.204104, Validation Loss: 2.203696\n",
      "Epoch : [399/8999], Training Loss: 2.203880, Validation Loss: 2.203468\n",
      "Epoch : [400/8999], Training Loss: 2.203652, Validation Loss: 2.203236\n",
      "Epoch : [401/8999], Training Loss: 2.203421, Validation Loss: 2.203001\n",
      "Epoch : [402/8999], Training Loss: 2.203186, Validation Loss: 2.202761\n",
      "Epoch : [403/8999], Training Loss: 2.202947, Validation Loss: 2.202517\n",
      "Epoch : [404/8999], Training Loss: 2.202703, Validation Loss: 2.202268\n",
      "Epoch : [405/8999], Training Loss: 2.202454, Validation Loss: 2.202014\n",
      "Epoch : [406/8999], Training Loss: 2.202201, Validation Loss: 2.201755\n",
      "Epoch : [407/8999], Training Loss: 2.201942, Validation Loss: 2.201491\n",
      "Epoch : [408/8999], Training Loss: 2.201679, Validation Loss: 2.201224\n",
      "Epoch : [409/8999], Training Loss: 2.201412, Validation Loss: 2.200952\n",
      "Epoch : [410/8999], Training Loss: 2.201142, Validation Loss: 2.200679\n",
      "Epoch : [411/8999], Training Loss: 2.200870, Validation Loss: 2.200403\n",
      "Epoch : [412/8999], Training Loss: 2.200596, Validation Loss: 2.200128\n",
      "Epoch : [413/8999], Training Loss: 2.200323, Validation Loss: 2.199854\n",
      "Epoch : [414/8999], Training Loss: 2.200051, Validation Loss: 2.199582\n",
      "Epoch : [415/8999], Training Loss: 2.199782, Validation Loss: 2.199313\n",
      "Epoch : [416/8999], Training Loss: 2.199516, Validation Loss: 2.199049\n",
      "Epoch : [417/8999], Training Loss: 2.199254, Validation Loss: 2.198788\n",
      "Epoch : [418/8999], Training Loss: 2.198997, Validation Loss: 2.198533\n",
      "Epoch : [419/8999], Training Loss: 2.198743, Validation Loss: 2.198282\n",
      "Epoch : [420/8999], Training Loss: 2.198495, Validation Loss: 2.198035\n",
      "Epoch : [421/8999], Training Loss: 2.198250, Validation Loss: 2.197793\n",
      "Epoch : [422/8999], Training Loss: 2.198009, Validation Loss: 2.197554\n",
      "Epoch : [423/8999], Training Loss: 2.197772, Validation Loss: 2.197319\n",
      "Epoch : [424/8999], Training Loss: 2.197538, Validation Loss: 2.197088\n",
      "Epoch : [425/8999], Training Loss: 2.197307, Validation Loss: 2.196859\n",
      "Epoch : [426/8999], Training Loss: 2.197079, Validation Loss: 2.196632\n",
      "Epoch : [427/8999], Training Loss: 2.196853, Validation Loss: 2.196408\n",
      "Epoch : [428/8999], Training Loss: 2.196629, Validation Loss: 2.196186\n",
      "Epoch : [429/8999], Training Loss: 2.196408, Validation Loss: 2.195966\n",
      "Epoch : [430/8999], Training Loss: 2.196188, Validation Loss: 2.195747\n",
      "Epoch : [431/8999], Training Loss: 2.195970, Validation Loss: 2.195531\n",
      "Epoch : [432/8999], Training Loss: 2.195754, Validation Loss: 2.195316\n",
      "Epoch : [433/8999], Training Loss: 2.195540, Validation Loss: 2.195103\n",
      "Epoch : [434/8999], Training Loss: 2.195327, Validation Loss: 2.194891\n",
      "Epoch : [435/8999], Training Loss: 2.195116, Validation Loss: 2.194680\n",
      "Epoch : [436/8999], Training Loss: 2.194906, Validation Loss: 2.194470\n",
      "Epoch : [437/8999], Training Loss: 2.194698, Validation Loss: 2.194262\n",
      "Epoch : [438/8999], Training Loss: 2.194491, Validation Loss: 2.194055\n",
      "Epoch : [439/8999], Training Loss: 2.194285, Validation Loss: 2.193849\n",
      "Epoch : [440/8999], Training Loss: 2.194081, Validation Loss: 2.193643\n",
      "Epoch : [441/8999], Training Loss: 2.193877, Validation Loss: 2.193438\n",
      "Epoch : [442/8999], Training Loss: 2.193674, Validation Loss: 2.193233\n",
      "Epoch : [443/8999], Training Loss: 2.193472, Validation Loss: 2.193029\n",
      "Epoch : [444/8999], Training Loss: 2.193270, Validation Loss: 2.192824\n",
      "Epoch : [445/8999], Training Loss: 2.193068, Validation Loss: 2.192618\n",
      "Epoch : [446/8999], Training Loss: 2.192866, Validation Loss: 2.192412\n",
      "Epoch : [447/8999], Training Loss: 2.192663, Validation Loss: 2.192204\n",
      "Epoch : [448/8999], Training Loss: 2.192459, Validation Loss: 2.191995\n",
      "Epoch : [449/8999], Training Loss: 2.192254, Validation Loss: 2.191783\n",
      "Epoch : [450/8999], Training Loss: 2.192048, Validation Loss: 2.191571\n",
      "Epoch : [451/8999], Training Loss: 2.191839, Validation Loss: 2.191356\n",
      "Epoch : [452/8999], Training Loss: 2.191628, Validation Loss: 2.191140\n",
      "Epoch : [453/8999], Training Loss: 2.191415, Validation Loss: 2.190924\n",
      "Epoch : [454/8999], Training Loss: 2.191200, Validation Loss: 2.190708\n",
      "Epoch : [455/8999], Training Loss: 2.190984, Validation Loss: 2.190494\n",
      "Epoch : [456/8999], Training Loss: 2.190768, Validation Loss: 2.190282\n",
      "Epoch : [457/8999], Training Loss: 2.190553, Validation Loss: 2.190073\n",
      "Epoch : [458/8999], Training Loss: 2.190341, Validation Loss: 2.189868\n",
      "Epoch : [459/8999], Training Loss: 2.190131, Validation Loss: 2.189665\n",
      "Epoch : [460/8999], Training Loss: 2.189925, Validation Loss: 2.189466\n",
      "Epoch : [461/8999], Training Loss: 2.189723, Validation Loss: 2.189270\n",
      "Epoch : [462/8999], Training Loss: 2.189525, Validation Loss: 2.189076\n",
      "Epoch : [463/8999], Training Loss: 2.189330, Validation Loss: 2.188885\n",
      "Epoch : [464/8999], Training Loss: 2.189138, Validation Loss: 2.188696\n",
      "Epoch : [465/8999], Training Loss: 2.188948, Validation Loss: 2.188509\n",
      "Epoch : [466/8999], Training Loss: 2.188761, Validation Loss: 2.188323\n",
      "Epoch : [467/8999], Training Loss: 2.188576, Validation Loss: 2.188138\n",
      "Epoch : [468/8999], Training Loss: 2.188393, Validation Loss: 2.187955\n",
      "Epoch : [469/8999], Training Loss: 2.188211, Validation Loss: 2.187772\n",
      "Epoch : [470/8999], Training Loss: 2.188030, Validation Loss: 2.187591\n",
      "Epoch : [471/8999], Training Loss: 2.187851, Validation Loss: 2.187410\n",
      "Epoch : [472/8999], Training Loss: 2.187672, Validation Loss: 2.187229\n",
      "Epoch : [473/8999], Training Loss: 2.187494, Validation Loss: 2.187049\n",
      "Epoch : [474/8999], Training Loss: 2.187317, Validation Loss: 2.186870\n",
      "Epoch : [475/8999], Training Loss: 2.187141, Validation Loss: 2.186691\n",
      "Epoch : [476/8999], Training Loss: 2.186964, Validation Loss: 2.186512\n",
      "Epoch : [477/8999], Training Loss: 2.186789, Validation Loss: 2.186334\n",
      "Epoch : [478/8999], Training Loss: 2.186613, Validation Loss: 2.186156\n",
      "Epoch : [479/8999], Training Loss: 2.186438, Validation Loss: 2.185978\n",
      "Epoch : [480/8999], Training Loss: 2.186264, Validation Loss: 2.185800\n",
      "Epoch : [481/8999], Training Loss: 2.186089, Validation Loss: 2.185623\n",
      "Epoch : [482/8999], Training Loss: 2.185915, Validation Loss: 2.185446\n",
      "Epoch : [483/8999], Training Loss: 2.185741, Validation Loss: 2.185269\n",
      "Epoch : [484/8999], Training Loss: 2.185567, Validation Loss: 2.185093\n",
      "Epoch : [485/8999], Training Loss: 2.185394, Validation Loss: 2.184916\n",
      "Epoch : [486/8999], Training Loss: 2.185221, Validation Loss: 2.184740\n",
      "Epoch : [487/8999], Training Loss: 2.185048, Validation Loss: 2.184563\n",
      "Epoch : [488/8999], Training Loss: 2.184875, Validation Loss: 2.184387\n",
      "Epoch : [489/8999], Training Loss: 2.184702, Validation Loss: 2.184211\n",
      "Epoch : [490/8999], Training Loss: 2.184529, Validation Loss: 2.184034\n",
      "Epoch : [491/8999], Training Loss: 2.184356, Validation Loss: 2.183858\n",
      "Epoch : [492/8999], Training Loss: 2.184183, Validation Loss: 2.183681\n",
      "Epoch : [493/8999], Training Loss: 2.184009, Validation Loss: 2.183503\n",
      "Epoch : [494/8999], Training Loss: 2.183836, Validation Loss: 2.183326\n",
      "Epoch : [495/8999], Training Loss: 2.183662, Validation Loss: 2.183148\n",
      "Epoch : [496/8999], Training Loss: 2.183488, Validation Loss: 2.182969\n",
      "Epoch : [497/8999], Training Loss: 2.183313, Validation Loss: 2.182789\n",
      "Epoch : [498/8999], Training Loss: 2.183138, Validation Loss: 2.182609\n",
      "Epoch : [499/8999], Training Loss: 2.182962, Validation Loss: 2.182428\n",
      "Epoch : [500/8999], Training Loss: 2.182785, Validation Loss: 2.182246\n",
      "Epoch : [501/8999], Training Loss: 2.182608, Validation Loss: 2.182063\n",
      "Epoch : [502/8999], Training Loss: 2.182429, Validation Loss: 2.181878\n",
      "Epoch : [503/8999], Training Loss: 2.182250, Validation Loss: 2.181693\n",
      "Epoch : [504/8999], Training Loss: 2.182069, Validation Loss: 2.181506\n",
      "Epoch : [505/8999], Training Loss: 2.181887, Validation Loss: 2.181317\n",
      "Epoch : [506/8999], Training Loss: 2.181704, Validation Loss: 2.181127\n",
      "Epoch : [507/8999], Training Loss: 2.181519, Validation Loss: 2.180934\n",
      "Epoch : [508/8999], Training Loss: 2.181333, Validation Loss: 2.180740\n",
      "Epoch : [509/8999], Training Loss: 2.181144, Validation Loss: 2.180544\n",
      "Epoch : [510/8999], Training Loss: 2.180954, Validation Loss: 2.180345\n",
      "Epoch : [511/8999], Training Loss: 2.180761, Validation Loss: 2.180144\n",
      "Epoch : [512/8999], Training Loss: 2.180566, Validation Loss: 2.179940\n",
      "Epoch : [513/8999], Training Loss: 2.180367, Validation Loss: 2.179733\n",
      "Epoch : [514/8999], Training Loss: 2.180166, Validation Loss: 2.179524\n",
      "Epoch : [515/8999], Training Loss: 2.179960, Validation Loss: 2.179311\n",
      "Epoch : [516/8999], Training Loss: 2.179751, Validation Loss: 2.179095\n",
      "Epoch : [517/8999], Training Loss: 2.179538, Validation Loss: 2.178875\n",
      "Epoch : [518/8999], Training Loss: 2.179320, Validation Loss: 2.178652\n",
      "Epoch : [519/8999], Training Loss: 2.179099, Validation Loss: 2.178425\n",
      "Epoch : [520/8999], Training Loss: 2.178874, Validation Loss: 2.178194\n",
      "Epoch : [521/8999], Training Loss: 2.178646, Validation Loss: 2.177959\n",
      "Epoch : [522/8999], Training Loss: 2.178414, Validation Loss: 2.177720\n",
      "Epoch : [523/8999], Training Loss: 2.178179, Validation Loss: 2.177477\n",
      "Epoch : [524/8999], Training Loss: 2.177941, Validation Loss: 2.177231\n",
      "Epoch : [525/8999], Training Loss: 2.177699, Validation Loss: 2.176980\n",
      "Epoch : [526/8999], Training Loss: 2.177455, Validation Loss: 2.176726\n",
      "Epoch : [527/8999], Training Loss: 2.177207, Validation Loss: 2.176467\n",
      "Epoch : [528/8999], Training Loss: 2.176956, Validation Loss: 2.176206\n",
      "Epoch : [529/8999], Training Loss: 2.176702, Validation Loss: 2.175941\n",
      "Epoch : [530/8999], Training Loss: 2.176446, Validation Loss: 2.175673\n",
      "Epoch : [531/8999], Training Loss: 2.176187, Validation Loss: 2.175403\n",
      "Epoch : [532/8999], Training Loss: 2.175926, Validation Loss: 2.175131\n",
      "Epoch : [533/8999], Training Loss: 2.175663, Validation Loss: 2.174857\n",
      "Epoch : [534/8999], Training Loss: 2.175400, Validation Loss: 2.174582\n",
      "Epoch : [535/8999], Training Loss: 2.175135, Validation Loss: 2.174306\n",
      "Epoch : [536/8999], Training Loss: 2.174870, Validation Loss: 2.174031\n",
      "Epoch : [537/8999], Training Loss: 2.174606, Validation Loss: 2.173755\n",
      "Epoch : [538/8999], Training Loss: 2.174342, Validation Loss: 2.173480\n",
      "Epoch : [539/8999], Training Loss: 2.174078, Validation Loss: 2.173206\n",
      "Epoch : [540/8999], Training Loss: 2.173816, Validation Loss: 2.172933\n",
      "Epoch : [541/8999], Training Loss: 2.173555, Validation Loss: 2.172662\n",
      "Epoch : [542/8999], Training Loss: 2.173296, Validation Loss: 2.172392\n",
      "Epoch : [543/8999], Training Loss: 2.173038, Validation Loss: 2.172123\n",
      "Epoch : [544/8999], Training Loss: 2.172781, Validation Loss: 2.171855\n",
      "Epoch : [545/8999], Training Loss: 2.172525, Validation Loss: 2.171587\n",
      "Epoch : [546/8999], Training Loss: 2.172269, Validation Loss: 2.171319\n",
      "Epoch : [547/8999], Training Loss: 2.172014, Validation Loss: 2.171051\n",
      "Epoch : [548/8999], Training Loss: 2.171758, Validation Loss: 2.170780\n",
      "Epoch : [549/8999], Training Loss: 2.171501, Validation Loss: 2.170506\n",
      "Epoch : [550/8999], Training Loss: 2.171240, Validation Loss: 2.170227\n",
      "Epoch : [551/8999], Training Loss: 2.170975, Validation Loss: 2.169941\n",
      "Epoch : [552/8999], Training Loss: 2.170704, Validation Loss: 2.169646\n",
      "Epoch : [553/8999], Training Loss: 2.170427, Validation Loss: 2.169342\n",
      "Epoch : [554/8999], Training Loss: 2.170141, Validation Loss: 2.169029\n",
      "Epoch : [555/8999], Training Loss: 2.169851, Validation Loss: 2.168709\n",
      "Epoch : [556/8999], Training Loss: 2.169557, Validation Loss: 2.168390\n",
      "Epoch : [557/8999], Training Loss: 2.169265, Validation Loss: 2.168077\n",
      "Epoch : [558/8999], Training Loss: 2.168977, Validation Loss: 2.167775\n",
      "Epoch : [559/8999], Training Loss: 2.168697, Validation Loss: 2.167484\n",
      "Epoch : [560/8999], Training Loss: 2.168421, Validation Loss: 2.167202\n",
      "Epoch : [561/8999], Training Loss: 2.168151, Validation Loss: 2.166929\n",
      "Epoch : [562/8999], Training Loss: 2.167885, Validation Loss: 2.166662\n",
      "Epoch : [563/8999], Training Loss: 2.167626, Validation Loss: 2.166400\n",
      "Epoch : [564/8999], Training Loss: 2.167375, Validation Loss: 2.166143\n",
      "Epoch : [565/8999], Training Loss: 2.167128, Validation Loss: 2.165887\n",
      "Epoch : [566/8999], Training Loss: 2.166880, Validation Loss: 2.165632\n",
      "Epoch : [567/8999], Training Loss: 2.166630, Validation Loss: 2.165376\n",
      "Epoch : [568/8999], Training Loss: 2.166381, Validation Loss: 2.165119\n",
      "Epoch : [569/8999], Training Loss: 2.166134, Validation Loss: 2.164862\n",
      "Epoch : [570/8999], Training Loss: 2.165887, Validation Loss: 2.164604\n",
      "Epoch : [571/8999], Training Loss: 2.165638, Validation Loss: 2.164343\n",
      "Epoch : [572/8999], Training Loss: 2.165388, Validation Loss: 2.164081\n",
      "Epoch : [573/8999], Training Loss: 2.165139, Validation Loss: 2.163818\n",
      "Epoch : [574/8999], Training Loss: 2.164893, Validation Loss: 2.163553\n",
      "Epoch : [575/8999], Training Loss: 2.164651, Validation Loss: 2.163287\n",
      "Epoch : [576/8999], Training Loss: 2.164410, Validation Loss: 2.163021\n",
      "Epoch : [577/8999], Training Loss: 2.164168, Validation Loss: 2.162755\n",
      "Epoch : [578/8999], Training Loss: 2.163925, Validation Loss: 2.162491\n",
      "Epoch : [579/8999], Training Loss: 2.163681, Validation Loss: 2.162228\n",
      "Epoch : [580/8999], Training Loss: 2.163434, Validation Loss: 2.161967\n",
      "Epoch : [581/8999], Training Loss: 2.163183, Validation Loss: 2.161708\n",
      "Epoch : [582/8999], Training Loss: 2.162925, Validation Loss: 2.161450\n",
      "Epoch : [583/8999], Training Loss: 2.162660, Validation Loss: 2.161189\n",
      "Epoch : [584/8999], Training Loss: 2.162385, Validation Loss: 2.160924\n",
      "Epoch : [585/8999], Training Loss: 2.162100, Validation Loss: 2.160653\n",
      "Epoch : [586/8999], Training Loss: 2.161806, Validation Loss: 2.160376\n",
      "Epoch : [587/8999], Training Loss: 2.161505, Validation Loss: 2.160093\n",
      "Epoch : [588/8999], Training Loss: 2.161199, Validation Loss: 2.159806\n",
      "Epoch : [589/8999], Training Loss: 2.160892, Validation Loss: 2.159516\n",
      "Epoch : [590/8999], Training Loss: 2.160587, Validation Loss: 2.159226\n",
      "Epoch : [591/8999], Training Loss: 2.160286, Validation Loss: 2.158935\n",
      "Epoch : [592/8999], Training Loss: 2.159989, Validation Loss: 2.158647\n",
      "Epoch : [593/8999], Training Loss: 2.159696, Validation Loss: 2.158359\n",
      "Epoch : [594/8999], Training Loss: 2.159408, Validation Loss: 2.158073\n",
      "Epoch : [595/8999], Training Loss: 2.159123, Validation Loss: 2.157789\n",
      "Epoch : [596/8999], Training Loss: 2.158842, Validation Loss: 2.157506\n",
      "Epoch : [597/8999], Training Loss: 2.158563, Validation Loss: 2.157224\n",
      "Epoch : [598/8999], Training Loss: 2.158287, Validation Loss: 2.156942\n",
      "Epoch : [599/8999], Training Loss: 2.158013, Validation Loss: 2.156662\n",
      "Epoch : [600/8999], Training Loss: 2.157740, Validation Loss: 2.156381\n",
      "Epoch : [601/8999], Training Loss: 2.157468, Validation Loss: 2.156101\n",
      "Epoch : [602/8999], Training Loss: 2.157196, Validation Loss: 2.155821\n",
      "Epoch : [603/8999], Training Loss: 2.156925, Validation Loss: 2.155540\n",
      "Epoch : [604/8999], Training Loss: 2.156654, Validation Loss: 2.155259\n",
      "Epoch : [605/8999], Training Loss: 2.156383, Validation Loss: 2.154978\n",
      "Epoch : [606/8999], Training Loss: 2.156111, Validation Loss: 2.154697\n",
      "Epoch : [607/8999], Training Loss: 2.155839, Validation Loss: 2.154414\n",
      "Epoch : [608/8999], Training Loss: 2.155567, Validation Loss: 2.154131\n",
      "Epoch : [609/8999], Training Loss: 2.155294, Validation Loss: 2.153848\n",
      "Epoch : [610/8999], Training Loss: 2.155021, Validation Loss: 2.153564\n",
      "Epoch : [611/8999], Training Loss: 2.154747, Validation Loss: 2.153280\n",
      "Epoch : [612/8999], Training Loss: 2.154472, Validation Loss: 2.152995\n",
      "Epoch : [613/8999], Training Loss: 2.154196, Validation Loss: 2.152710\n",
      "Epoch : [614/8999], Training Loss: 2.153920, Validation Loss: 2.152425\n",
      "Epoch : [615/8999], Training Loss: 2.153644, Validation Loss: 2.152139\n",
      "Epoch : [616/8999], Training Loss: 2.153367, Validation Loss: 2.151854\n",
      "Epoch : [617/8999], Training Loss: 2.153089, Validation Loss: 2.151569\n",
      "Epoch : [618/8999], Training Loss: 2.152811, Validation Loss: 2.151283\n",
      "Epoch : [619/8999], Training Loss: 2.152532, Validation Loss: 2.150998\n",
      "Epoch : [620/8999], Training Loss: 2.152252, Validation Loss: 2.150714\n",
      "Epoch : [621/8999], Training Loss: 2.151973, Validation Loss: 2.150429\n",
      "Epoch : [622/8999], Training Loss: 2.151692, Validation Loss: 2.150146\n",
      "Epoch : [623/8999], Training Loss: 2.151412, Validation Loss: 2.149863\n",
      "Epoch : [624/8999], Training Loss: 2.151131, Validation Loss: 2.149580\n",
      "Epoch : [625/8999], Training Loss: 2.150849, Validation Loss: 2.149299\n",
      "Epoch : [626/8999], Training Loss: 2.150568, Validation Loss: 2.149020\n",
      "Epoch : [627/8999], Training Loss: 2.150286, Validation Loss: 2.148741\n",
      "Epoch : [628/8999], Training Loss: 2.150003, Validation Loss: 2.148465\n",
      "Epoch : [629/8999], Training Loss: 2.149720, Validation Loss: 2.148190\n",
      "Epoch : [630/8999], Training Loss: 2.149436, Validation Loss: 2.147916\n",
      "Epoch : [631/8999], Training Loss: 2.149151, Validation Loss: 2.147645\n",
      "Epoch : [632/8999], Training Loss: 2.148865, Validation Loss: 2.147376\n",
      "Epoch : [633/8999], Training Loss: 2.148578, Validation Loss: 2.147108\n",
      "Epoch : [634/8999], Training Loss: 2.148289, Validation Loss: 2.146842\n",
      "Epoch : [635/8999], Training Loss: 2.147999, Validation Loss: 2.146577\n",
      "Epoch : [636/8999], Training Loss: 2.147707, Validation Loss: 2.146314\n",
      "Epoch : [637/8999], Training Loss: 2.147412, Validation Loss: 2.146051\n",
      "Epoch : [638/8999], Training Loss: 2.147115, Validation Loss: 2.145788\n",
      "Epoch : [639/8999], Training Loss: 2.146816, Validation Loss: 2.145525\n",
      "Epoch : [640/8999], Training Loss: 2.146515, Validation Loss: 2.145261\n",
      "Epoch : [641/8999], Training Loss: 2.146211, Validation Loss: 2.144996\n",
      "Epoch : [642/8999], Training Loss: 2.145904, Validation Loss: 2.144729\n",
      "Epoch : [643/8999], Training Loss: 2.145596, Validation Loss: 2.144459\n",
      "Epoch : [644/8999], Training Loss: 2.145286, Validation Loss: 2.144187\n",
      "Epoch : [645/8999], Training Loss: 2.144975, Validation Loss: 2.143913\n",
      "Epoch : [646/8999], Training Loss: 2.144663, Validation Loss: 2.143636\n",
      "Epoch : [647/8999], Training Loss: 2.144352, Validation Loss: 2.143358\n",
      "Epoch : [648/8999], Training Loss: 2.144040, Validation Loss: 2.143079\n",
      "Epoch : [649/8999], Training Loss: 2.143730, Validation Loss: 2.142798\n",
      "Epoch : [650/8999], Training Loss: 2.143421, Validation Loss: 2.142517\n",
      "Epoch : [651/8999], Training Loss: 2.143113, Validation Loss: 2.142237\n",
      "Epoch : [652/8999], Training Loss: 2.142807, Validation Loss: 2.141956\n",
      "Epoch : [653/8999], Training Loss: 2.142503, Validation Loss: 2.141676\n",
      "Epoch : [654/8999], Training Loss: 2.142201, Validation Loss: 2.141397\n",
      "Epoch : [655/8999], Training Loss: 2.141901, Validation Loss: 2.141119\n",
      "Epoch : [656/8999], Training Loss: 2.141602, Validation Loss: 2.140842\n",
      "Epoch : [657/8999], Training Loss: 2.141306, Validation Loss: 2.140567\n",
      "Epoch : [658/8999], Training Loss: 2.141011, Validation Loss: 2.140293\n",
      "Epoch : [659/8999], Training Loss: 2.140718, Validation Loss: 2.140020\n",
      "Epoch : [660/8999], Training Loss: 2.140427, Validation Loss: 2.139750\n",
      "Epoch : [661/8999], Training Loss: 2.140138, Validation Loss: 2.139480\n",
      "Epoch : [662/8999], Training Loss: 2.139850, Validation Loss: 2.139212\n",
      "Epoch : [663/8999], Training Loss: 2.139564, Validation Loss: 2.138946\n",
      "Epoch : [664/8999], Training Loss: 2.139280, Validation Loss: 2.138681\n",
      "Epoch : [665/8999], Training Loss: 2.138997, Validation Loss: 2.138417\n",
      "Epoch : [666/8999], Training Loss: 2.138715, Validation Loss: 2.138155\n",
      "Epoch : [667/8999], Training Loss: 2.138435, Validation Loss: 2.137895\n",
      "Epoch : [668/8999], Training Loss: 2.138156, Validation Loss: 2.137635\n",
      "Epoch : [669/8999], Training Loss: 2.137878, Validation Loss: 2.137377\n",
      "Epoch : [670/8999], Training Loss: 2.137601, Validation Loss: 2.137120\n",
      "Epoch : [671/8999], Training Loss: 2.137325, Validation Loss: 2.136864\n",
      "Epoch : [672/8999], Training Loss: 2.137050, Validation Loss: 2.136608\n",
      "Epoch : [673/8999], Training Loss: 2.136776, Validation Loss: 2.136354\n",
      "Epoch : [674/8999], Training Loss: 2.136503, Validation Loss: 2.136100\n",
      "Epoch : [675/8999], Training Loss: 2.136230, Validation Loss: 2.135846\n",
      "Epoch : [676/8999], Training Loss: 2.135957, Validation Loss: 2.135592\n",
      "Epoch : [677/8999], Training Loss: 2.135686, Validation Loss: 2.135339\n",
      "Epoch : [678/8999], Training Loss: 2.135414, Validation Loss: 2.135085\n",
      "Epoch : [679/8999], Training Loss: 2.135144, Validation Loss: 2.134831\n",
      "Epoch : [680/8999], Training Loss: 2.134873, Validation Loss: 2.134577\n",
      "Epoch : [681/8999], Training Loss: 2.134603, Validation Loss: 2.134323\n",
      "Epoch : [682/8999], Training Loss: 2.134333, Validation Loss: 2.134067\n",
      "Epoch : [683/8999], Training Loss: 2.134063, Validation Loss: 2.133811\n",
      "Epoch : [684/8999], Training Loss: 2.133794, Validation Loss: 2.133554\n",
      "Epoch : [685/8999], Training Loss: 2.133525, Validation Loss: 2.133296\n",
      "Epoch : [686/8999], Training Loss: 2.133256, Validation Loss: 2.133038\n",
      "Epoch : [687/8999], Training Loss: 2.132987, Validation Loss: 2.132778\n",
      "Epoch : [688/8999], Training Loss: 2.132718, Validation Loss: 2.132517\n",
      "Epoch : [689/8999], Training Loss: 2.132450, Validation Loss: 2.132256\n",
      "Epoch : [690/8999], Training Loss: 2.132182, Validation Loss: 2.131993\n",
      "Epoch : [691/8999], Training Loss: 2.131913, Validation Loss: 2.131729\n",
      "Epoch : [692/8999], Training Loss: 2.131645, Validation Loss: 2.131463\n",
      "Epoch : [693/8999], Training Loss: 2.131376, Validation Loss: 2.131197\n",
      "Epoch : [694/8999], Training Loss: 2.131107, Validation Loss: 2.130929\n",
      "Epoch : [695/8999], Training Loss: 2.130837, Validation Loss: 2.130660\n",
      "Epoch : [696/8999], Training Loss: 2.130567, Validation Loss: 2.130389\n",
      "Epoch : [697/8999], Training Loss: 2.130297, Validation Loss: 2.130117\n",
      "Epoch : [698/8999], Training Loss: 2.130025, Validation Loss: 2.129842\n",
      "Epoch : [699/8999], Training Loss: 2.129752, Validation Loss: 2.129566\n",
      "Epoch : [700/8999], Training Loss: 2.129478, Validation Loss: 2.129288\n",
      "Epoch : [701/8999], Training Loss: 2.129202, Validation Loss: 2.129007\n",
      "Epoch : [702/8999], Training Loss: 2.128925, Validation Loss: 2.128724\n",
      "Epoch : [703/8999], Training Loss: 2.128645, Validation Loss: 2.128439\n",
      "Epoch : [704/8999], Training Loss: 2.128365, Validation Loss: 2.128151\n",
      "Epoch : [705/8999], Training Loss: 2.128083, Validation Loss: 2.127861\n",
      "Epoch : [706/8999], Training Loss: 2.127800, Validation Loss: 2.127570\n",
      "Epoch : [707/8999], Training Loss: 2.127518, Validation Loss: 2.127278\n",
      "Epoch : [708/8999], Training Loss: 2.127238, Validation Loss: 2.126987\n",
      "Epoch : [709/8999], Training Loss: 2.126959, Validation Loss: 2.126696\n",
      "Epoch : [710/8999], Training Loss: 2.126684, Validation Loss: 2.126408\n",
      "Epoch : [711/8999], Training Loss: 2.126414, Validation Loss: 2.126122\n",
      "Epoch : [712/8999], Training Loss: 2.126147, Validation Loss: 2.125839\n",
      "Epoch : [713/8999], Training Loss: 2.125886, Validation Loss: 2.125559\n",
      "Epoch : [714/8999], Training Loss: 2.125628, Validation Loss: 2.125283\n",
      "Epoch : [715/8999], Training Loss: 2.125374, Validation Loss: 2.125009\n",
      "Epoch : [716/8999], Training Loss: 2.125124, Validation Loss: 2.124739\n",
      "Epoch : [717/8999], Training Loss: 2.124878, Validation Loss: 2.124472\n",
      "Epoch : [718/8999], Training Loss: 2.124634, Validation Loss: 2.124208\n",
      "Epoch : [719/8999], Training Loss: 2.124392, Validation Loss: 2.123946\n",
      "Epoch : [720/8999], Training Loss: 2.124154, Validation Loss: 2.123687\n",
      "Epoch : [721/8999], Training Loss: 2.123917, Validation Loss: 2.123431\n",
      "Epoch : [722/8999], Training Loss: 2.123682, Validation Loss: 2.123177\n",
      "Epoch : [723/8999], Training Loss: 2.123450, Validation Loss: 2.122924\n",
      "Epoch : [724/8999], Training Loss: 2.123219, Validation Loss: 2.122674\n",
      "Epoch : [725/8999], Training Loss: 2.122989, Validation Loss: 2.122425\n",
      "Epoch : [726/8999], Training Loss: 2.122761, Validation Loss: 2.122178\n",
      "Epoch : [727/8999], Training Loss: 2.122534, Validation Loss: 2.121932\n",
      "Epoch : [728/8999], Training Loss: 2.122308, Validation Loss: 2.121687\n",
      "Epoch : [729/8999], Training Loss: 2.122083, Validation Loss: 2.121444\n",
      "Epoch : [730/8999], Training Loss: 2.121860, Validation Loss: 2.121200\n",
      "Epoch : [731/8999], Training Loss: 2.121637, Validation Loss: 2.120958\n",
      "Epoch : [732/8999], Training Loss: 2.121414, Validation Loss: 2.120716\n",
      "Epoch : [733/8999], Training Loss: 2.121193, Validation Loss: 2.120474\n",
      "Epoch : [734/8999], Training Loss: 2.120971, Validation Loss: 2.120232\n",
      "Epoch : [735/8999], Training Loss: 2.120750, Validation Loss: 2.119991\n",
      "Epoch : [736/8999], Training Loss: 2.120530, Validation Loss: 2.119749\n",
      "Epoch : [737/8999], Training Loss: 2.120309, Validation Loss: 2.119508\n",
      "Epoch : [738/8999], Training Loss: 2.120089, Validation Loss: 2.119266\n",
      "Epoch : [739/8999], Training Loss: 2.119868, Validation Loss: 2.119025\n",
      "Epoch : [740/8999], Training Loss: 2.119648, Validation Loss: 2.118784\n",
      "Epoch : [741/8999], Training Loss: 2.119427, Validation Loss: 2.118544\n",
      "Epoch : [742/8999], Training Loss: 2.119207, Validation Loss: 2.118305\n",
      "Epoch : [743/8999], Training Loss: 2.118986, Validation Loss: 2.118068\n",
      "Epoch : [744/8999], Training Loss: 2.118764, Validation Loss: 2.117832\n",
      "Epoch : [745/8999], Training Loss: 2.118542, Validation Loss: 2.117600\n",
      "Epoch : [746/8999], Training Loss: 2.118320, Validation Loss: 2.117371\n",
      "Epoch : [747/8999], Training Loss: 2.118097, Validation Loss: 2.117146\n",
      "Epoch : [748/8999], Training Loss: 2.117874, Validation Loss: 2.116925\n",
      "Epoch : [749/8999], Training Loss: 2.117651, Validation Loss: 2.116708\n",
      "Epoch : [750/8999], Training Loss: 2.117430, Validation Loss: 2.116495\n",
      "Epoch : [751/8999], Training Loss: 2.117209, Validation Loss: 2.116284\n",
      "Epoch : [752/8999], Training Loss: 2.116990, Validation Loss: 2.116075\n",
      "Epoch : [753/8999], Training Loss: 2.116771, Validation Loss: 2.115869\n",
      "Epoch : [754/8999], Training Loss: 2.116553, Validation Loss: 2.115664\n",
      "Epoch : [755/8999], Training Loss: 2.116335, Validation Loss: 2.115459\n",
      "Epoch : [756/8999], Training Loss: 2.116117, Validation Loss: 2.115256\n",
      "Epoch : [757/8999], Training Loss: 2.115899, Validation Loss: 2.115053\n",
      "Epoch : [758/8999], Training Loss: 2.115681, Validation Loss: 2.114851\n",
      "Epoch : [759/8999], Training Loss: 2.115462, Validation Loss: 2.114648\n",
      "Epoch : [760/8999], Training Loss: 2.115243, Validation Loss: 2.114446\n",
      "Epoch : [761/8999], Training Loss: 2.115023, Validation Loss: 2.114242\n",
      "Epoch : [762/8999], Training Loss: 2.114802, Validation Loss: 2.114038\n",
      "Epoch : [763/8999], Training Loss: 2.114578, Validation Loss: 2.113830\n",
      "Epoch : [764/8999], Training Loss: 2.114351, Validation Loss: 2.113617\n",
      "Epoch : [765/8999], Training Loss: 2.114117, Validation Loss: 2.113395\n",
      "Epoch : [766/8999], Training Loss: 2.113877, Validation Loss: 2.113171\n",
      "Epoch : [767/8999], Training Loss: 2.113652, Validation Loss: 2.112982\n",
      "Epoch : [768/8999], Training Loss: 2.113451, Validation Loss: 2.112827\n",
      "Epoch : [769/8999], Training Loss: 2.113257, Validation Loss: 2.112676\n",
      "Epoch : [770/8999], Training Loss: 2.113064, Validation Loss: 2.112524\n",
      "Epoch : [771/8999], Training Loss: 2.112872, Validation Loss: 2.112370\n",
      "Epoch : [772/8999], Training Loss: 2.112679, Validation Loss: 2.112213\n",
      "Epoch : [773/8999], Training Loss: 2.112487, Validation Loss: 2.112051\n",
      "Epoch : [774/8999], Training Loss: 2.112296, Validation Loss: 2.111883\n",
      "Epoch : [775/8999], Training Loss: 2.112106, Validation Loss: 2.111707\n",
      "Epoch : [776/8999], Training Loss: 2.111916, Validation Loss: 2.111522\n",
      "Epoch : [777/8999], Training Loss: 2.111727, Validation Loss: 2.111329\n",
      "Epoch : [778/8999], Training Loss: 2.111539, Validation Loss: 2.111126\n",
      "Epoch : [779/8999], Training Loss: 2.111352, Validation Loss: 2.110914\n",
      "Epoch : [780/8999], Training Loss: 2.111166, Validation Loss: 2.110694\n",
      "Epoch : [781/8999], Training Loss: 2.110981, Validation Loss: 2.110467\n",
      "Epoch : [782/8999], Training Loss: 2.110797, Validation Loss: 2.110232\n",
      "Epoch : [783/8999], Training Loss: 2.110614, Validation Loss: 2.109993\n",
      "Epoch : [784/8999], Training Loss: 2.110433, Validation Loss: 2.109749\n",
      "Epoch : [785/8999], Training Loss: 2.110252, Validation Loss: 2.109502\n",
      "Epoch : [786/8999], Training Loss: 2.110072, Validation Loss: 2.109254\n",
      "Epoch : [787/8999], Training Loss: 2.109893, Validation Loss: 2.109006\n",
      "Epoch : [788/8999], Training Loss: 2.109715, Validation Loss: 2.108758\n",
      "Epoch : [789/8999], Training Loss: 2.109537, Validation Loss: 2.108512\n",
      "Epoch : [790/8999], Training Loss: 2.109358, Validation Loss: 2.108268\n",
      "Epoch : [791/8999], Training Loss: 2.109179, Validation Loss: 2.108026\n",
      "Epoch : [792/8999], Training Loss: 2.108998, Validation Loss: 2.107787\n",
      "Epoch : [793/8999], Training Loss: 2.108815, Validation Loss: 2.107549\n",
      "Epoch : [794/8999], Training Loss: 2.108628, Validation Loss: 2.107314\n",
      "Epoch : [795/8999], Training Loss: 2.108438, Validation Loss: 2.107080\n",
      "Epoch : [796/8999], Training Loss: 2.108242, Validation Loss: 2.106847\n",
      "Epoch : [797/8999], Training Loss: 2.108042, Validation Loss: 2.106614\n",
      "Epoch : [798/8999], Training Loss: 2.107836, Validation Loss: 2.106382\n",
      "Epoch : [799/8999], Training Loss: 2.107623, Validation Loss: 2.106150\n",
      "Epoch : [800/8999], Training Loss: 2.107403, Validation Loss: 2.105919\n",
      "Epoch : [801/8999], Training Loss: 2.107173, Validation Loss: 2.105687\n",
      "Epoch : [802/8999], Training Loss: 2.106931, Validation Loss: 2.105456\n",
      "Epoch : [803/8999], Training Loss: 2.106675, Validation Loss: 2.105225\n",
      "Epoch : [804/8999], Training Loss: 2.106404, Validation Loss: 2.104993\n",
      "Epoch : [805/8999], Training Loss: 2.106122, Validation Loss: 2.104763\n",
      "Epoch : [806/8999], Training Loss: 2.105852, Validation Loss: 2.104556\n",
      "Epoch : [807/8999], Training Loss: 2.105577, Validation Loss: 2.104382\n",
      "Epoch : [808/8999], Training Loss: 2.105303, Validation Loss: 2.104185\n",
      "Epoch : [809/8999], Training Loss: 2.105086, Validation Loss: 2.103968\n",
      "Epoch : [810/8999], Training Loss: 2.104850, Validation Loss: 2.103775\n",
      "Epoch : [811/8999], Training Loss: 2.104653, Validation Loss: 2.103577\n",
      "Epoch : [812/8999], Training Loss: 2.104457, Validation Loss: 2.103409\n",
      "Epoch : [813/8999], Training Loss: 2.104281, Validation Loss: 2.103227\n",
      "Epoch : [814/8999], Training Loss: 2.104107, Validation Loss: 2.103034\n",
      "Epoch : [815/8999], Training Loss: 2.103942, Validation Loss: 2.102785\n",
      "Epoch : [816/8999], Training Loss: 2.103778, Validation Loss: 2.102513\n",
      "Epoch : [817/8999], Training Loss: 2.103616, Validation Loss: 2.102243\n",
      "Epoch : [818/8999], Training Loss: 2.103447, Validation Loss: 2.102006\n",
      "Epoch : [819/8999], Training Loss: 2.103269, Validation Loss: 2.101790\n",
      "Epoch : [820/8999], Training Loss: 2.103085, Validation Loss: 2.101593\n",
      "Epoch : [821/8999], Training Loss: 2.102901, Validation Loss: 2.101401\n",
      "Epoch : [822/8999], Training Loss: 2.102716, Validation Loss: 2.101225\n",
      "Epoch : [823/8999], Training Loss: 2.102535, Validation Loss: 2.101038\n",
      "Epoch : [824/8999], Training Loss: 2.102353, Validation Loss: 2.100891\n",
      "Epoch : [825/8999], Training Loss: 2.102183, Validation Loss: 2.100669\n",
      "Epoch : [826/8999], Training Loss: 2.102000, Validation Loss: 2.100678\n",
      "Epoch : [827/8999], Training Loss: 2.101920, Validation Loss: 2.100246\n",
      "Epoch : [828/8999], Training Loss: 2.101802, Validation Loss: 2.101095\n",
      "Epoch : [829/8999], Training Loss: 2.103752, Validation Loss: 2.100708\n",
      "Epoch : [830/8999], Training Loss: 2.101591, Validation Loss: 2.099945\n",
      "Epoch : [831/8999], Training Loss: 2.101149, Validation Loss: 2.099726\n",
      "Epoch : [832/8999], Training Loss: 2.101022, Validation Loss: 2.099618\n",
      "Epoch : [833/8999], Training Loss: 2.100877, Validation Loss: 2.099405\n",
      "Epoch : [834/8999], Training Loss: 2.100699, Validation Loss: 2.099243\n",
      "Epoch : [835/8999], Training Loss: 2.100572, Validation Loss: 2.099216\n",
      "Epoch : [836/8999], Training Loss: 2.100457, Validation Loss: 2.098945\n",
      "Epoch : [837/8999], Training Loss: 2.100410, Validation Loss: 2.099722\n",
      "Epoch : [838/8999], Training Loss: 2.102555, Validation Loss: 2.099356\n",
      "Epoch : [839/8999], Training Loss: 2.100337, Validation Loss: 2.098638\n",
      "Epoch : [840/8999], Training Loss: 2.099784, Validation Loss: 2.098441\n",
      "Epoch : [841/8999], Training Loss: 2.099673, Validation Loss: 2.098342\n",
      "Epoch : [842/8999], Training Loss: 2.099612, Validation Loss: 2.098296\n",
      "Epoch : [843/8999], Training Loss: 2.099507, Validation Loss: 2.098004\n",
      "Epoch : [844/8999], Training Loss: 2.099705, Validation Loss: 2.099314\n",
      "Epoch : [845/8999], Training Loss: 2.102143, Validation Loss: 2.098647\n",
      "Epoch : [846/8999], Training Loss: 2.099368, Validation Loss: 2.097998\n",
      "Epoch : [847/8999], Training Loss: 2.098926, Validation Loss: 2.097645\n",
      "Epoch : [848/8999], Training Loss: 2.098736, Validation Loss: 2.097462\n",
      "Epoch : [849/8999], Training Loss: 2.098671, Validation Loss: 2.097326\n",
      "Epoch : [850/8999], Training Loss: 2.098539, Validation Loss: 2.097255\n",
      "Epoch : [851/8999], Training Loss: 2.098502, Validation Loss: 2.097634\n",
      "Epoch : [852/8999], Training Loss: 2.098626, Validation Loss: 2.096937\n",
      "Epoch : [853/8999], Training Loss: 2.098271, Validation Loss: 2.097135\n",
      "Epoch : [854/8999], Training Loss: 2.097982, Validation Loss: 2.096929\n",
      "Epoch : [855/8999], Training Loss: 2.098030, Validation Loss: 2.097363\n",
      "Epoch : [856/8999], Training Loss: 2.098766, Validation Loss: 2.096605\n",
      "Epoch : [857/8999], Training Loss: 2.097801, Validation Loss: 2.096295\n",
      "Epoch : [858/8999], Training Loss: 2.097763, Validation Loss: 2.097337\n",
      "Epoch : [859/8999], Training Loss: 2.099066, Validation Loss: 2.097305\n",
      "Epoch : [860/8999], Training Loss: 2.097769, Validation Loss: 2.096094\n",
      "Epoch : [861/8999], Training Loss: 2.097118, Validation Loss: 2.096087\n",
      "Epoch : [862/8999], Training Loss: 2.097056, Validation Loss: 2.096025\n",
      "Epoch : [863/8999], Training Loss: 2.097284, Validation Loss: 2.096921\n",
      "Epoch : [864/8999], Training Loss: 2.099985, Validation Loss: 2.098136\n",
      "Epoch : [865/8999], Training Loss: 2.110872, Validation Loss: 2.113617\n",
      "Epoch : [866/8999], Training Loss: 2.106633, Validation Loss: 2.100589\n",
      "Epoch : [867/8999], Training Loss: 2.099102, Validation Loss: 2.097596\n",
      "Epoch : [868/8999], Training Loss: 2.096825, Validation Loss: 2.096762\n",
      "Epoch : [869/8999], Training Loss: 2.096541, Validation Loss: 2.096471\n",
      "Epoch : [870/8999], Training Loss: 2.096397, Validation Loss: 2.096070\n",
      "Epoch : [871/8999], Training Loss: 2.096227, Validation Loss: 2.095724\n",
      "Epoch : [872/8999], Training Loss: 2.096073, Validation Loss: 2.095370\n",
      "Epoch : [873/8999], Training Loss: 2.095931, Validation Loss: 2.095049\n",
      "Epoch : [874/8999], Training Loss: 2.095815, Validation Loss: 2.094673\n",
      "Epoch : [875/8999], Training Loss: 2.095651, Validation Loss: 2.094298\n",
      "Epoch : [876/8999], Training Loss: 2.095449, Validation Loss: 2.094167\n",
      "Epoch : [877/8999], Training Loss: 2.095248, Validation Loss: 2.094224\n",
      "Epoch : [878/8999], Training Loss: 2.095067, Validation Loss: 2.094496\n",
      "Epoch : [879/8999], Training Loss: 2.095443, Validation Loss: 2.094221\n",
      "Epoch : [880/8999], Training Loss: 2.094875, Validation Loss: 2.093973\n",
      "Epoch : [881/8999], Training Loss: 2.094616, Validation Loss: 2.093607\n",
      "Epoch : [882/8999], Training Loss: 2.094552, Validation Loss: 2.093476\n",
      "Epoch : [883/8999], Training Loss: 2.094561, Validation Loss: 2.093614\n",
      "Epoch : [884/8999], Training Loss: 2.094502, Validation Loss: 2.093380\n",
      "Epoch : [885/8999], Training Loss: 2.094047, Validation Loss: 2.093476\n",
      "Epoch : [886/8999], Training Loss: 2.093799, Validation Loss: 2.093376\n",
      "Epoch : [887/8999], Training Loss: 2.093754, Validation Loss: 2.093342\n",
      "Epoch : [888/8999], Training Loss: 2.093644, Validation Loss: 2.093189\n",
      "Epoch : [889/8999], Training Loss: 2.093506, Validation Loss: 2.092853\n",
      "Epoch : [890/8999], Training Loss: 2.093400, Validation Loss: 2.092895\n",
      "Epoch : [891/8999], Training Loss: 2.093267, Validation Loss: 2.092718\n",
      "Epoch : [892/8999], Training Loss: 2.093104, Validation Loss: 2.092365\n",
      "Epoch : [893/8999], Training Loss: 2.093019, Validation Loss: 2.092624\n",
      "Epoch : [894/8999], Training Loss: 2.092892, Validation Loss: 2.092133\n",
      "Epoch : [895/8999], Training Loss: 2.092788, Validation Loss: 2.092161\n",
      "Epoch : [896/8999], Training Loss: 2.092610, Validation Loss: 2.091917\n",
      "Epoch : [897/8999], Training Loss: 2.092541, Validation Loss: 2.092363\n",
      "Epoch : [898/8999], Training Loss: 2.092862, Validation Loss: 2.092100\n",
      "Epoch : [899/8999], Training Loss: 2.092764, Validation Loss: 2.092751\n",
      "Epoch : [900/8999], Training Loss: 2.093714, Validation Loss: 2.094062\n",
      "Epoch : [901/8999], Training Loss: 2.100692, Validation Loss: 2.096639\n",
      "Epoch : [902/8999], Training Loss: 2.099900, Validation Loss: 2.093718\n",
      "Epoch : [903/8999], Training Loss: 2.094189, Validation Loss: 2.092969\n",
      "Epoch : [904/8999], Training Loss: 2.103694, Validation Loss: 2.107741\n",
      "Epoch : [905/8999], Training Loss: 2.100509, Validation Loss: 2.095444\n",
      "Epoch : [906/8999], Training Loss: 2.093596, Validation Loss: 2.091360\n",
      "Epoch : [907/8999], Training Loss: 2.092073, Validation Loss: 2.090681\n",
      "Epoch : [908/8999], Training Loss: 2.091785, Validation Loss: 2.090389\n",
      "Epoch : [909/8999], Training Loss: 2.091761, Validation Loss: 2.090532\n",
      "Epoch : [910/8999], Training Loss: 2.091885, Validation Loss: 2.094414\n",
      "Epoch : [911/8999], Training Loss: 2.092567, Validation Loss: 2.092403\n",
      "Epoch : [912/8999], Training Loss: 2.103154, Validation Loss: 2.104498\n",
      "Epoch : [913/8999], Training Loss: 2.096961, Validation Loss: 2.090851\n",
      "Epoch : [914/8999], Training Loss: 2.093380, Validation Loss: 2.091236\n",
      "Epoch : [915/8999], Training Loss: 2.091775, Validation Loss: 2.091741\n",
      "Epoch : [916/8999], Training Loss: 2.091787, Validation Loss: 2.092296\n",
      "Epoch : [917/8999], Training Loss: 2.097023, Validation Loss: 2.095223\n",
      "Epoch : [918/8999], Training Loss: 2.093296, Validation Loss: 2.090606\n",
      "Epoch : [919/8999], Training Loss: 2.091503, Validation Loss: 2.090181\n",
      "Epoch : [920/8999], Training Loss: 2.090756, Validation Loss: 2.090166\n",
      "Epoch : [921/8999], Training Loss: 2.090776, Validation Loss: 2.090875\n",
      "Epoch : [922/8999], Training Loss: 2.090399, Validation Loss: 2.089838\n",
      "Epoch : [923/8999], Training Loss: 2.090469, Validation Loss: 2.090092\n",
      "Epoch : [924/8999], Training Loss: 2.090110, Validation Loss: 2.090713\n",
      "Epoch : [925/8999], Training Loss: 2.093305, Validation Loss: 2.090120\n",
      "Epoch : [926/8999], Training Loss: 2.090634, Validation Loss: 2.089094\n",
      "Epoch : [927/8999], Training Loss: 2.091014, Validation Loss: 2.092538\n",
      "Epoch : [928/8999], Training Loss: 2.099012, Validation Loss: 2.091373\n",
      "Epoch : [929/8999], Training Loss: 2.091623, Validation Loss: 2.099338\n",
      "Epoch : [930/8999], Training Loss: 2.094626, Validation Loss: 2.091595\n",
      "Epoch : [931/8999], Training Loss: 2.099593, Validation Loss: 2.093067\n",
      "Epoch : [932/8999], Training Loss: 2.092122, Validation Loss: 2.095992\n",
      "Epoch : [933/8999], Training Loss: 2.098690, Validation Loss: 2.091685\n",
      "Epoch : [934/8999], Training Loss: 2.096138, Validation Loss: 2.092479\n",
      "Epoch : [935/8999], Training Loss: 2.095347, Validation Loss: 2.092163\n",
      "Epoch : [936/8999], Training Loss: 2.090948, Validation Loss: 2.089736\n",
      "Epoch : [937/8999], Training Loss: 2.089470, Validation Loss: 2.090174\n",
      "Epoch : [938/8999], Training Loss: 2.090381, Validation Loss: 2.099705\n",
      "Epoch : [939/8999], Training Loss: 2.094729, Validation Loss: 2.088475\n",
      "Epoch : [940/8999], Training Loss: 2.089456, Validation Loss: 2.089610\n",
      "Epoch : [941/8999], Training Loss: 2.101203, Validation Loss: 2.106640\n",
      "Epoch : [942/8999], Training Loss: 2.096770, Validation Loss: 2.092709\n",
      "Epoch : [943/8999], Training Loss: 2.098883, Validation Loss: 2.104433\n",
      "Epoch : [944/8999], Training Loss: 2.096478, Validation Loss: 2.092155\n",
      "Epoch : [945/8999], Training Loss: 2.095494, Validation Loss: 2.097199\n",
      "Epoch : [946/8999], Training Loss: 2.100181, Validation Loss: 2.106391\n",
      "Epoch : [947/8999], Training Loss: 2.098066, Validation Loss: 2.099215\n",
      "Epoch : [948/8999], Training Loss: 2.097250, Validation Loss: 2.097150\n",
      "Epoch : [949/8999], Training Loss: 2.094720, Validation Loss: 2.095510\n",
      "Epoch : [950/8999], Training Loss: 2.090834, Validation Loss: 2.089906\n",
      "Epoch : [951/8999], Training Loss: 2.090273, Validation Loss: 2.087949\n",
      "Epoch : [952/8999], Training Loss: 2.089569, Validation Loss: 2.089333\n",
      "Epoch : [953/8999], Training Loss: 2.089483, Validation Loss: 2.088848\n",
      "Epoch : [954/8999], Training Loss: 2.088566, Validation Loss: 2.088404\n",
      "Epoch : [955/8999], Training Loss: 2.088693, Validation Loss: 2.088005\n",
      "Epoch : [956/8999], Training Loss: 2.089307, Validation Loss: 2.089183\n",
      "Epoch : [957/8999], Training Loss: 2.087599, Validation Loss: 2.087949\n",
      "Epoch : [958/8999], Training Loss: 2.087674, Validation Loss: 2.088889\n",
      "Epoch : [959/8999], Training Loss: 2.089502, Validation Loss: 2.093799\n",
      "Epoch : [960/8999], Training Loss: 2.096379, Validation Loss: 2.087701\n",
      "Epoch : [961/8999], Training Loss: 2.087557, Validation Loss: 2.087968\n",
      "Epoch : [962/8999], Training Loss: 2.087295, Validation Loss: 2.087652\n",
      "Epoch : [963/8999], Training Loss: 2.087556, Validation Loss: 2.088832\n",
      "Epoch : [964/8999], Training Loss: 2.088163, Validation Loss: 2.086639\n",
      "Epoch : [965/8999], Training Loss: 2.087189, Validation Loss: 2.087638\n",
      "Epoch : [966/8999], Training Loss: 2.086974, Validation Loss: 2.087365\n",
      "Epoch : [967/8999], Training Loss: 2.086883, Validation Loss: 2.087904\n",
      "Epoch : [968/8999], Training Loss: 2.087671, Validation Loss: 2.085900\n",
      "Epoch : [969/8999], Training Loss: 2.087334, Validation Loss: 2.087230\n",
      "Epoch : [970/8999], Training Loss: 2.086596, Validation Loss: 2.087152\n",
      "Epoch : [971/8999], Training Loss: 2.086448, Validation Loss: 2.087349\n",
      "Epoch : [972/8999], Training Loss: 2.086035, Validation Loss: 2.085459\n",
      "Epoch : [973/8999], Training Loss: 2.086192, Validation Loss: 2.086997\n",
      "Epoch : [974/8999], Training Loss: 2.085908, Validation Loss: 2.084837\n",
      "Epoch : [975/8999], Training Loss: 2.085977, Validation Loss: 2.085820\n",
      "Epoch : [976/8999], Training Loss: 2.085719, Validation Loss: 2.085718\n",
      "Epoch : [977/8999], Training Loss: 2.085783, Validation Loss: 2.086827\n",
      "Epoch : [978/8999], Training Loss: 2.085523, Validation Loss: 2.084470\n",
      "Epoch : [979/8999], Training Loss: 2.085604, Validation Loss: 2.085319\n",
      "Epoch : [980/8999], Training Loss: 2.085376, Validation Loss: 2.085324\n",
      "Epoch : [981/8999], Training Loss: 2.085355, Validation Loss: 2.086033\n",
      "Epoch : [982/8999], Training Loss: 2.085278, Validation Loss: 2.084129\n",
      "Epoch : [983/8999], Training Loss: 2.085057, Validation Loss: 2.084653\n",
      "Epoch : [984/8999], Training Loss: 2.084952, Validation Loss: 2.084981\n",
      "Epoch : [985/8999], Training Loss: 2.084822, Validation Loss: 2.085121\n",
      "Epoch : [986/8999], Training Loss: 2.084779, Validation Loss: 2.084727\n",
      "Epoch : [987/8999], Training Loss: 2.084759, Validation Loss: 2.085168\n",
      "Epoch : [988/8999], Training Loss: 2.084443, Validation Loss: 2.084240\n",
      "Epoch : [989/8999], Training Loss: 2.084543, Validation Loss: 2.085114\n",
      "Epoch : [990/8999], Training Loss: 2.084240, Validation Loss: 2.083638\n",
      "Epoch : [991/8999], Training Loss: 2.084281, Validation Loss: 2.084512\n",
      "Epoch : [992/8999], Training Loss: 2.084112, Validation Loss: 2.083886\n",
      "Epoch : [993/8999], Training Loss: 2.084205, Validation Loss: 2.084613\n",
      "Epoch : [994/8999], Training Loss: 2.083891, Validation Loss: 2.083532\n",
      "Epoch : [995/8999], Training Loss: 2.084050, Validation Loss: 2.084669\n",
      "Epoch : [996/8999], Training Loss: 2.083658, Validation Loss: 2.083168\n",
      "Epoch : [997/8999], Training Loss: 2.083922, Validation Loss: 2.084318\n",
      "Epoch : [998/8999], Training Loss: 2.083474, Validation Loss: 2.082927\n",
      "Epoch : [999/8999], Training Loss: 2.083690, Validation Loss: 2.084199\n",
      "Epoch : [1000/8999], Training Loss: 2.083330, Validation Loss: 2.082452\n",
      "Epoch : [1001/8999], Training Loss: 2.083327, Validation Loss: 2.083078\n",
      "Epoch : [1002/8999], Training Loss: 2.083207, Validation Loss: 2.082689\n",
      "Epoch : [1003/8999], Training Loss: 2.083264, Validation Loss: 2.083427\n",
      "Epoch : [1004/8999], Training Loss: 2.083050, Validation Loss: 2.082613\n",
      "Epoch : [1005/8999], Training Loss: 2.083109, Validation Loss: 2.083812\n",
      "Epoch : [1006/8999], Training Loss: 2.083117, Validation Loss: 2.083207\n",
      "Epoch : [1007/8999], Training Loss: 2.082779, Validation Loss: 2.082432\n",
      "Epoch : [1008/8999], Training Loss: 2.082667, Validation Loss: 2.082927\n",
      "Epoch : [1009/8999], Training Loss: 2.082745, Validation Loss: 2.082570\n",
      "Epoch : [1010/8999], Training Loss: 2.082479, Validation Loss: 2.082272\n",
      "Epoch : [1011/8999], Training Loss: 2.082545, Validation Loss: 2.082854\n",
      "Epoch : [1012/8999], Training Loss: 2.082275, Validation Loss: 2.081814\n",
      "Epoch : [1013/8999], Training Loss: 2.082344, Validation Loss: 2.083066\n",
      "Epoch : [1014/8999], Training Loss: 2.082261, Validation Loss: 2.082085\n",
      "Epoch : [1015/8999], Training Loss: 2.082112, Validation Loss: 2.082402\n",
      "Epoch : [1016/8999], Training Loss: 2.081975, Validation Loss: 2.081998\n",
      "Epoch : [1017/8999], Training Loss: 2.081944, Validation Loss: 2.082472\n",
      "Epoch : [1018/8999], Training Loss: 2.081759, Validation Loss: 2.081619\n",
      "Epoch : [1019/8999], Training Loss: 2.081855, Validation Loss: 2.082212\n",
      "Epoch : [1020/8999], Training Loss: 2.081542, Validation Loss: 2.081071\n",
      "Epoch : [1021/8999], Training Loss: 2.081588, Validation Loss: 2.082073\n",
      "Epoch : [1022/8999], Training Loss: 2.081526, Validation Loss: 2.081441\n",
      "Epoch : [1023/8999], Training Loss: 2.081476, Validation Loss: 2.081031\n",
      "Epoch : [1024/8999], Training Loss: 2.081104, Validation Loss: 2.080615\n",
      "Epoch : [1025/8999], Training Loss: 2.081144, Validation Loss: 2.081468\n",
      "Epoch : [1026/8999], Training Loss: 2.081126, Validation Loss: 2.080653\n",
      "Epoch : [1027/8999], Training Loss: 2.080843, Validation Loss: 2.080231\n",
      "Epoch : [1028/8999], Training Loss: 2.080663, Validation Loss: 2.079530\n",
      "Epoch : [1029/8999], Training Loss: 2.080445, Validation Loss: 2.083355\n",
      "Epoch : [1030/8999], Training Loss: 2.081997, Validation Loss: 2.080029\n",
      "Epoch : [1031/8999], Training Loss: 2.084043, Validation Loss: 2.082496\n",
      "Epoch : [1032/8999], Training Loss: 2.080772, Validation Loss: 2.079312\n",
      "Epoch : [1033/8999], Training Loss: 2.083291, Validation Loss: 2.080546\n",
      "Epoch : [1034/8999], Training Loss: 2.084109, Validation Loss: 2.080298\n",
      "Epoch : [1035/8999], Training Loss: 2.080476, Validation Loss: 2.078920\n",
      "Epoch : [1036/8999], Training Loss: 2.079545, Validation Loss: 2.079293\n",
      "Epoch : [1037/8999], Training Loss: 2.079314, Validation Loss: 2.078316\n",
      "Epoch : [1038/8999], Training Loss: 2.078960, Validation Loss: 2.078632\n",
      "Epoch : [1039/8999], Training Loss: 2.078773, Validation Loss: 2.077969\n",
      "Epoch : [1040/8999], Training Loss: 2.078713, Validation Loss: 2.078464\n",
      "Epoch : [1041/8999], Training Loss: 2.078574, Validation Loss: 2.077831\n",
      "Epoch : [1042/8999], Training Loss: 2.078476, Validation Loss: 2.078291\n",
      "Epoch : [1043/8999], Training Loss: 2.078325, Validation Loss: 2.077659\n",
      "Epoch : [1044/8999], Training Loss: 2.078220, Validation Loss: 2.078327\n",
      "Epoch : [1045/8999], Training Loss: 2.078118, Validation Loss: 2.077469\n",
      "Epoch : [1046/8999], Training Loss: 2.078074, Validation Loss: 2.078015\n",
      "Epoch : [1047/8999], Training Loss: 2.077989, Validation Loss: 2.077298\n",
      "Epoch : [1048/8999], Training Loss: 2.077898, Validation Loss: 2.077572\n",
      "Epoch : [1049/8999], Training Loss: 2.077818, Validation Loss: 2.077389\n",
      "Epoch : [1050/8999], Training Loss: 2.077799, Validation Loss: 2.077061\n",
      "Epoch : [1051/8999], Training Loss: 2.077698, Validation Loss: 2.077827\n",
      "Epoch : [1052/8999], Training Loss: 2.077615, Validation Loss: 2.076776\n",
      "Epoch : [1053/8999], Training Loss: 2.077616, Validation Loss: 2.078203\n",
      "Epoch : [1054/8999], Training Loss: 2.077714, Validation Loss: 2.077188\n",
      "Epoch : [1055/8999], Training Loss: 2.082754, Validation Loss: 2.106723\n",
      "Epoch : [1056/8999], Training Loss: 2.102647, Validation Loss: 2.084498\n",
      "Epoch : [1057/8999], Training Loss: 2.090633, Validation Loss: 2.087164\n",
      "Epoch : [1058/8999], Training Loss: 2.084101, Validation Loss: 2.083936\n",
      "Epoch : [1059/8999], Training Loss: 2.090357, Validation Loss: 2.092883\n",
      "Epoch : [1060/8999], Training Loss: 2.101296, Validation Loss: 2.126367\n",
      "Epoch : [1061/8999], Training Loss: 2.113539, Validation Loss: 2.088698\n",
      "Epoch : [1062/8999], Training Loss: 2.088268, Validation Loss: 2.079644\n",
      "Epoch : [1063/8999], Training Loss: 2.079899, Validation Loss: 2.077967\n",
      "Epoch : [1064/8999], Training Loss: 2.086390, Validation Loss: 2.084981\n",
      "Epoch : [1065/8999], Training Loss: 2.082611, Validation Loss: 2.077789\n",
      "Epoch : [1066/8999], Training Loss: 2.078147, Validation Loss: 2.077243\n",
      "Epoch : [1067/8999], Training Loss: 2.077474, Validation Loss: 2.078104\n",
      "Epoch : [1068/8999], Training Loss: 2.077098, Validation Loss: 2.078335\n",
      "Epoch : [1069/8999], Training Loss: 2.076409, Validation Loss: 2.077883\n",
      "Epoch : [1070/8999], Training Loss: 2.077489, Validation Loss: 2.080228\n",
      "Epoch : [1071/8999], Training Loss: 2.080535, Validation Loss: 2.079848\n",
      "Epoch : [1072/8999], Training Loss: 2.078508, Validation Loss: 2.078798\n",
      "Epoch : [1073/8999], Training Loss: 2.077415, Validation Loss: 2.086271\n",
      "Epoch : [1074/8999], Training Loss: 2.098376, Validation Loss: 2.087541\n",
      "Epoch : [1075/8999], Training Loss: 2.086953, Validation Loss: 2.079832\n",
      "Epoch : [1076/8999], Training Loss: 2.079253, Validation Loss: 2.081125\n",
      "Epoch : [1077/8999], Training Loss: 2.077948, Validation Loss: 2.078378\n",
      "Epoch : [1078/8999], Training Loss: 2.076659, Validation Loss: 2.082909\n",
      "Epoch : [1079/8999], Training Loss: 2.096285, Validation Loss: 2.093988\n",
      "Epoch : [1080/8999], Training Loss: 2.092503, Validation Loss: 2.097399\n",
      "Epoch : [1081/8999], Training Loss: 2.090124, Validation Loss: 2.092634\n",
      "Epoch : [1082/8999], Training Loss: 2.085668, Validation Loss: 2.085358\n",
      "Epoch : [1083/8999], Training Loss: 2.078453, Validation Loss: 2.077991\n",
      "Epoch : [1084/8999], Training Loss: 2.078106, Validation Loss: 2.082071\n",
      "Epoch : [1085/8999], Training Loss: 2.079667, Validation Loss: 2.089192\n",
      "Epoch : [1086/8999], Training Loss: 2.079857, Validation Loss: 2.078684\n",
      "Epoch : [1087/8999], Training Loss: 2.076582, Validation Loss: 2.078399\n",
      "Epoch : [1088/8999], Training Loss: 2.076079, Validation Loss: 2.074314\n",
      "Epoch : [1089/8999], Training Loss: 2.075783, Validation Loss: 2.075333\n",
      "Epoch : [1090/8999], Training Loss: 2.075745, Validation Loss: 2.074482\n",
      "Epoch : [1091/8999], Training Loss: 2.080688, Validation Loss: 2.076708\n",
      "Epoch : [1092/8999], Training Loss: 2.083565, Validation Loss: 2.081972\n",
      "Epoch : [1093/8999], Training Loss: 2.078094, Validation Loss: 2.080527\n",
      "Epoch : [1094/8999], Training Loss: 2.076793, Validation Loss: 2.077722\n",
      "Epoch : [1095/8999], Training Loss: 2.077158, Validation Loss: 2.073834\n",
      "Epoch : [1096/8999], Training Loss: 2.075493, Validation Loss: 2.073794\n",
      "Epoch : [1097/8999], Training Loss: 2.074086, Validation Loss: 2.073023\n",
      "Epoch : [1098/8999], Training Loss: 2.077454, Validation Loss: 2.075846\n",
      "Epoch : [1099/8999], Training Loss: 2.076575, Validation Loss: 2.075234\n",
      "Epoch : [1100/8999], Training Loss: 2.077117, Validation Loss: 2.080222\n",
      "Epoch : [1101/8999], Training Loss: 2.078112, Validation Loss: 2.077580\n",
      "Epoch : [1102/8999], Training Loss: 2.082194, Validation Loss: 2.080065\n",
      "Epoch : [1103/8999], Training Loss: 2.078336, Validation Loss: 2.080068\n",
      "Epoch : [1104/8999], Training Loss: 2.081330, Validation Loss: 2.075745\n",
      "Epoch : [1105/8999], Training Loss: 2.078095, Validation Loss: 2.073852\n",
      "Epoch : [1106/8999], Training Loss: 2.076875, Validation Loss: 2.074471\n",
      "Epoch : [1107/8999], Training Loss: 2.073807, Validation Loss: 2.072811\n",
      "Epoch : [1108/8999], Training Loss: 2.076164, Validation Loss: 2.072799\n",
      "Epoch : [1109/8999], Training Loss: 2.073279, Validation Loss: 2.071157\n",
      "Epoch : [1110/8999], Training Loss: 2.072126, Validation Loss: 2.070552\n",
      "Epoch : [1111/8999], Training Loss: 2.071651, Validation Loss: 2.070439\n",
      "Epoch : [1112/8999], Training Loss: 2.071608, Validation Loss: 2.070227\n",
      "Epoch : [1113/8999], Training Loss: 2.071217, Validation Loss: 2.070365\n",
      "Epoch : [1114/8999], Training Loss: 2.071471, Validation Loss: 2.069992\n",
      "Epoch : [1115/8999], Training Loss: 2.071161, Validation Loss: 2.070322\n",
      "Epoch : [1116/8999], Training Loss: 2.070830, Validation Loss: 2.069589\n",
      "Epoch : [1117/8999], Training Loss: 2.070559, Validation Loss: 2.069365\n",
      "Epoch : [1118/8999], Training Loss: 2.070307, Validation Loss: 2.069195\n",
      "Epoch : [1119/8999], Training Loss: 2.069964, Validation Loss: 2.069105\n",
      "Epoch : [1120/8999], Training Loss: 2.069849, Validation Loss: 2.069130\n",
      "Epoch : [1121/8999], Training Loss: 2.069722, Validation Loss: 2.069064\n",
      "Epoch : [1122/8999], Training Loss: 2.069539, Validation Loss: 2.068287\n",
      "Epoch : [1123/8999], Training Loss: 2.068749, Validation Loss: 2.067941\n",
      "Epoch : [1124/8999], Training Loss: 2.068866, Validation Loss: 2.067681\n",
      "Epoch : [1125/8999], Training Loss: 2.068463, Validation Loss: 2.067717\n",
      "Epoch : [1126/8999], Training Loss: 2.068646, Validation Loss: 2.067438\n",
      "Epoch : [1127/8999], Training Loss: 2.068227, Validation Loss: 2.067539\n",
      "Epoch : [1128/8999], Training Loss: 2.070027, Validation Loss: 2.067887\n",
      "Epoch : [1129/8999], Training Loss: 2.068366, Validation Loss: 2.067333\n",
      "Epoch : [1130/8999], Training Loss: 2.068183, Validation Loss: 2.068622\n",
      "Epoch : [1131/8999], Training Loss: 2.070945, Validation Loss: 2.068674\n",
      "Epoch : [1132/8999], Training Loss: 2.068978, Validation Loss: 2.068272\n",
      "Epoch : [1133/8999], Training Loss: 2.069584, Validation Loss: 2.068359\n",
      "Epoch : [1134/8999], Training Loss: 2.068644, Validation Loss: 2.067590\n",
      "Epoch : [1135/8999], Training Loss: 2.068397, Validation Loss: 2.068159\n",
      "Epoch : [1136/8999], Training Loss: 2.070813, Validation Loss: 2.068507\n",
      "Epoch : [1137/8999], Training Loss: 2.070207, Validation Loss: 2.071413\n",
      "Epoch : [1138/8999], Training Loss: 2.070042, Validation Loss: 2.069811\n",
      "Epoch : [1139/8999], Training Loss: 2.070540, Validation Loss: 2.068243\n",
      "Epoch : [1140/8999], Training Loss: 2.068624, Validation Loss: 2.067118\n",
      "Epoch : [1141/8999], Training Loss: 2.067828, Validation Loss: 2.067223\n",
      "Epoch : [1142/8999], Training Loss: 2.068245, Validation Loss: 2.067015\n",
      "Epoch : [1143/8999], Training Loss: 2.067982, Validation Loss: 2.067169\n",
      "Epoch : [1144/8999], Training Loss: 2.067867, Validation Loss: 2.067209\n",
      "Epoch : [1145/8999], Training Loss: 2.069324, Validation Loss: 2.067482\n",
      "Epoch : [1146/8999], Training Loss: 2.068075, Validation Loss: 2.066886\n",
      "Epoch : [1147/8999], Training Loss: 2.067500, Validation Loss: 2.066420\n",
      "Epoch : [1148/8999], Training Loss: 2.067058, Validation Loss: 2.066211\n",
      "Epoch : [1149/8999], Training Loss: 2.066963, Validation Loss: 2.066126\n",
      "Epoch : [1150/8999], Training Loss: 2.066815, Validation Loss: 2.066137\n",
      "Epoch : [1151/8999], Training Loss: 2.066804, Validation Loss: 2.066125\n",
      "Epoch : [1152/8999], Training Loss: 2.066678, Validation Loss: 2.066100\n",
      "Epoch : [1153/8999], Training Loss: 2.066573, Validation Loss: 2.066098\n",
      "Epoch : [1154/8999], Training Loss: 2.066490, Validation Loss: 2.066139\n",
      "Epoch : [1155/8999], Training Loss: 2.066446, Validation Loss: 2.066101\n",
      "Epoch : [1156/8999], Training Loss: 2.066371, Validation Loss: 2.066030\n",
      "Epoch : [1157/8999], Training Loss: 2.066277, Validation Loss: 2.065798\n",
      "Epoch : [1158/8999], Training Loss: 2.066124, Validation Loss: 2.065727\n",
      "Epoch : [1159/8999], Training Loss: 2.066044, Validation Loss: 2.065654\n",
      "Epoch : [1160/8999], Training Loss: 2.065977, Validation Loss: 2.065435\n",
      "Epoch : [1161/8999], Training Loss: 2.065829, Validation Loss: 2.065251\n",
      "Epoch : [1162/8999], Training Loss: 2.065697, Validation Loss: 2.064992\n",
      "Epoch : [1163/8999], Training Loss: 2.065566, Validation Loss: 2.064900\n",
      "Epoch : [1164/8999], Training Loss: 2.065572, Validation Loss: 2.064956\n",
      "Epoch : [1165/8999], Training Loss: 2.065620, Validation Loss: 2.064801\n",
      "Epoch : [1166/8999], Training Loss: 2.065699, Validation Loss: 2.065417\n",
      "Epoch : [1167/8999], Training Loss: 2.065898, Validation Loss: 2.064661\n",
      "Epoch : [1168/8999], Training Loss: 2.065398, Validation Loss: 2.064888\n",
      "Epoch : [1169/8999], Training Loss: 2.065399, Validation Loss: 2.064971\n",
      "Epoch : [1170/8999], Training Loss: 2.065250, Validation Loss: 2.064895\n",
      "Epoch : [1171/8999], Training Loss: 2.065194, Validation Loss: 2.065315\n",
      "Epoch : [1172/8999], Training Loss: 2.065089, Validation Loss: 2.064987\n",
      "Epoch : [1173/8999], Training Loss: 2.065059, Validation Loss: 2.065635\n",
      "Epoch : [1174/8999], Training Loss: 2.064956, Validation Loss: 2.064871\n",
      "Epoch : [1175/8999], Training Loss: 2.064932, Validation Loss: 2.065811\n",
      "Epoch : [1176/8999], Training Loss: 2.064916, Validation Loss: 2.065246\n",
      "Epoch : [1177/8999], Training Loss: 2.064793, Validation Loss: 2.065014\n",
      "Epoch : [1178/8999], Training Loss: 2.064837, Validation Loss: 2.065624\n",
      "Epoch : [1179/8999], Training Loss: 2.064736, Validation Loss: 2.064966\n",
      "Epoch : [1180/8999], Training Loss: 2.064600, Validation Loss: 2.065043\n",
      "Epoch : [1181/8999], Training Loss: 2.064652, Validation Loss: 2.065274\n",
      "Epoch : [1182/8999], Training Loss: 2.064431, Validation Loss: 2.064768\n",
      "Epoch : [1183/8999], Training Loss: 2.064460, Validation Loss: 2.065110\n",
      "Epoch : [1184/8999], Training Loss: 2.064294, Validation Loss: 2.064552\n",
      "Epoch : [1185/8999], Training Loss: 2.064236, Validation Loss: 2.064952\n",
      "Epoch : [1186/8999], Training Loss: 2.064237, Validation Loss: 2.064752\n",
      "Epoch : [1187/8999], Training Loss: 2.063958, Validation Loss: 2.064210\n",
      "Epoch : [1188/8999], Training Loss: 2.063961, Validation Loss: 2.065014\n",
      "Epoch : [1189/8999], Training Loss: 2.064006, Validation Loss: 2.064646\n",
      "Epoch : [1190/8999], Training Loss: 2.063762, Validation Loss: 2.064245\n",
      "Epoch : [1191/8999], Training Loss: 2.063867, Validation Loss: 2.064540\n",
      "Epoch : [1192/8999], Training Loss: 2.063656, Validation Loss: 2.064503\n",
      "Epoch : [1193/8999], Training Loss: 2.063646, Validation Loss: 2.064688\n",
      "Epoch : [1194/8999], Training Loss: 2.063572, Validation Loss: 2.064652\n",
      "Epoch : [1195/8999], Training Loss: 2.063601, Validation Loss: 2.064967\n",
      "Epoch : [1196/8999], Training Loss: 2.063412, Validation Loss: 2.064442\n",
      "Epoch : [1197/8999], Training Loss: 2.063498, Validation Loss: 2.065112\n",
      "Epoch : [1198/8999], Training Loss: 2.063425, Validation Loss: 2.064679\n",
      "Epoch : [1199/8999], Training Loss: 2.063435, Validation Loss: 2.064746\n",
      "Epoch : [1200/8999], Training Loss: 2.063299, Validation Loss: 2.064686\n",
      "Epoch : [1201/8999], Training Loss: 2.063303, Validation Loss: 2.064753\n",
      "Epoch : [1202/8999], Training Loss: 2.063384, Validation Loss: 2.064843\n",
      "Epoch : [1203/8999], Training Loss: 2.063292, Validation Loss: 2.065222\n",
      "Epoch : [1204/8999], Training Loss: 2.065350, Validation Loss: 2.071705\n",
      "Epoch : [1205/8999], Training Loss: 2.069848, Validation Loss: 2.072043\n",
      "Epoch : [1206/8999], Training Loss: 2.070920, Validation Loss: 2.066943\n",
      "Epoch : [1207/8999], Training Loss: 2.069730, Validation Loss: 2.067723\n",
      "Epoch : [1208/8999], Training Loss: 2.069934, Validation Loss: 2.070032\n",
      "Epoch : [1209/8999], Training Loss: 2.065966, Validation Loss: 2.064265\n",
      "Epoch : [1210/8999], Training Loss: 2.064260, Validation Loss: 2.065076\n",
      "Epoch : [1211/8999], Training Loss: 2.066737, Validation Loss: 2.073891\n",
      "Epoch : [1212/8999], Training Loss: 2.074944, Validation Loss: 2.065886\n",
      "Epoch : [1213/8999], Training Loss: 2.067399, Validation Loss: 2.063198\n",
      "Epoch : [1214/8999], Training Loss: 2.065223, Validation Loss: 2.064161\n",
      "Epoch : [1215/8999], Training Loss: 2.064738, Validation Loss: 2.062871\n",
      "Epoch : [1216/8999], Training Loss: 2.064100, Validation Loss: 2.064165\n",
      "Epoch : [1217/8999], Training Loss: 2.063406, Validation Loss: 2.062537\n",
      "Epoch : [1218/8999], Training Loss: 2.063196, Validation Loss: 2.061992\n",
      "Epoch : [1219/8999], Training Loss: 2.062978, Validation Loss: 2.062602\n",
      "Epoch : [1220/8999], Training Loss: 2.062978, Validation Loss: 2.061501\n",
      "Epoch : [1221/8999], Training Loss: 2.063380, Validation Loss: 2.062941\n",
      "Epoch : [1222/8999], Training Loss: 2.062324, Validation Loss: 2.060749\n",
      "Epoch : [1223/8999], Training Loss: 2.061434, Validation Loss: 2.060905\n",
      "Epoch : [1224/8999], Training Loss: 2.061336, Validation Loss: 2.060694\n",
      "Epoch : [1225/8999], Training Loss: 2.061747, Validation Loss: 2.060676\n",
      "Epoch : [1226/8999], Training Loss: 2.061372, Validation Loss: 2.061414\n",
      "Epoch : [1227/8999], Training Loss: 2.061443, Validation Loss: 2.062080\n",
      "Epoch : [1228/8999], Training Loss: 2.061552, Validation Loss: 2.062526\n",
      "Epoch : [1229/8999], Training Loss: 2.061509, Validation Loss: 2.062503\n",
      "Epoch : [1230/8999], Training Loss: 2.060879, Validation Loss: 2.061866\n",
      "Epoch : [1231/8999], Training Loss: 2.061056, Validation Loss: 2.062272\n",
      "Epoch : [1232/8999], Training Loss: 2.060692, Validation Loss: 2.062548\n",
      "Epoch : [1233/8999], Training Loss: 2.060598, Validation Loss: 2.062612\n",
      "Epoch : [1234/8999], Training Loss: 2.060895, Validation Loss: 2.062029\n",
      "Epoch : [1235/8999], Training Loss: 2.062239, Validation Loss: 2.063489\n",
      "Epoch : [1236/8999], Training Loss: 2.061633, Validation Loss: 2.063367\n",
      "Epoch : [1237/8999], Training Loss: 2.061230, Validation Loss: 2.062612\n",
      "Epoch : [1238/8999], Training Loss: 2.062042, Validation Loss: 2.062072\n",
      "Epoch : [1239/8999], Training Loss: 2.064942, Validation Loss: 2.061558\n",
      "Epoch : [1240/8999], Training Loss: 2.061929, Validation Loss: 2.064008\n",
      "Epoch : [1241/8999], Training Loss: 2.060540, Validation Loss: 2.062922\n",
      "Epoch : [1242/8999], Training Loss: 2.061283, Validation Loss: 2.066077\n",
      "Epoch : [1243/8999], Training Loss: 2.064393, Validation Loss: 2.064545\n",
      "Epoch : [1244/8999], Training Loss: 2.063510, Validation Loss: 2.062455\n",
      "Epoch : [1245/8999], Training Loss: 2.060919, Validation Loss: 2.062607\n",
      "Epoch : [1246/8999], Training Loss: 2.063925, Validation Loss: 2.065623\n",
      "Epoch : [1247/8999], Training Loss: 2.075360, Validation Loss: 2.074689\n",
      "Epoch : [1248/8999], Training Loss: 2.071060, Validation Loss: 2.064727\n",
      "Epoch : [1249/8999], Training Loss: 2.063714, Validation Loss: 2.063477\n",
      "Epoch : [1250/8999], Training Loss: 2.064405, Validation Loss: 2.060616\n",
      "Epoch : [1251/8999], Training Loss: 2.060559, Validation Loss: 2.058901\n",
      "Epoch : [1252/8999], Training Loss: 2.059195, Validation Loss: 2.058579\n",
      "Epoch : [1253/8999], Training Loss: 2.059513, Validation Loss: 2.058041\n",
      "Epoch : [1254/8999], Training Loss: 2.058163, Validation Loss: 2.059146\n",
      "Epoch : [1255/8999], Training Loss: 2.058295, Validation Loss: 2.059750\n",
      "Epoch : [1256/8999], Training Loss: 2.058199, Validation Loss: 2.059851\n",
      "Epoch : [1257/8999], Training Loss: 2.058281, Validation Loss: 2.060227\n",
      "Epoch : [1258/8999], Training Loss: 2.058194, Validation Loss: 2.059814\n",
      "Epoch : [1259/8999], Training Loss: 2.058171, Validation Loss: 2.060185\n",
      "Epoch : [1260/8999], Training Loss: 2.058182, Validation Loss: 2.060271\n",
      "Epoch : [1261/8999], Training Loss: 2.058148, Validation Loss: 2.059941\n",
      "Epoch : [1262/8999], Training Loss: 2.058043, Validation Loss: 2.059789\n",
      "Epoch : [1263/8999], Training Loss: 2.057925, Validation Loss: 2.059659\n",
      "Epoch : [1264/8999], Training Loss: 2.057805, Validation Loss: 2.059413\n",
      "Epoch : [1265/8999], Training Loss: 2.057682, Validation Loss: 2.059158\n",
      "Epoch : [1266/8999], Training Loss: 2.057572, Validation Loss: 2.058870\n",
      "Epoch : [1267/8999], Training Loss: 2.057470, Validation Loss: 2.058598\n",
      "Epoch : [1268/8999], Training Loss: 2.057360, Validation Loss: 2.058226\n",
      "Epoch : [1269/8999], Training Loss: 2.057236, Validation Loss: 2.057837\n",
      "Epoch : [1270/8999], Training Loss: 2.057108, Validation Loss: 2.057420\n",
      "Epoch : [1271/8999], Training Loss: 2.057015, Validation Loss: 2.057045\n",
      "Epoch : [1272/8999], Training Loss: 2.056910, Validation Loss: 2.056705\n",
      "Epoch : [1273/8999], Training Loss: 2.056771, Validation Loss: 2.056346\n",
      "Epoch : [1274/8999], Training Loss: 2.056621, Validation Loss: 2.055804\n",
      "Epoch : [1275/8999], Training Loss: 2.056414, Validation Loss: 2.055630\n",
      "Epoch : [1276/8999], Training Loss: 2.057228, Validation Loss: 2.058507\n",
      "Epoch : [1277/8999], Training Loss: 2.058290, Validation Loss: 2.059670\n",
      "Epoch : [1278/8999], Training Loss: 2.060568, Validation Loss: 2.060867\n",
      "Epoch : [1279/8999], Training Loss: 2.061366, Validation Loss: 2.056715\n",
      "Epoch : [1280/8999], Training Loss: 2.056501, Validation Loss: 2.055262\n",
      "Epoch : [1281/8999], Training Loss: 2.056314, Validation Loss: 2.056951\n",
      "Epoch : [1282/8999], Training Loss: 2.057069, Validation Loss: 2.055667\n",
      "Epoch : [1283/8999], Training Loss: 2.055807, Validation Loss: 2.058257\n",
      "Epoch : [1284/8999], Training Loss: 2.056195, Validation Loss: 2.057280\n",
      "Epoch : [1285/8999], Training Loss: 2.056243, Validation Loss: 2.060378\n",
      "Epoch : [1286/8999], Training Loss: 2.056223, Validation Loss: 2.063442\n",
      "Epoch : [1287/8999], Training Loss: 2.058449, Validation Loss: 2.061508\n",
      "Epoch : [1288/8999], Training Loss: 2.056869, Validation Loss: 2.060238\n",
      "Epoch : [1289/8999], Training Loss: 2.057320, Validation Loss: 2.065897\n",
      "Epoch : [1290/8999], Training Loss: 2.058363, Validation Loss: 2.059663\n",
      "Epoch : [1291/8999], Training Loss: 2.058660, Validation Loss: 2.070532\n",
      "Epoch : [1292/8999], Training Loss: 2.062437, Validation Loss: 2.067000\n",
      "Epoch : [1293/8999], Training Loss: 2.067372, Validation Loss: 2.070595\n",
      "Epoch : [1294/8999], Training Loss: 2.068331, Validation Loss: 2.060562\n",
      "Epoch : [1295/8999], Training Loss: 2.058740, Validation Loss: 2.058171\n",
      "Epoch : [1296/8999], Training Loss: 2.059204, Validation Loss: 2.057396\n",
      "Epoch : [1297/8999], Training Loss: 2.056303, Validation Loss: 2.056550\n",
      "Epoch : [1298/8999], Training Loss: 2.059195, Validation Loss: 2.055656\n",
      "Epoch : [1299/8999], Training Loss: 2.059374, Validation Loss: 2.061167\n",
      "Epoch : [1300/8999], Training Loss: 2.061475, Validation Loss: 2.054556\n",
      "Epoch : [1301/8999], Training Loss: 2.054906, Validation Loss: 2.054873\n",
      "Epoch : [1302/8999], Training Loss: 2.055366, Validation Loss: 2.054327\n",
      "Epoch : [1303/8999], Training Loss: 2.054355, Validation Loss: 2.054945\n",
      "Epoch : [1304/8999], Training Loss: 2.054108, Validation Loss: 2.053435\n",
      "Epoch : [1305/8999], Training Loss: 2.053789, Validation Loss: 2.054418\n",
      "Epoch : [1306/8999], Training Loss: 2.053653, Validation Loss: 2.054338\n",
      "Epoch : [1307/8999], Training Loss: 2.053567, Validation Loss: 2.054242\n",
      "Epoch : [1308/8999], Training Loss: 2.053420, Validation Loss: 2.054192\n",
      "Epoch : [1309/8999], Training Loss: 2.053283, Validation Loss: 2.054148\n",
      "Epoch : [1310/8999], Training Loss: 2.053160, Validation Loss: 2.054867\n",
      "Epoch : [1311/8999], Training Loss: 2.053886, Validation Loss: 2.054407\n",
      "Epoch : [1312/8999], Training Loss: 2.054639, Validation Loss: 2.055053\n",
      "Epoch : [1313/8999], Training Loss: 2.055211, Validation Loss: 2.057990\n",
      "Epoch : [1314/8999], Training Loss: 2.053948, Validation Loss: 2.055026\n",
      "Epoch : [1315/8999], Training Loss: 2.053064, Validation Loss: 2.053517\n",
      "Epoch : [1316/8999], Training Loss: 2.053006, Validation Loss: 2.054631\n",
      "Epoch : [1317/8999], Training Loss: 2.052859, Validation Loss: 2.054283\n",
      "Epoch : [1318/8999], Training Loss: 2.052879, Validation Loss: 2.053496\n",
      "Epoch : [1319/8999], Training Loss: 2.052650, Validation Loss: 2.054291\n",
      "Epoch : [1320/8999], Training Loss: 2.052655, Validation Loss: 2.054485\n",
      "Epoch : [1321/8999], Training Loss: 2.052648, Validation Loss: 2.053088\n",
      "Epoch : [1322/8999], Training Loss: 2.052442, Validation Loss: 2.052725\n",
      "Epoch : [1323/8999], Training Loss: 2.052324, Validation Loss: 2.053179\n",
      "Epoch : [1324/8999], Training Loss: 2.052282, Validation Loss: 2.053159\n",
      "Epoch : [1325/8999], Training Loss: 2.052457, Validation Loss: 2.051566\n",
      "Epoch : [1326/8999], Training Loss: 2.052457, Validation Loss: 2.050890\n",
      "Epoch : [1327/8999], Training Loss: 2.051951, Validation Loss: 2.051086\n",
      "Epoch : [1328/8999], Training Loss: 2.052367, Validation Loss: 2.051816\n",
      "Epoch : [1329/8999], Training Loss: 2.051820, Validation Loss: 2.051711\n",
      "Epoch : [1330/8999], Training Loss: 2.051362, Validation Loss: 2.049823\n",
      "Epoch : [1331/8999], Training Loss: 2.050258, Validation Loss: 2.049801\n",
      "Epoch : [1332/8999], Training Loss: 2.050134, Validation Loss: 2.049266\n",
      "Epoch : [1333/8999], Training Loss: 2.049921, Validation Loss: 2.049753\n",
      "Epoch : [1334/8999], Training Loss: 2.050020, Validation Loss: 2.049521\n",
      "Epoch : [1335/8999], Training Loss: 2.049769, Validation Loss: 2.049404\n",
      "Epoch : [1336/8999], Training Loss: 2.049621, Validation Loss: 2.048693\n",
      "Epoch : [1337/8999], Training Loss: 2.049595, Validation Loss: 2.048649\n",
      "Epoch : [1338/8999], Training Loss: 2.049462, Validation Loss: 2.048751\n",
      "Epoch : [1339/8999], Training Loss: 2.049362, Validation Loss: 2.048303\n",
      "Epoch : [1340/8999], Training Loss: 2.048914, Validation Loss: 2.048028\n",
      "Epoch : [1341/8999], Training Loss: 2.048728, Validation Loss: 2.047884\n",
      "Epoch : [1342/8999], Training Loss: 2.048616, Validation Loss: 2.047824\n",
      "Epoch : [1343/8999], Training Loss: 2.048510, Validation Loss: 2.047932\n",
      "Epoch : [1344/8999], Training Loss: 2.048470, Validation Loss: 2.047713\n",
      "Epoch : [1345/8999], Training Loss: 2.048307, Validation Loss: 2.047541\n",
      "Epoch : [1346/8999], Training Loss: 2.048208, Validation Loss: 2.047373\n",
      "Epoch : [1347/8999], Training Loss: 2.048196, Validation Loss: 2.047361\n",
      "Epoch : [1348/8999], Training Loss: 2.048698, Validation Loss: 2.048603\n",
      "Epoch : [1349/8999], Training Loss: 2.049878, Validation Loss: 2.052938\n",
      "Epoch : [1350/8999], Training Loss: 2.052328, Validation Loss: 2.054160\n",
      "Epoch : [1351/8999], Training Loss: 2.055824, Validation Loss: 2.058491\n",
      "Epoch : [1352/8999], Training Loss: 2.057019, Validation Loss: 2.050858\n",
      "Epoch : [1353/8999], Training Loss: 2.051780, Validation Loss: 2.056737\n",
      "Epoch : [1354/8999], Training Loss: 2.052340, Validation Loss: 2.053383\n",
      "Epoch : [1355/8999], Training Loss: 2.055552, Validation Loss: 2.063453\n",
      "Epoch : [1356/8999], Training Loss: 2.062158, Validation Loss: 2.051052\n",
      "Epoch : [1357/8999], Training Loss: 2.050577, Validation Loss: 2.049315\n",
      "Epoch : [1358/8999], Training Loss: 2.048423, Validation Loss: 2.047588\n",
      "Epoch : [1359/8999], Training Loss: 2.048140, Validation Loss: 2.045981\n",
      "Epoch : [1360/8999], Training Loss: 2.046586, Validation Loss: 2.045931\n",
      "Epoch : [1361/8999], Training Loss: 2.046287, Validation Loss: 2.045784\n",
      "Epoch : [1362/8999], Training Loss: 2.046504, Validation Loss: 2.045847\n",
      "Epoch : [1363/8999], Training Loss: 2.047948, Validation Loss: 2.047328\n",
      "Epoch : [1364/8999], Training Loss: 2.047322, Validation Loss: 2.046045\n",
      "Epoch : [1365/8999], Training Loss: 2.046626, Validation Loss: 2.045066\n",
      "Epoch : [1366/8999], Training Loss: 2.045622, Validation Loss: 2.045095\n",
      "Epoch : [1367/8999], Training Loss: 2.045692, Validation Loss: 2.046672\n",
      "Epoch : [1368/8999], Training Loss: 2.046115, Validation Loss: 2.045759\n",
      "Epoch : [1369/8999], Training Loss: 2.046600, Validation Loss: 2.047620\n",
      "Epoch : [1370/8999], Training Loss: 2.046527, Validation Loss: 2.045843\n",
      "Epoch : [1371/8999], Training Loss: 2.045332, Validation Loss: 2.046707\n",
      "Epoch : [1372/8999], Training Loss: 2.045428, Validation Loss: 2.046654\n",
      "Epoch : [1373/8999], Training Loss: 2.045297, Validation Loss: 2.047632\n",
      "Epoch : [1374/8999], Training Loss: 2.045210, Validation Loss: 2.048327\n",
      "Epoch : [1375/8999], Training Loss: 2.045266, Validation Loss: 2.046965\n",
      "Epoch : [1376/8999], Training Loss: 2.047325, Validation Loss: 2.051448\n",
      "Epoch : [1377/8999], Training Loss: 2.047289, Validation Loss: 2.051931\n",
      "Epoch : [1378/8999], Training Loss: 2.053110, Validation Loss: 2.048780\n",
      "Epoch : [1379/8999], Training Loss: 2.048384, Validation Loss: 2.045722\n",
      "Epoch : [1380/8999], Training Loss: 2.047140, Validation Loss: 2.052805\n",
      "Epoch : [1381/8999], Training Loss: 2.049281, Validation Loss: 2.046278\n",
      "Epoch : [1382/8999], Training Loss: 2.046278, Validation Loss: 2.046712\n",
      "Epoch : [1383/8999], Training Loss: 2.046651, Validation Loss: 2.043944\n",
      "Epoch : [1384/8999], Training Loss: 2.044576, Validation Loss: 2.050172\n",
      "Epoch : [1385/8999], Training Loss: 2.045799, Validation Loss: 2.045435\n",
      "Epoch : [1386/8999], Training Loss: 2.046572, Validation Loss: 2.045489\n",
      "Epoch : [1387/8999], Training Loss: 2.044901, Validation Loss: 2.048932\n",
      "Epoch : [1388/8999], Training Loss: 2.045356, Validation Loss: 2.044717\n",
      "Epoch : [1389/8999], Training Loss: 2.044948, Validation Loss: 2.044004\n",
      "Epoch : [1390/8999], Training Loss: 2.044050, Validation Loss: 2.044827\n",
      "Epoch : [1391/8999], Training Loss: 2.043983, Validation Loss: 2.044971\n",
      "Epoch : [1392/8999], Training Loss: 2.043987, Validation Loss: 2.044853\n",
      "Epoch : [1393/8999], Training Loss: 2.043948, Validation Loss: 2.045106\n",
      "Epoch : [1394/8999], Training Loss: 2.043923, Validation Loss: 2.045030\n",
      "Epoch : [1395/8999], Training Loss: 2.043985, Validation Loss: 2.043681\n",
      "Epoch : [1396/8999], Training Loss: 2.043884, Validation Loss: 2.043298\n",
      "Epoch : [1397/8999], Training Loss: 2.043746, Validation Loss: 2.043753\n",
      "Epoch : [1398/8999], Training Loss: 2.043592, Validation Loss: 2.043116\n",
      "Epoch : [1399/8999], Training Loss: 2.043542, Validation Loss: 2.042646\n",
      "Epoch : [1400/8999], Training Loss: 2.043318, Validation Loss: 2.042551\n",
      "Epoch : [1401/8999], Training Loss: 2.043164, Validation Loss: 2.042472\n",
      "Epoch : [1402/8999], Training Loss: 2.043115, Validation Loss: 2.042650\n",
      "Epoch : [1403/8999], Training Loss: 2.043089, Validation Loss: 2.042945\n",
      "Epoch : [1404/8999], Training Loss: 2.043264, Validation Loss: 2.042925\n",
      "Epoch : [1405/8999], Training Loss: 2.043255, Validation Loss: 2.042890\n",
      "Epoch : [1406/8999], Training Loss: 2.043007, Validation Loss: 2.042936\n",
      "Epoch : [1407/8999], Training Loss: 2.043059, Validation Loss: 2.042452\n",
      "Epoch : [1408/8999], Training Loss: 2.042612, Validation Loss: 2.042019\n",
      "Epoch : [1409/8999], Training Loss: 2.042440, Validation Loss: 2.041451\n",
      "Epoch : [1410/8999], Training Loss: 2.042059, Validation Loss: 2.041160\n",
      "Epoch : [1411/8999], Training Loss: 2.041852, Validation Loss: 2.041070\n",
      "Epoch : [1412/8999], Training Loss: 2.041684, Validation Loss: 2.041038\n",
      "Epoch : [1413/8999], Training Loss: 2.041540, Validation Loss: 2.041016\n",
      "Epoch : [1414/8999], Training Loss: 2.041387, Validation Loss: 2.040946\n",
      "Epoch : [1415/8999], Training Loss: 2.041458, Validation Loss: 2.041254\n",
      "Epoch : [1416/8999], Training Loss: 2.041308, Validation Loss: 2.041205\n",
      "Epoch : [1417/8999], Training Loss: 2.041507, Validation Loss: 2.041069\n",
      "Epoch : [1418/8999], Training Loss: 2.041181, Validation Loss: 2.040755\n",
      "Epoch : [1419/8999], Training Loss: 2.041140, Validation Loss: 2.041181\n",
      "Epoch : [1420/8999], Training Loss: 2.040892, Validation Loss: 2.041027\n",
      "Epoch : [1421/8999], Training Loss: 2.040925, Validation Loss: 2.041289\n",
      "Epoch : [1422/8999], Training Loss: 2.040796, Validation Loss: 2.041657\n",
      "Epoch : [1423/8999], Training Loss: 2.040855, Validation Loss: 2.041735\n",
      "Epoch : [1424/8999], Training Loss: 2.040921, Validation Loss: 2.041200\n",
      "Epoch : [1425/8999], Training Loss: 2.040743, Validation Loss: 2.041941\n",
      "Epoch : [1426/8999], Training Loss: 2.040808, Validation Loss: 2.040541\n",
      "Epoch : [1427/8999], Training Loss: 2.040666, Validation Loss: 2.041737\n",
      "Epoch : [1428/8999], Training Loss: 2.040581, Validation Loss: 2.040334\n",
      "Epoch : [1429/8999], Training Loss: 2.040589, Validation Loss: 2.041066\n",
      "Epoch : [1430/8999], Training Loss: 2.040118, Validation Loss: 2.040169\n",
      "Epoch : [1431/8999], Training Loss: 2.040203, Validation Loss: 2.040332\n",
      "Epoch : [1432/8999], Training Loss: 2.039924, Validation Loss: 2.040267\n",
      "Epoch : [1433/8999], Training Loss: 2.039784, Validation Loss: 2.040172\n",
      "Epoch : [1434/8999], Training Loss: 2.039573, Validation Loss: 2.039581\n",
      "Epoch : [1435/8999], Training Loss: 2.039551, Validation Loss: 2.040040\n",
      "Epoch : [1436/8999], Training Loss: 2.039422, Validation Loss: 2.039323\n",
      "Epoch : [1437/8999], Training Loss: 2.039355, Validation Loss: 2.040291\n",
      "Epoch : [1438/8999], Training Loss: 2.039341, Validation Loss: 2.038985\n",
      "Epoch : [1439/8999], Training Loss: 2.039257, Validation Loss: 2.040000\n",
      "Epoch : [1440/8999], Training Loss: 2.039229, Validation Loss: 2.038840\n",
      "Epoch : [1441/8999], Training Loss: 2.039930, Validation Loss: 2.039249\n",
      "Epoch : [1442/8999], Training Loss: 2.039732, Validation Loss: 2.039018\n",
      "Epoch : [1443/8999], Training Loss: 2.039537, Validation Loss: 2.041299\n",
      "Epoch : [1444/8999], Training Loss: 2.040497, Validation Loss: 2.039176\n",
      "Epoch : [1445/8999], Training Loss: 2.039522, Validation Loss: 2.041468\n",
      "Epoch : [1446/8999], Training Loss: 2.039589, Validation Loss: 2.038805\n",
      "Epoch : [1447/8999], Training Loss: 2.039279, Validation Loss: 2.039012\n",
      "Epoch : [1448/8999], Training Loss: 2.039127, Validation Loss: 2.038578\n",
      "Epoch : [1449/8999], Training Loss: 2.038404, Validation Loss: 2.040002\n",
      "Epoch : [1450/8999], Training Loss: 2.038451, Validation Loss: 2.038271\n",
      "Epoch : [1451/8999], Training Loss: 2.038539, Validation Loss: 2.037937\n",
      "Epoch : [1452/8999], Training Loss: 2.037928, Validation Loss: 2.037078\n",
      "Epoch : [1453/8999], Training Loss: 2.038191, Validation Loss: 2.037949\n",
      "Epoch : [1454/8999], Training Loss: 2.038258, Validation Loss: 2.037233\n",
      "Epoch : [1455/8999], Training Loss: 2.038134, Validation Loss: 2.040594\n",
      "Epoch : [1456/8999], Training Loss: 2.038345, Validation Loss: 2.037861\n",
      "Epoch : [1457/8999], Training Loss: 2.038315, Validation Loss: 2.037683\n",
      "Epoch : [1458/8999], Training Loss: 2.037486, Validation Loss: 2.036626\n",
      "Epoch : [1459/8999], Training Loss: 2.037551, Validation Loss: 2.037709\n",
      "Epoch : [1460/8999], Training Loss: 2.037379, Validation Loss: 2.037645\n",
      "Epoch : [1461/8999], Training Loss: 2.037718, Validation Loss: 2.037510\n",
      "Epoch : [1462/8999], Training Loss: 2.037285, Validation Loss: 2.036616\n",
      "Epoch : [1463/8999], Training Loss: 2.037060, Validation Loss: 2.036545\n",
      "Epoch : [1464/8999], Training Loss: 2.037002, Validation Loss: 2.037220\n",
      "Epoch : [1465/8999], Training Loss: 2.036996, Validation Loss: 2.036812\n",
      "Epoch : [1466/8999], Training Loss: 2.036970, Validation Loss: 2.036245\n",
      "Epoch : [1467/8999], Training Loss: 2.036874, Validation Loss: 2.036340\n",
      "Epoch : [1468/8999], Training Loss: 2.036579, Validation Loss: 2.037364\n",
      "Epoch : [1469/8999], Training Loss: 2.037160, Validation Loss: 2.036970\n",
      "Epoch : [1470/8999], Training Loss: 2.037698, Validation Loss: 2.038307\n",
      "Epoch : [1471/8999], Training Loss: 2.037312, Validation Loss: 2.041841\n",
      "Epoch : [1472/8999], Training Loss: 2.039378, Validation Loss: 2.040470\n",
      "Epoch : [1473/8999], Training Loss: 2.039969, Validation Loss: 2.037899\n",
      "Epoch : [1474/8999], Training Loss: 2.037311, Validation Loss: 2.038847\n",
      "Epoch : [1475/8999], Training Loss: 2.038046, Validation Loss: 2.037877\n",
      "Epoch : [1476/8999], Training Loss: 2.039931, Validation Loss: 2.042044\n",
      "Epoch : [1477/8999], Training Loss: 2.040793, Validation Loss: 2.042542\n",
      "Epoch : [1478/8999], Training Loss: 2.042887, Validation Loss: 2.043738\n",
      "Epoch : [1479/8999], Training Loss: 2.043366, Validation Loss: 2.040697\n",
      "Epoch : [1480/8999], Training Loss: 2.038779, Validation Loss: 2.037736\n",
      "Epoch : [1481/8999], Training Loss: 2.037672, Validation Loss: 2.042404\n",
      "Epoch : [1482/8999], Training Loss: 2.037936, Validation Loss: 2.040560\n",
      "Epoch : [1483/8999], Training Loss: 2.040088, Validation Loss: 2.047153\n",
      "Epoch : [1484/8999], Training Loss: 2.038379, Validation Loss: 2.048749\n",
      "Epoch : [1485/8999], Training Loss: 2.038412, Validation Loss: 2.048244\n",
      "Epoch : [1486/8999], Training Loss: 2.038519, Validation Loss: 2.042158\n",
      "Epoch : [1487/8999], Training Loss: 2.038140, Validation Loss: 2.044595\n",
      "Epoch : [1488/8999], Training Loss: 2.038297, Validation Loss: 2.045037\n",
      "Epoch : [1489/8999], Training Loss: 2.039555, Validation Loss: 2.045983\n",
      "Epoch : [1490/8999], Training Loss: 2.038750, Validation Loss: 2.041584\n",
      "Epoch : [1491/8999], Training Loss: 2.038588, Validation Loss: 2.043970\n",
      "Epoch : [1492/8999], Training Loss: 2.037944, Validation Loss: 2.045155\n",
      "Epoch : [1493/8999], Training Loss: 2.037951, Validation Loss: 2.041701\n",
      "Epoch : [1494/8999], Training Loss: 2.037788, Validation Loss: 2.040387\n",
      "Epoch : [1495/8999], Training Loss: 2.038248, Validation Loss: 2.040287\n",
      "Epoch : [1496/8999], Training Loss: 2.037782, Validation Loss: 2.039190\n",
      "Epoch : [1497/8999], Training Loss: 2.038081, Validation Loss: 2.042910\n",
      "Epoch : [1498/8999], Training Loss: 2.042930, Validation Loss: 2.052302\n",
      "Epoch : [1499/8999], Training Loss: 2.042184, Validation Loss: 2.037130\n",
      "Epoch : [1500/8999], Training Loss: 2.037272, Validation Loss: 2.037062\n",
      "Epoch : [1501/8999], Training Loss: 2.037713, Validation Loss: 2.039548\n",
      "Epoch : [1502/8999], Training Loss: 2.052954, Validation Loss: 2.046580\n",
      "Epoch : [1503/8999], Training Loss: 2.045227, Validation Loss: 2.037521\n",
      "Epoch : [1504/8999], Training Loss: 2.040721, Validation Loss: 2.047555\n",
      "Epoch : [1505/8999], Training Loss: 2.041513, Validation Loss: 2.039796\n",
      "Epoch : [1506/8999], Training Loss: 2.041442, Validation Loss: 2.041759\n",
      "Epoch : [1507/8999], Training Loss: 2.044326, Validation Loss: 2.050026\n",
      "Epoch : [1508/8999], Training Loss: 2.039910, Validation Loss: 2.036548\n",
      "Epoch : [1509/8999], Training Loss: 2.038235, Validation Loss: 2.036796\n",
      "Epoch : [1510/8999], Training Loss: 2.038493, Validation Loss: 2.041919\n",
      "Epoch : [1511/8999], Training Loss: 2.038373, Validation Loss: 2.036138\n",
      "Epoch : [1512/8999], Training Loss: 2.039281, Validation Loss: 2.039371\n",
      "Epoch : [1513/8999], Training Loss: 2.044819, Validation Loss: 2.046942\n",
      "Epoch : [1514/8999], Training Loss: 2.044803, Validation Loss: 2.036851\n",
      "Epoch : [1515/8999], Training Loss: 2.041852, Validation Loss: 2.039799\n",
      "Epoch : [1516/8999], Training Loss: 2.037288, Validation Loss: 2.035813\n",
      "Epoch : [1517/8999], Training Loss: 2.036124, Validation Loss: 2.034380\n",
      "Epoch : [1518/8999], Training Loss: 2.034455, Validation Loss: 2.034461\n",
      "Epoch : [1519/8999], Training Loss: 2.035196, Validation Loss: 2.033130\n",
      "Epoch : [1520/8999], Training Loss: 2.034016, Validation Loss: 2.033481\n",
      "Epoch : [1521/8999], Training Loss: 2.033906, Validation Loss: 2.034331\n",
      "Epoch : [1522/8999], Training Loss: 2.033832, Validation Loss: 2.035789\n",
      "Epoch : [1523/8999], Training Loss: 2.033335, Validation Loss: 2.034181\n",
      "Epoch : [1524/8999], Training Loss: 2.033634, Validation Loss: 2.036782\n",
      "Epoch : [1525/8999], Training Loss: 2.033356, Validation Loss: 2.034489\n",
      "Epoch : [1526/8999], Training Loss: 2.033537, Validation Loss: 2.036970\n",
      "Epoch : [1527/8999], Training Loss: 2.034126, Validation Loss: 2.034625\n",
      "Epoch : [1528/8999], Training Loss: 2.033643, Validation Loss: 2.034987\n",
      "Epoch : [1529/8999], Training Loss: 2.033649, Validation Loss: 2.036392\n",
      "Epoch : [1530/8999], Training Loss: 2.033826, Validation Loss: 2.036139\n",
      "Epoch : [1531/8999], Training Loss: 2.033758, Validation Loss: 2.033869\n",
      "Epoch : [1532/8999], Training Loss: 2.033311, Validation Loss: 2.033586\n",
      "Epoch : [1533/8999], Training Loss: 2.033123, Validation Loss: 2.034234\n",
      "Epoch : [1534/8999], Training Loss: 2.033154, Validation Loss: 2.033286\n",
      "Epoch : [1535/8999], Training Loss: 2.032997, Validation Loss: 2.032301\n",
      "Epoch : [1536/8999], Training Loss: 2.032893, Validation Loss: 2.031745\n",
      "Epoch : [1537/8999], Training Loss: 2.032776, Validation Loss: 2.048635\n",
      "Epoch : [1538/8999], Training Loss: 2.043931, Validation Loss: 2.036097\n",
      "Epoch : [1539/8999], Training Loss: 2.038044, Validation Loss: 2.040017\n",
      "Epoch : [1540/8999], Training Loss: 2.037274, Validation Loss: 2.037544\n",
      "Epoch : [1541/8999], Training Loss: 2.035317, Validation Loss: 2.033224\n",
      "Epoch : [1542/8999], Training Loss: 2.033756, Validation Loss: 2.032185\n",
      "Epoch : [1543/8999], Training Loss: 2.033066, Validation Loss: 2.031956\n",
      "Epoch : [1544/8999], Training Loss: 2.032584, Validation Loss: 2.031983\n",
      "Epoch : [1545/8999], Training Loss: 2.032115, Validation Loss: 2.031480\n",
      "Epoch : [1546/8999], Training Loss: 2.031616, Validation Loss: 2.031113\n",
      "Epoch : [1547/8999], Training Loss: 2.031542, Validation Loss: 2.031002\n",
      "Epoch : [1548/8999], Training Loss: 2.031402, Validation Loss: 2.030873\n",
      "Epoch : [1549/8999], Training Loss: 2.031369, Validation Loss: 2.030922\n",
      "Epoch : [1550/8999], Training Loss: 2.031283, Validation Loss: 2.030838\n",
      "Epoch : [1551/8999], Training Loss: 2.031202, Validation Loss: 2.030755\n",
      "Epoch : [1552/8999], Training Loss: 2.031169, Validation Loss: 2.030754\n",
      "Epoch : [1553/8999], Training Loss: 2.031021, Validation Loss: 2.030649\n",
      "Epoch : [1554/8999], Training Loss: 2.030951, Validation Loss: 2.030605\n",
      "Epoch : [1555/8999], Training Loss: 2.030943, Validation Loss: 2.030543\n",
      "Epoch : [1556/8999], Training Loss: 2.030839, Validation Loss: 2.030532\n",
      "Epoch : [1557/8999], Training Loss: 2.030903, Validation Loss: 2.030503\n",
      "Epoch : [1558/8999], Training Loss: 2.030821, Validation Loss: 2.030464\n",
      "Epoch : [1559/8999], Training Loss: 2.030876, Validation Loss: 2.030449\n",
      "Epoch : [1560/8999], Training Loss: 2.030811, Validation Loss: 2.030484\n",
      "Epoch : [1561/8999], Training Loss: 2.030777, Validation Loss: 2.030429\n",
      "Epoch : [1562/8999], Training Loss: 2.030649, Validation Loss: 2.030275\n",
      "Epoch : [1563/8999], Training Loss: 2.030520, Validation Loss: 2.030232\n",
      "Epoch : [1564/8999], Training Loss: 2.030527, Validation Loss: 2.030524\n",
      "Epoch : [1565/8999], Training Loss: 2.030692, Validation Loss: 2.030569\n",
      "Epoch : [1566/8999], Training Loss: 2.030724, Validation Loss: 2.030282\n",
      "Epoch : [1567/8999], Training Loss: 2.030547, Validation Loss: 2.030187\n",
      "Epoch : [1568/8999], Training Loss: 2.030594, Validation Loss: 2.030355\n",
      "Epoch : [1569/8999], Training Loss: 2.030761, Validation Loss: 2.030177\n",
      "Epoch : [1570/8999], Training Loss: 2.030523, Validation Loss: 2.030344\n",
      "Epoch : [1571/8999], Training Loss: 2.030420, Validation Loss: 2.030087\n",
      "Epoch : [1572/8999], Training Loss: 2.030318, Validation Loss: 2.029879\n",
      "Epoch : [1573/8999], Training Loss: 2.030218, Validation Loss: 2.029926\n",
      "Epoch : [1574/8999], Training Loss: 2.030156, Validation Loss: 2.030185\n",
      "Epoch : [1575/8999], Training Loss: 2.030255, Validation Loss: 2.030199\n",
      "Epoch : [1576/8999], Training Loss: 2.030266, Validation Loss: 2.029990\n",
      "Epoch : [1577/8999], Training Loss: 2.030173, Validation Loss: 2.029967\n",
      "Epoch : [1578/8999], Training Loss: 2.030234, Validation Loss: 2.030056\n",
      "Epoch : [1579/8999], Training Loss: 2.030411, Validation Loss: 2.029961\n",
      "Epoch : [1580/8999], Training Loss: 2.030182, Validation Loss: 2.029854\n",
      "Epoch : [1581/8999], Training Loss: 2.030103, Validation Loss: 2.030010\n",
      "Epoch : [1582/8999], Training Loss: 2.029991, Validation Loss: 2.029869\n",
      "Epoch : [1583/8999], Training Loss: 2.029947, Validation Loss: 2.029625\n",
      "Epoch : [1584/8999], Training Loss: 2.029883, Validation Loss: 2.029697\n",
      "Epoch : [1585/8999], Training Loss: 2.029887, Validation Loss: 2.029966\n",
      "Epoch : [1586/8999], Training Loss: 2.030015, Validation Loss: 2.029984\n",
      "Epoch : [1587/8999], Training Loss: 2.029989, Validation Loss: 2.029698\n",
      "Epoch : [1588/8999], Training Loss: 2.029885, Validation Loss: 2.029688\n",
      "Epoch : [1589/8999], Training Loss: 2.029907, Validation Loss: 2.029733\n",
      "Epoch : [1590/8999], Training Loss: 2.030001, Validation Loss: 2.029827\n",
      "Epoch : [1591/8999], Training Loss: 2.029984, Validation Loss: 2.029475\n",
      "Epoch : [1592/8999], Training Loss: 2.029673, Validation Loss: 2.029618\n",
      "Epoch : [1593/8999], Training Loss: 2.029765, Validation Loss: 2.029616\n",
      "Epoch : [1594/8999], Training Loss: 2.029710, Validation Loss: 2.029714\n",
      "Epoch : [1595/8999], Training Loss: 2.029585, Validation Loss: 2.029463\n",
      "Epoch : [1596/8999], Training Loss: 2.029637, Validation Loss: 2.029426\n",
      "Epoch : [1597/8999], Training Loss: 2.029777, Validation Loss: 2.029755\n",
      "Epoch : [1598/8999], Training Loss: 2.029793, Validation Loss: 2.029435\n",
      "Epoch : [1599/8999], Training Loss: 2.029512, Validation Loss: 2.029254\n",
      "Epoch : [1600/8999], Training Loss: 2.029398, Validation Loss: 2.029326\n",
      "Epoch : [1601/8999], Training Loss: 2.029343, Validation Loss: 2.029189\n",
      "Epoch : [1602/8999], Training Loss: 2.029306, Validation Loss: 2.029186\n",
      "Epoch : [1603/8999], Training Loss: 2.029395, Validation Loss: 2.029151\n",
      "Epoch : [1604/8999], Training Loss: 2.029423, Validation Loss: 2.029104\n",
      "Epoch : [1605/8999], Training Loss: 2.029350, Validation Loss: 2.029110\n",
      "Epoch : [1606/8999], Training Loss: 2.029435, Validation Loss: 2.029147\n",
      "Epoch : [1607/8999], Training Loss: 2.029296, Validation Loss: 2.028936\n",
      "Epoch : [1608/8999], Training Loss: 2.029215, Validation Loss: 2.028900\n",
      "Epoch : [1609/8999], Training Loss: 2.029120, Validation Loss: 2.028926\n",
      "Epoch : [1610/8999], Training Loss: 2.029113, Validation Loss: 2.028954\n",
      "Epoch : [1611/8999], Training Loss: 2.029076, Validation Loss: 2.028924\n",
      "Epoch : [1612/8999], Training Loss: 2.029151, Validation Loss: 2.028989\n",
      "Epoch : [1613/8999], Training Loss: 2.029185, Validation Loss: 2.029049\n",
      "Epoch : [1614/8999], Training Loss: 2.029244, Validation Loss: 2.029105\n",
      "Epoch : [1615/8999], Training Loss: 2.029538, Validation Loss: 2.029301\n",
      "Epoch : [1616/8999], Training Loss: 2.029623, Validation Loss: 2.029297\n",
      "Epoch : [1617/8999], Training Loss: 2.029693, Validation Loss: 2.029471\n",
      "Epoch : [1618/8999], Training Loss: 2.029326, Validation Loss: 2.028873\n",
      "Epoch : [1619/8999], Training Loss: 2.029249, Validation Loss: 2.028906\n",
      "Epoch : [1620/8999], Training Loss: 2.029104, Validation Loss: 2.028921\n",
      "Epoch : [1621/8999], Training Loss: 2.028970, Validation Loss: 2.028878\n",
      "Epoch : [1622/8999], Training Loss: 2.029225, Validation Loss: 2.029316\n",
      "Epoch : [1623/8999], Training Loss: 2.029154, Validation Loss: 2.028923\n",
      "Epoch : [1624/8999], Training Loss: 2.029140, Validation Loss: 2.029540\n",
      "Epoch : [1625/8999], Training Loss: 2.029184, Validation Loss: 2.029287\n",
      "Epoch : [1626/8999], Training Loss: 2.029152, Validation Loss: 2.029527\n",
      "Epoch : [1627/8999], Training Loss: 2.029361, Validation Loss: 2.029059\n",
      "Epoch : [1628/8999], Training Loss: 2.028923, Validation Loss: 2.028962\n",
      "Epoch : [1629/8999], Training Loss: 2.029036, Validation Loss: 2.028905\n",
      "Epoch : [1630/8999], Training Loss: 2.028884, Validation Loss: 2.028771\n",
      "Epoch : [1631/8999], Training Loss: 2.029351, Validation Loss: 2.028829\n",
      "Epoch : [1632/8999], Training Loss: 2.029027, Validation Loss: 2.028786\n",
      "Epoch : [1633/8999], Training Loss: 2.029170, Validation Loss: 2.028558\n",
      "Epoch : [1634/8999], Training Loss: 2.028964, Validation Loss: 2.028767\n",
      "Epoch : [1635/8999], Training Loss: 2.029296, Validation Loss: 2.028606\n",
      "Epoch : [1636/8999], Training Loss: 2.028953, Validation Loss: 2.028596\n",
      "Epoch : [1637/8999], Training Loss: 2.029205, Validation Loss: 2.028570\n",
      "Epoch : [1638/8999], Training Loss: 2.028955, Validation Loss: 2.028556\n",
      "Epoch : [1639/8999], Training Loss: 2.029220, Validation Loss: 2.028611\n",
      "Epoch : [1640/8999], Training Loss: 2.028934, Validation Loss: 2.028489\n",
      "Epoch : [1641/8999], Training Loss: 2.029151, Validation Loss: 2.028600\n",
      "Epoch : [1642/8999], Training Loss: 2.028898, Validation Loss: 2.028440\n",
      "Epoch : [1643/8999], Training Loss: 2.029063, Validation Loss: 2.028563\n",
      "Epoch : [1644/8999], Training Loss: 2.028845, Validation Loss: 2.028416\n",
      "Epoch : [1645/8999], Training Loss: 2.028973, Validation Loss: 2.028544\n",
      "Epoch : [1646/8999], Training Loss: 2.028812, Validation Loss: 2.028435\n",
      "Epoch : [1647/8999], Training Loss: 2.028756, Validation Loss: 2.028427\n",
      "Epoch : [1648/8999], Training Loss: 2.028785, Validation Loss: 2.028509\n",
      "Epoch : [1649/8999], Training Loss: 2.028639, Validation Loss: 2.028479\n",
      "Epoch : [1650/8999], Training Loss: 2.028919, Validation Loss: 2.028792\n",
      "Epoch : [1651/8999], Training Loss: 2.029133, Validation Loss: 2.028735\n",
      "Epoch : [1652/8999], Training Loss: 2.028961, Validation Loss: 2.029333\n",
      "Epoch : [1653/8999], Training Loss: 2.029253, Validation Loss: 2.029554\n",
      "Epoch : [1654/8999], Training Loss: 2.029165, Validation Loss: 2.028871\n",
      "Epoch : [1655/8999], Training Loss: 2.030523, Validation Loss: 2.038859\n",
      "Epoch : [1656/8999], Training Loss: 2.032969, Validation Loss: 2.029945\n",
      "Epoch : [1657/8999], Training Loss: 2.029399, Validation Loss: 2.028778\n",
      "Epoch : [1658/8999], Training Loss: 2.028965, Validation Loss: 2.028965\n",
      "Epoch : [1659/8999], Training Loss: 2.029818, Validation Loss: 2.029798\n",
      "Epoch : [1660/8999], Training Loss: 2.029787, Validation Loss: 2.029557\n",
      "Epoch : [1661/8999], Training Loss: 2.029000, Validation Loss: 2.028345\n",
      "Epoch : [1662/8999], Training Loss: 2.028713, Validation Loss: 2.028823\n",
      "Epoch : [1663/8999], Training Loss: 2.031644, Validation Loss: 2.049252\n",
      "Epoch : [1664/8999], Training Loss: 2.041589, Validation Loss: 2.038756\n",
      "Epoch : [1665/8999], Training Loss: 2.037904, Validation Loss: 2.038658\n",
      "Epoch : [1666/8999], Training Loss: 2.036524, Validation Loss: 2.034624\n",
      "Epoch : [1667/8999], Training Loss: 2.038484, Validation Loss: 2.041585\n",
      "Epoch : [1668/8999], Training Loss: 2.037540, Validation Loss: 2.037217\n",
      "Epoch : [1669/8999], Training Loss: 2.032595, Validation Loss: 2.029664\n",
      "Epoch : [1670/8999], Training Loss: 2.029842, Validation Loss: 2.028786\n",
      "Epoch : [1671/8999], Training Loss: 2.029109, Validation Loss: 2.029201\n",
      "Epoch : [1672/8999], Training Loss: 2.029717, Validation Loss: 2.031349\n",
      "Epoch : [1673/8999], Training Loss: 2.029692, Validation Loss: 2.029291\n",
      "Epoch : [1674/8999], Training Loss: 2.029178, Validation Loss: 2.028752\n",
      "Epoch : [1675/8999], Training Loss: 2.028904, Validation Loss: 2.028161\n",
      "Epoch : [1676/8999], Training Loss: 2.028375, Validation Loss: 2.027995\n",
      "Epoch : [1677/8999], Training Loss: 2.028307, Validation Loss: 2.028031\n",
      "Epoch : [1678/8999], Training Loss: 2.028230, Validation Loss: 2.027942\n",
      "Epoch : [1679/8999], Training Loss: 2.028085, Validation Loss: 2.027811\n",
      "Epoch : [1680/8999], Training Loss: 2.028060, Validation Loss: 2.027843\n",
      "Epoch : [1681/8999], Training Loss: 2.028021, Validation Loss: 2.028372\n",
      "Epoch : [1682/8999], Training Loss: 2.029265, Validation Loss: 2.029890\n",
      "Epoch : [1683/8999], Training Loss: 2.030567, Validation Loss: 2.030329\n",
      "Epoch : [1684/8999], Training Loss: 2.029559, Validation Loss: 2.028669\n",
      "Epoch : [1685/8999], Training Loss: 2.028429, Validation Loss: 2.027825\n",
      "Epoch : [1686/8999], Training Loss: 2.028499, Validation Loss: 2.028508\n",
      "Epoch : [1687/8999], Training Loss: 2.029151, Validation Loss: 2.028397\n",
      "Epoch : [1688/8999], Training Loss: 2.029451, Validation Loss: 2.028797\n",
      "Epoch : [1689/8999], Training Loss: 2.029094, Validation Loss: 2.028178\n",
      "Epoch : [1690/8999], Training Loss: 2.028652, Validation Loss: 2.028207\n",
      "Epoch : [1691/8999], Training Loss: 2.028270, Validation Loss: 2.028158\n",
      "Epoch : [1692/8999], Training Loss: 2.029244, Validation Loss: 2.029423\n",
      "Epoch : [1693/8999], Training Loss: 2.029896, Validation Loss: 2.029124\n",
      "Epoch : [1694/8999], Training Loss: 2.028736, Validation Loss: 2.028223\n",
      "Epoch : [1695/8999], Training Loss: 2.028748, Validation Loss: 2.028260\n",
      "Epoch : [1696/8999], Training Loss: 2.029691, Validation Loss: 2.028810\n",
      "Epoch : [1697/8999], Training Loss: 2.028874, Validation Loss: 2.028361\n",
      "Epoch : [1698/8999], Training Loss: 2.029408, Validation Loss: 2.029404\n",
      "Epoch : [1699/8999], Training Loss: 2.030036, Validation Loss: 2.040968\n",
      "Epoch : [1700/8999], Training Loss: 2.035496, Validation Loss: 2.037431\n",
      "Epoch : [1701/8999], Training Loss: 2.034010, Validation Loss: 2.044693\n",
      "Epoch : [1702/8999], Training Loss: 2.052245, Validation Loss: 2.064239\n",
      "Epoch : [1703/8999], Training Loss: 2.052058, Validation Loss: 2.038888\n",
      "Epoch : [1704/8999], Training Loss: 2.044734, Validation Loss: 2.042689\n",
      "Epoch : [1705/8999], Training Loss: 2.038609, Validation Loss: 2.041177\n",
      "Epoch : [1706/8999], Training Loss: 2.037622, Validation Loss: 2.048107\n",
      "Epoch : [1707/8999], Training Loss: 2.039910, Validation Loss: 2.033248\n",
      "Epoch : [1708/8999], Training Loss: 2.037060, Validation Loss: 2.034105\n",
      "Epoch : [1709/8999], Training Loss: 2.035583, Validation Loss: 2.032335\n",
      "Epoch : [1710/8999], Training Loss: 2.033168, Validation Loss: 2.038269\n",
      "Epoch : [1711/8999], Training Loss: 2.035633, Validation Loss: 2.037651\n",
      "Epoch : [1712/8999], Training Loss: 2.035540, Validation Loss: 2.038274\n",
      "Epoch : [1713/8999], Training Loss: 2.033005, Validation Loss: 2.030714\n",
      "Epoch : [1714/8999], Training Loss: 2.031323, Validation Loss: 2.029625\n",
      "Epoch : [1715/8999], Training Loss: 2.031873, Validation Loss: 2.035748\n",
      "Epoch : [1716/8999], Training Loss: 2.034609, Validation Loss: 2.033579\n",
      "Epoch : [1717/8999], Training Loss: 2.042424, Validation Loss: 2.051917\n",
      "Epoch : [1718/8999], Training Loss: 2.043747, Validation Loss: 2.033896\n",
      "Epoch : [1719/8999], Training Loss: 2.032435, Validation Loss: 2.031230\n",
      "Epoch : [1720/8999], Training Loss: 2.033552, Validation Loss: 2.036963\n",
      "Epoch : [1721/8999], Training Loss: 2.048873, Validation Loss: 2.044744\n",
      "Epoch : [1722/8999], Training Loss: 2.035265, Validation Loss: 2.029916\n",
      "Epoch : [1723/8999], Training Loss: 2.030085, Validation Loss: 2.029720\n",
      "Epoch : [1724/8999], Training Loss: 2.029537, Validation Loss: 2.029009\n",
      "Epoch : [1725/8999], Training Loss: 2.030627, Validation Loss: 2.028991\n",
      "Epoch : [1726/8999], Training Loss: 2.030094, Validation Loss: 2.030220\n",
      "Epoch : [1727/8999], Training Loss: 2.034478, Validation Loss: 2.031775\n",
      "Epoch : [1728/8999], Training Loss: 2.037870, Validation Loss: 2.037221\n",
      "Epoch : [1729/8999], Training Loss: 2.037650, Validation Loss: 2.031994\n",
      "Epoch : [1730/8999], Training Loss: 2.032675, Validation Loss: 2.036196\n",
      "Epoch : [1731/8999], Training Loss: 2.035750, Validation Loss: 2.033988\n",
      "Epoch : [1732/8999], Training Loss: 2.030857, Validation Loss: 2.033738\n",
      "Epoch : [1733/8999], Training Loss: 2.035661, Validation Loss: 2.038726\n",
      "Epoch : [1734/8999], Training Loss: 2.034426, Validation Loss: 2.032657\n",
      "Epoch : [1735/8999], Training Loss: 2.031045, Validation Loss: 2.029568\n",
      "Epoch : [1736/8999], Training Loss: 2.030293, Validation Loss: 2.030465\n",
      "Epoch : [1737/8999], Training Loss: 2.031804, Validation Loss: 2.032267\n",
      "Epoch : [1738/8999], Training Loss: 2.043383, Validation Loss: 2.038163\n",
      "Epoch : [1739/8999], Training Loss: 2.040568, Validation Loss: 2.032318\n",
      "Epoch : [1740/8999], Training Loss: 2.030406, Validation Loss: 2.029093\n",
      "Epoch : [1741/8999], Training Loss: 2.029519, Validation Loss: 2.029725\n",
      "Epoch : [1742/8999], Training Loss: 2.029016, Validation Loss: 2.030892\n",
      "Epoch : [1743/8999], Training Loss: 2.029361, Validation Loss: 2.028778\n",
      "Epoch : [1744/8999], Training Loss: 2.029437, Validation Loss: 2.028945\n",
      "Epoch : [1745/8999], Training Loss: 2.029136, Validation Loss: 2.029195\n",
      "Epoch : [1746/8999], Training Loss: 2.028969, Validation Loss: 2.028359\n",
      "Epoch : [1747/8999], Training Loss: 2.028365, Validation Loss: 2.028357\n",
      "Epoch : [1748/8999], Training Loss: 2.028305, Validation Loss: 2.027800\n",
      "Epoch : [1749/8999], Training Loss: 2.028279, Validation Loss: 2.028224\n",
      "Epoch : [1750/8999], Training Loss: 2.028946, Validation Loss: 2.028500\n",
      "Epoch : [1751/8999], Training Loss: 2.029000, Validation Loss: 2.029348\n",
      "Epoch : [1752/8999], Training Loss: 2.028973, Validation Loss: 2.029295\n",
      "Epoch : [1753/8999], Training Loss: 2.028959, Validation Loss: 2.027779\n",
      "Epoch : [1754/8999], Training Loss: 2.028032, Validation Loss: 2.027782\n",
      "Epoch : [1755/8999], Training Loss: 2.028663, Validation Loss: 2.028415\n",
      "Epoch : [1756/8999], Training Loss: 2.030301, Validation Loss: 2.028624\n",
      "Epoch : [1757/8999], Training Loss: 2.028336, Validation Loss: 2.028108\n",
      "Epoch : [1758/8999], Training Loss: 2.028716, Validation Loss: 2.028309\n",
      "Epoch : [1759/8999], Training Loss: 2.027989, Validation Loss: 2.028131\n",
      "Epoch : [1760/8999], Training Loss: 2.029714, Validation Loss: 2.027962\n",
      "Epoch : [1761/8999], Training Loss: 2.028167, Validation Loss: 2.027925\n",
      "Epoch : [1762/8999], Training Loss: 2.028259, Validation Loss: 2.027192\n",
      "Epoch : [1763/8999], Training Loss: 2.027491, Validation Loss: 2.027405\n",
      "Epoch : [1764/8999], Training Loss: 2.027366, Validation Loss: 2.027693\n",
      "Epoch : [1765/8999], Training Loss: 2.029259, Validation Loss: 2.028202\n",
      "Epoch : [1766/8999], Training Loss: 2.029474, Validation Loss: 2.028526\n",
      "Epoch : [1767/8999], Training Loss: 2.036084, Validation Loss: 2.031769\n",
      "Epoch : [1768/8999], Training Loss: 2.030964, Validation Loss: 2.031282\n",
      "Epoch : [1769/8999], Training Loss: 2.034561, Validation Loss: 2.045165\n",
      "Epoch : [1770/8999], Training Loss: 2.035298, Validation Loss: 2.029460\n",
      "Epoch : [1771/8999], Training Loss: 2.028835, Validation Loss: 2.028560\n",
      "Epoch : [1772/8999], Training Loss: 2.028611, Validation Loss: 2.027230\n",
      "Epoch : [1773/8999], Training Loss: 2.028562, Validation Loss: 2.028330\n",
      "Epoch : [1774/8999], Training Loss: 2.030809, Validation Loss: 2.032674\n",
      "Epoch : [1775/8999], Training Loss: 2.031320, Validation Loss: 2.038295\n",
      "Epoch : [1776/8999], Training Loss: 2.033201, Validation Loss: 2.038659\n",
      "Epoch : [1777/8999], Training Loss: 2.032032, Validation Loss: 2.029774\n",
      "Epoch : [1778/8999], Training Loss: 2.028456, Validation Loss: 2.036649\n",
      "Epoch : [1779/8999], Training Loss: 2.032043, Validation Loss: 2.031578\n",
      "Epoch : [1780/8999], Training Loss: 2.029403, Validation Loss: 2.028210\n",
      "Epoch : [1781/8999], Training Loss: 2.028511, Validation Loss: 2.027847\n",
      "Epoch : [1782/8999], Training Loss: 2.028259, Validation Loss: 2.028093\n",
      "Epoch : [1783/8999], Training Loss: 2.027919, Validation Loss: 2.028498\n",
      "Epoch : [1784/8999], Training Loss: 2.028353, Validation Loss: 2.027737\n",
      "Epoch : [1785/8999], Training Loss: 2.027992, Validation Loss: 2.027084\n",
      "Epoch : [1786/8999], Training Loss: 2.027984, Validation Loss: 2.027566\n",
      "Epoch : [1787/8999], Training Loss: 2.028333, Validation Loss: 2.027674\n",
      "Epoch : [1788/8999], Training Loss: 2.029220, Validation Loss: 2.033893\n",
      "Epoch : [1789/8999], Training Loss: 2.029470, Validation Loss: 2.027902\n",
      "Epoch : [1790/8999], Training Loss: 2.029605, Validation Loss: 2.027327\n",
      "Epoch : [1791/8999], Training Loss: 2.027691, Validation Loss: 2.027242\n",
      "Epoch : [1792/8999], Training Loss: 2.026862, Validation Loss: 2.026321\n",
      "Epoch : [1793/8999], Training Loss: 2.026485, Validation Loss: 2.026192\n",
      "Epoch : [1794/8999], Training Loss: 2.026388, Validation Loss: 2.026045\n",
      "Epoch : [1795/8999], Training Loss: 2.026223, Validation Loss: 2.025985\n",
      "Epoch : [1796/8999], Training Loss: 2.026358, Validation Loss: 2.026099\n",
      "Epoch : [1797/8999], Training Loss: 2.026195, Validation Loss: 2.026118\n",
      "Epoch : [1798/8999], Training Loss: 2.026369, Validation Loss: 2.026209\n",
      "Epoch : [1799/8999], Training Loss: 2.026171, Validation Loss: 2.026018\n",
      "Epoch : [1800/8999], Training Loss: 2.026167, Validation Loss: 2.026082\n",
      "Epoch : [1801/8999], Training Loss: 2.026670, Validation Loss: 2.026017\n",
      "Epoch : [1802/8999], Training Loss: 2.026161, Validation Loss: 2.025901\n",
      "Epoch : [1803/8999], Training Loss: 2.026091, Validation Loss: 2.025756\n",
      "Epoch : [1804/8999], Training Loss: 2.026043, Validation Loss: 2.025974\n",
      "Epoch : [1805/8999], Training Loss: 2.026280, Validation Loss: 2.025829\n",
      "Epoch : [1806/8999], Training Loss: 2.026912, Validation Loss: 2.026789\n",
      "Epoch : [1807/8999], Training Loss: 2.028058, Validation Loss: 2.036499\n",
      "Epoch : [1808/8999], Training Loss: 2.029746, Validation Loss: 2.028471\n",
      "Epoch : [1809/8999], Training Loss: 2.034606, Validation Loss: 2.054518\n",
      "Epoch : [1810/8999], Training Loss: 2.040041, Validation Loss: 2.047681\n",
      "Epoch : [1811/8999], Training Loss: 2.035570, Validation Loss: 2.032382\n",
      "Epoch : [1812/8999], Training Loss: 2.037316, Validation Loss: 2.034309\n",
      "Epoch : [1813/8999], Training Loss: 2.048486, Validation Loss: 2.046453\n",
      "Epoch : [1814/8999], Training Loss: 2.041716, Validation Loss: 2.066360\n",
      "Epoch : [1815/8999], Training Loss: 2.045152, Validation Loss: 2.032940\n",
      "Epoch : [1816/8999], Training Loss: 2.039732, Validation Loss: 2.038544\n",
      "Epoch : [1817/8999], Training Loss: 2.034850, Validation Loss: 2.029663\n",
      "Epoch : [1818/8999], Training Loss: 2.031221, Validation Loss: 2.029595\n",
      "Epoch : [1819/8999], Training Loss: 2.031545, Validation Loss: 2.032125\n",
      "Epoch : [1820/8999], Training Loss: 2.042522, Validation Loss: 2.070380\n",
      "Epoch : [1821/8999], Training Loss: 2.082430, Validation Loss: 2.059787\n",
      "Epoch : [1822/8999], Training Loss: 2.098216, Validation Loss: 2.081769\n",
      "Epoch : [1823/8999], Training Loss: 2.074882, Validation Loss: 2.058927\n",
      "Epoch : [1824/8999], Training Loss: 2.057424, Validation Loss: 2.049311\n",
      "Epoch : [1825/8999], Training Loss: 2.045154, Validation Loss: 2.035601\n",
      "Epoch : [1826/8999], Training Loss: 2.036078, Validation Loss: 2.031540\n",
      "Epoch : [1827/8999], Training Loss: 2.031973, Validation Loss: 2.030804\n",
      "Epoch : [1828/8999], Training Loss: 2.031830, Validation Loss: 2.033819\n",
      "Epoch : [1829/8999], Training Loss: 2.042044, Validation Loss: 2.037623\n",
      "Epoch : [1830/8999], Training Loss: 2.037665, Validation Loss: 2.041954\n",
      "Epoch : [1831/8999], Training Loss: 2.032884, Validation Loss: 2.031896\n",
      "Epoch : [1832/8999], Training Loss: 2.039451, Validation Loss: 2.032835\n",
      "Epoch : [1833/8999], Training Loss: 2.034278, Validation Loss: 2.029684\n",
      "Epoch : [1834/8999], Training Loss: 2.030633, Validation Loss: 2.028750\n",
      "Epoch : [1835/8999], Training Loss: 2.030626, Validation Loss: 2.034989\n",
      "Epoch : [1836/8999], Training Loss: 2.033985, Validation Loss: 2.032631\n",
      "Epoch : [1837/8999], Training Loss: 2.034221, Validation Loss: 2.030523\n",
      "Epoch : [1838/8999], Training Loss: 2.030161, Validation Loss: 2.032546\n",
      "Epoch : [1839/8999], Training Loss: 2.034785, Validation Loss: 2.033223\n",
      "Epoch : [1840/8999], Training Loss: 2.034305, Validation Loss: 2.030259\n",
      "Epoch : [1841/8999], Training Loss: 2.030507, Validation Loss: 2.029743\n",
      "Epoch : [1842/8999], Training Loss: 2.030417, Validation Loss: 2.028833\n",
      "Epoch : [1843/8999], Training Loss: 2.029103, Validation Loss: 2.027698\n",
      "Epoch : [1844/8999], Training Loss: 2.028499, Validation Loss: 2.028335\n",
      "Epoch : [1845/8999], Training Loss: 2.028330, Validation Loss: 2.027345\n",
      "Epoch : [1846/8999], Training Loss: 2.028689, Validation Loss: 2.027214\n",
      "Epoch : [1847/8999], Training Loss: 2.027623, Validation Loss: 2.027774\n",
      "Epoch : [1848/8999], Training Loss: 2.028306, Validation Loss: 2.027127\n",
      "Epoch : [1849/8999], Training Loss: 2.027854, Validation Loss: 2.027797\n",
      "Epoch : [1850/8999], Training Loss: 2.029182, Validation Loss: 2.028061\n",
      "Epoch : [1851/8999], Training Loss: 2.028061, Validation Loss: 2.027265\n",
      "Epoch : [1852/8999], Training Loss: 2.027539, Validation Loss: 2.027107\n",
      "Epoch : [1853/8999], Training Loss: 2.027128, Validation Loss: 2.027628\n",
      "Epoch : [1854/8999], Training Loss: 2.027678, Validation Loss: 2.026617\n",
      "Epoch : [1855/8999], Training Loss: 2.026721, Validation Loss: 2.026137\n",
      "Epoch : [1856/8999], Training Loss: 2.026433, Validation Loss: 2.027003\n",
      "Epoch : [1857/8999], Training Loss: 2.027895, Validation Loss: 2.027172\n",
      "Epoch : [1858/8999], Training Loss: 2.026922, Validation Loss: 2.026111\n",
      "Epoch : [1859/8999], Training Loss: 2.026397, Validation Loss: 2.026460\n",
      "Epoch : [1860/8999], Training Loss: 2.027170, Validation Loss: 2.028172\n",
      "Epoch : [1861/8999], Training Loss: 2.028002, Validation Loss: 2.031776\n",
      "Epoch : [1862/8999], Training Loss: 2.029029, Validation Loss: 2.035265\n",
      "Epoch : [1863/8999], Training Loss: 2.030225, Validation Loss: 2.033385\n",
      "Epoch : [1864/8999], Training Loss: 2.033539, Validation Loss: 2.030706\n",
      "Epoch : [1865/8999], Training Loss: 2.036044, Validation Loss: 2.028868\n",
      "Epoch : [1866/8999], Training Loss: 2.033272, Validation Loss: 2.030539\n",
      "Epoch : [1867/8999], Training Loss: 2.034302, Validation Loss: 2.034244\n",
      "Epoch : [1868/8999], Training Loss: 2.033897, Validation Loss: 2.031538\n",
      "Epoch : [1869/8999], Training Loss: 2.031895, Validation Loss: 2.030620\n",
      "Epoch : [1870/8999], Training Loss: 2.030927, Validation Loss: 2.028150\n",
      "Epoch : [1871/8999], Training Loss: 2.037372, Validation Loss: 2.035522\n",
      "Epoch : [1872/8999], Training Loss: 2.038553, Validation Loss: 2.031280\n",
      "Epoch : [1873/8999], Training Loss: 2.030665, Validation Loss: 2.028546\n",
      "Epoch : [1874/8999], Training Loss: 2.030198, Validation Loss: 2.036945\n",
      "Epoch : [1875/8999], Training Loss: 2.032448, Validation Loss: 2.032408\n",
      "Epoch : [1876/8999], Training Loss: 2.029993, Validation Loss: 2.027567\n",
      "Epoch : [1877/8999], Training Loss: 2.029057, Validation Loss: 2.032108\n",
      "Epoch : [1878/8999], Training Loss: 2.029144, Validation Loss: 2.028924\n",
      "Epoch : [1879/8999], Training Loss: 2.028645, Validation Loss: 2.028422\n",
      "Epoch : [1880/8999], Training Loss: 2.029259, Validation Loss: 2.029911\n",
      "Epoch : [1881/8999], Training Loss: 2.029170, Validation Loss: 2.025815\n",
      "Epoch : [1882/8999], Training Loss: 2.026782, Validation Loss: 2.026929\n",
      "Epoch : [1883/8999], Training Loss: 2.027118, Validation Loss: 2.026069\n",
      "Epoch : [1884/8999], Training Loss: 2.026885, Validation Loss: 2.026818\n",
      "Epoch : [1885/8999], Training Loss: 2.027087, Validation Loss: 2.026159\n",
      "Epoch : [1886/8999], Training Loss: 2.028151, Validation Loss: 2.034014\n",
      "Epoch : [1887/8999], Training Loss: 2.036042, Validation Loss: 2.045957\n",
      "Epoch : [1888/8999], Training Loss: 2.038589, Validation Loss: 2.029895\n",
      "Epoch : [1889/8999], Training Loss: 2.032158, Validation Loss: 2.035281\n",
      "Epoch : [1890/8999], Training Loss: 2.029865, Validation Loss: 2.029305\n",
      "Epoch : [1891/8999], Training Loss: 2.029223, Validation Loss: 2.028634\n",
      "Epoch : [1892/8999], Training Loss: 2.028648, Validation Loss: 2.026779\n",
      "Epoch : [1893/8999], Training Loss: 2.027093, Validation Loss: 2.027219\n",
      "Epoch : [1894/8999], Training Loss: 2.026262, Validation Loss: 2.025873\n",
      "Epoch : [1895/8999], Training Loss: 2.026668, Validation Loss: 2.027151\n",
      "Epoch : [1896/8999], Training Loss: 2.027913, Validation Loss: 2.027479\n",
      "Epoch : [1897/8999], Training Loss: 2.028897, Validation Loss: 2.026699\n",
      "Epoch : [1898/8999], Training Loss: 2.027372, Validation Loss: 2.027055\n",
      "Epoch : [1899/8999], Training Loss: 2.026672, Validation Loss: 2.025598\n",
      "Epoch : [1900/8999], Training Loss: 2.025722, Validation Loss: 2.025349\n",
      "Epoch : [1901/8999], Training Loss: 2.025766, Validation Loss: 2.025795\n",
      "Epoch : [1902/8999], Training Loss: 2.026325, Validation Loss: 2.026288\n",
      "Epoch : [1903/8999], Training Loss: 2.026940, Validation Loss: 2.030058\n",
      "Epoch : [1904/8999], Training Loss: 2.027377, Validation Loss: 2.026958\n",
      "Epoch : [1905/8999], Training Loss: 2.029301, Validation Loss: 2.028552\n",
      "Epoch : [1906/8999], Training Loss: 2.028347, Validation Loss: 2.026276\n",
      "Epoch : [1907/8999], Training Loss: 2.026229, Validation Loss: 2.026063\n",
      "Epoch : [1908/8999], Training Loss: 2.025592, Validation Loss: 2.026010\n",
      "Epoch : [1909/8999], Training Loss: 2.026521, Validation Loss: 2.025477\n",
      "Epoch : [1910/8999], Training Loss: 2.025169, Validation Loss: 2.025735\n",
      "Epoch : [1911/8999], Training Loss: 2.026692, Validation Loss: 2.026807\n",
      "Epoch : [1912/8999], Training Loss: 2.026569, Validation Loss: 2.026232\n",
      "Epoch : [1913/8999], Training Loss: 2.025876, Validation Loss: 2.025850\n",
      "Epoch : [1914/8999], Training Loss: 2.026252, Validation Loss: 2.027015\n",
      "Epoch : [1915/8999], Training Loss: 2.028417, Validation Loss: 2.030069\n",
      "Epoch : [1916/8999], Training Loss: 2.035216, Validation Loss: 2.031150\n",
      "Epoch : [1917/8999], Training Loss: 2.031425, Validation Loss: 2.029204\n",
      "Epoch : [1918/8999], Training Loss: 2.027923, Validation Loss: 2.027844\n",
      "Epoch : [1919/8999], Training Loss: 2.029024, Validation Loss: 2.025919\n",
      "Epoch : [1920/8999], Training Loss: 2.026038, Validation Loss: 2.025638\n",
      "Epoch : [1921/8999], Training Loss: 2.025702, Validation Loss: 2.025223\n",
      "Epoch : [1922/8999], Training Loss: 2.026432, Validation Loss: 2.033939\n",
      "Epoch : [1923/8999], Training Loss: 2.028167, Validation Loss: 2.026968\n",
      "Epoch : [1924/8999], Training Loss: 2.026978, Validation Loss: 2.032079\n",
      "Epoch : [1925/8999], Training Loss: 2.028880, Validation Loss: 2.025660\n",
      "Epoch : [1926/8999], Training Loss: 2.026122, Validation Loss: 2.026026\n",
      "Epoch : [1927/8999], Training Loss: 2.028920, Validation Loss: 2.025167\n",
      "Epoch : [1928/8999], Training Loss: 2.033775, Validation Loss: 2.044778\n",
      "Epoch : [1929/8999], Training Loss: 2.036978, Validation Loss: 2.031471\n",
      "Epoch : [1930/8999], Training Loss: 2.032533, Validation Loss: 2.031700\n",
      "Epoch : [1931/8999], Training Loss: 2.035510, Validation Loss: 2.032167\n",
      "Epoch : [1932/8999], Training Loss: 2.031197, Validation Loss: 2.028703\n",
      "Epoch : [1933/8999], Training Loss: 2.031650, Validation Loss: 2.029142\n",
      "Epoch : [1934/8999], Training Loss: 2.034089, Validation Loss: 2.031330\n",
      "Epoch : [1935/8999], Training Loss: 2.042429, Validation Loss: 2.060605\n",
      "Epoch : [1936/8999], Training Loss: 2.041887, Validation Loss: 2.032578\n",
      "Epoch : [1937/8999], Training Loss: 2.032225, Validation Loss: 2.031502\n",
      "Epoch : [1938/8999], Training Loss: 2.032576, Validation Loss: 2.031895\n",
      "Epoch : [1939/8999], Training Loss: 2.034021, Validation Loss: 2.033149\n",
      "Epoch : [1940/8999], Training Loss: 2.033958, Validation Loss: 2.029296\n",
      "Epoch : [1941/8999], Training Loss: 2.028338, Validation Loss: 2.026288\n",
      "Epoch : [1942/8999], Training Loss: 2.026502, Validation Loss: 2.026428\n",
      "Epoch : [1943/8999], Training Loss: 2.027369, Validation Loss: 2.031662\n",
      "Epoch : [1944/8999], Training Loss: 2.027930, Validation Loss: 2.025779\n",
      "Epoch : [1945/8999], Training Loss: 2.026569, Validation Loss: 2.026635\n",
      "Epoch : [1946/8999], Training Loss: 2.026067, Validation Loss: 2.025919\n",
      "Epoch : [1947/8999], Training Loss: 2.026815, Validation Loss: 2.027329\n",
      "Epoch : [1948/8999], Training Loss: 2.026985, Validation Loss: 2.025437\n",
      "Epoch : [1949/8999], Training Loss: 2.025348, Validation Loss: 2.024986\n",
      "Epoch : [1950/8999], Training Loss: 2.025002, Validation Loss: 2.024733\n",
      "Epoch : [1951/8999], Training Loss: 2.024948, Validation Loss: 2.024813\n",
      "Epoch : [1952/8999], Training Loss: 2.024778, Validation Loss: 2.024605\n",
      "Epoch : [1953/8999], Training Loss: 2.024924, Validation Loss: 2.024774\n",
      "Epoch : [1954/8999], Training Loss: 2.025021, Validation Loss: 2.025101\n",
      "Epoch : [1955/8999], Training Loss: 2.024956, Validation Loss: 2.024758\n",
      "Epoch : [1956/8999], Training Loss: 2.024689, Validation Loss: 2.024484\n",
      "Epoch : [1957/8999], Training Loss: 2.024796, Validation Loss: 2.024590\n",
      "Epoch : [1958/8999], Training Loss: 2.024527, Validation Loss: 2.024239\n",
      "Epoch : [1959/8999], Training Loss: 2.024356, Validation Loss: 2.024193\n",
      "Epoch : [1960/8999], Training Loss: 2.024339, Validation Loss: 2.024354\n",
      "Epoch : [1961/8999], Training Loss: 2.024428, Validation Loss: 2.024396\n",
      "Epoch : [1962/8999], Training Loss: 2.024757, Validation Loss: 2.024854\n",
      "Epoch : [1963/8999], Training Loss: 2.024921, Validation Loss: 2.024849\n",
      "Epoch : [1964/8999], Training Loss: 2.024813, Validation Loss: 2.024383\n",
      "Epoch : [1965/8999], Training Loss: 2.024484, Validation Loss: 2.024164\n",
      "Epoch : [1966/8999], Training Loss: 2.024202, Validation Loss: 2.024142\n",
      "Epoch : [1967/8999], Training Loss: 2.024564, Validation Loss: 2.024402\n",
      "Epoch : [1968/8999], Training Loss: 2.024493, Validation Loss: 2.024114\n",
      "Epoch : [1969/8999], Training Loss: 2.024200, Validation Loss: 2.024187\n",
      "Epoch : [1970/8999], Training Loss: 2.024157, Validation Loss: 2.023949\n",
      "Epoch : [1971/8999], Training Loss: 2.024137, Validation Loss: 2.023898\n",
      "Epoch : [1972/8999], Training Loss: 2.024120, Validation Loss: 2.023940\n",
      "Epoch : [1973/8999], Training Loss: 2.024098, Validation Loss: 2.024002\n",
      "Epoch : [1974/8999], Training Loss: 2.024159, Validation Loss: 2.023781\n",
      "Epoch : [1975/8999], Training Loss: 2.024104, Validation Loss: 2.024333\n",
      "Epoch : [1976/8999], Training Loss: 2.024480, Validation Loss: 2.024087\n",
      "Epoch : [1977/8999], Training Loss: 2.024573, Validation Loss: 2.023773\n",
      "Epoch : [1978/8999], Training Loss: 2.024176, Validation Loss: 2.023866\n",
      "Epoch : [1979/8999], Training Loss: 2.024702, Validation Loss: 2.024127\n",
      "Epoch : [1980/8999], Training Loss: 2.024076, Validation Loss: 2.023999\n",
      "Epoch : [1981/8999], Training Loss: 2.024325, Validation Loss: 2.024299\n",
      "Epoch : [1982/8999], Training Loss: 2.024563, Validation Loss: 2.023610\n",
      "Epoch : [1983/8999], Training Loss: 2.024096, Validation Loss: 2.023456\n",
      "Epoch : [1984/8999], Training Loss: 2.024070, Validation Loss: 2.023527\n",
      "Epoch : [1985/8999], Training Loss: 2.023962, Validation Loss: 2.023514\n",
      "Epoch : [1986/8999], Training Loss: 2.023820, Validation Loss: 2.024596\n",
      "Epoch : [1987/8999], Training Loss: 2.024596, Validation Loss: 2.023931\n",
      "Epoch : [1988/8999], Training Loss: 2.024296, Validation Loss: 2.023330\n",
      "Epoch : [1989/8999], Training Loss: 2.024088, Validation Loss: 2.024137\n",
      "Epoch : [1990/8999], Training Loss: 2.024104, Validation Loss: 2.028198\n",
      "Epoch : [1991/8999], Training Loss: 2.026804, Validation Loss: 2.023527\n",
      "Epoch : [1992/8999], Training Loss: 2.024456, Validation Loss: 2.026035\n",
      "Epoch : [1993/8999], Training Loss: 2.028226, Validation Loss: 2.031366\n",
      "Epoch : [1994/8999], Training Loss: 2.030395, Validation Loss: 2.028943\n",
      "Epoch : [1995/8999], Training Loss: 2.030401, Validation Loss: 2.028668\n",
      "Epoch : [1996/8999], Training Loss: 2.034058, Validation Loss: 2.031361\n",
      "Epoch : [1997/8999], Training Loss: 2.031266, Validation Loss: 2.034436\n",
      "Epoch : [1998/8999], Training Loss: 2.063818, Validation Loss: 2.051792\n",
      "Epoch : [1999/8999], Training Loss: 2.050091, Validation Loss: 2.035200\n",
      "Epoch : [2000/8999], Training Loss: 2.035127, Validation Loss: 2.030589\n",
      "Epoch : [2001/8999], Training Loss: 2.030331, Validation Loss: 2.029006\n",
      "Epoch : [2002/8999], Training Loss: 2.030387, Validation Loss: 2.030468\n",
      "Epoch : [2003/8999], Training Loss: 2.031880, Validation Loss: 2.042993\n",
      "Epoch : [2004/8999], Training Loss: 2.034960, Validation Loss: 2.029913\n",
      "Epoch : [2005/8999], Training Loss: 2.030739, Validation Loss: 2.026387\n",
      "Epoch : [2006/8999], Training Loss: 2.026283, Validation Loss: 2.024713\n",
      "Epoch : [2007/8999], Training Loss: 2.024726, Validation Loss: 2.028649\n",
      "Epoch : [2008/8999], Training Loss: 2.026249, Validation Loss: 2.024287\n",
      "Epoch : [2009/8999], Training Loss: 2.024528, Validation Loss: 2.025286\n",
      "Epoch : [2010/8999], Training Loss: 2.025321, Validation Loss: 2.024347\n",
      "Epoch : [2011/8999], Training Loss: 2.024264, Validation Loss: 2.023544\n",
      "Epoch : [2012/8999], Training Loss: 2.023608, Validation Loss: 2.023326\n",
      "Epoch : [2013/8999], Training Loss: 2.023519, Validation Loss: 2.023257\n",
      "Epoch : [2014/8999], Training Loss: 2.023430, Validation Loss: 2.023245\n",
      "Epoch : [2015/8999], Training Loss: 2.023314, Validation Loss: 2.023157\n",
      "Epoch : [2016/8999], Training Loss: 2.023242, Validation Loss: 2.023094\n",
      "Epoch : [2017/8999], Training Loss: 2.023166, Validation Loss: 2.023061\n",
      "Epoch : [2018/8999], Training Loss: 2.023135, Validation Loss: 2.023059\n",
      "Epoch : [2019/8999], Training Loss: 2.023134, Validation Loss: 2.023516\n",
      "Epoch : [2020/8999], Training Loss: 2.023280, Validation Loss: 2.023510\n",
      "Epoch : [2021/8999], Training Loss: 2.023254, Validation Loss: 2.023639\n",
      "Epoch : [2022/8999], Training Loss: 2.023315, Validation Loss: 2.023264\n",
      "Epoch : [2023/8999], Training Loss: 2.023160, Validation Loss: 2.023568\n",
      "Epoch : [2024/8999], Training Loss: 2.023368, Validation Loss: 2.024032\n",
      "Epoch : [2025/8999], Training Loss: 2.023480, Validation Loss: 2.023580\n",
      "Epoch : [2026/8999], Training Loss: 2.023394, Validation Loss: 2.023083\n",
      "Epoch : [2027/8999], Training Loss: 2.023117, Validation Loss: 2.022989\n",
      "Epoch : [2028/8999], Training Loss: 2.023124, Validation Loss: 2.023122\n",
      "Epoch : [2029/8999], Training Loss: 2.023144, Validation Loss: 2.023136\n",
      "Epoch : [2030/8999], Training Loss: 2.023067, Validation Loss: 2.023059\n",
      "Epoch : [2031/8999], Training Loss: 2.022956, Validation Loss: 2.023025\n",
      "Epoch : [2032/8999], Training Loss: 2.022956, Validation Loss: 2.022982\n",
      "Epoch : [2033/8999], Training Loss: 2.022937, Validation Loss: 2.023208\n",
      "Epoch : [2034/8999], Training Loss: 2.022992, Validation Loss: 2.023160\n",
      "Epoch : [2035/8999], Training Loss: 2.022978, Validation Loss: 2.022942\n",
      "Epoch : [2036/8999], Training Loss: 2.022932, Validation Loss: 2.023022\n",
      "Epoch : [2037/8999], Training Loss: 2.023130, Validation Loss: 2.022955\n",
      "Epoch : [2038/8999], Training Loss: 2.023088, Validation Loss: 2.022736\n",
      "Epoch : [2039/8999], Training Loss: 2.022892, Validation Loss: 2.023014\n",
      "Epoch : [2040/8999], Training Loss: 2.022964, Validation Loss: 2.023304\n",
      "Epoch : [2041/8999], Training Loss: 2.023257, Validation Loss: 2.023096\n",
      "Epoch : [2042/8999], Training Loss: 2.022885, Validation Loss: 2.023151\n",
      "Epoch : [2043/8999], Training Loss: 2.022956, Validation Loss: 2.023296\n",
      "Epoch : [2044/8999], Training Loss: 2.023149, Validation Loss: 2.023234\n",
      "Epoch : [2045/8999], Training Loss: 2.023051, Validation Loss: 2.023320\n",
      "Epoch : [2046/8999], Training Loss: 2.023205, Validation Loss: 2.023774\n",
      "Epoch : [2047/8999], Training Loss: 2.023530, Validation Loss: 2.023359\n",
      "Epoch : [2048/8999], Training Loss: 2.023335, Validation Loss: 2.022829\n",
      "Epoch : [2049/8999], Training Loss: 2.022857, Validation Loss: 2.022875\n",
      "Epoch : [2050/8999], Training Loss: 2.022786, Validation Loss: 2.022843\n",
      "Epoch : [2051/8999], Training Loss: 2.022741, Validation Loss: 2.022784\n",
      "Epoch : [2052/8999], Training Loss: 2.022768, Validation Loss: 2.022752\n",
      "Epoch : [2053/8999], Training Loss: 2.022792, Validation Loss: 2.023044\n",
      "Epoch : [2054/8999], Training Loss: 2.022787, Validation Loss: 2.022879\n",
      "Epoch : [2055/8999], Training Loss: 2.022804, Validation Loss: 2.022729\n",
      "Epoch : [2056/8999], Training Loss: 2.022669, Validation Loss: 2.022580\n",
      "Epoch : [2057/8999], Training Loss: 2.022601, Validation Loss: 2.022593\n",
      "Epoch : [2058/8999], Training Loss: 2.022662, Validation Loss: 2.022794\n",
      "Epoch : [2059/8999], Training Loss: 2.022702, Validation Loss: 2.022614\n",
      "Epoch : [2060/8999], Training Loss: 2.022706, Validation Loss: 2.022826\n",
      "Epoch : [2061/8999], Training Loss: 2.022735, Validation Loss: 2.022466\n",
      "Epoch : [2062/8999], Training Loss: 2.022550, Validation Loss: 2.022538\n",
      "Epoch : [2063/8999], Training Loss: 2.022532, Validation Loss: 2.022512\n",
      "Epoch : [2064/8999], Training Loss: 2.022620, Validation Loss: 2.022916\n",
      "Epoch : [2065/8999], Training Loss: 2.022626, Validation Loss: 2.022744\n",
      "Epoch : [2066/8999], Training Loss: 2.022887, Validation Loss: 2.022938\n",
      "Epoch : [2067/8999], Training Loss: 2.022778, Validation Loss: 2.023036\n",
      "Epoch : [2068/8999], Training Loss: 2.022779, Validation Loss: 2.022805\n",
      "Epoch : [2069/8999], Training Loss: 2.022721, Validation Loss: 2.022565\n",
      "Epoch : [2070/8999], Training Loss: 2.022647, Validation Loss: 2.022583\n",
      "Epoch : [2071/8999], Training Loss: 2.022589, Validation Loss: 2.022477\n",
      "Epoch : [2072/8999], Training Loss: 2.022533, Validation Loss: 2.022634\n",
      "Epoch : [2073/8999], Training Loss: 2.022596, Validation Loss: 2.022530\n",
      "Epoch : [2074/8999], Training Loss: 2.022584, Validation Loss: 2.022623\n",
      "Epoch : [2075/8999], Training Loss: 2.022604, Validation Loss: 2.022737\n",
      "Epoch : [2076/8999], Training Loss: 2.022537, Validation Loss: 2.022651\n",
      "Epoch : [2077/8999], Training Loss: 2.022599, Validation Loss: 2.022730\n",
      "Epoch : [2078/8999], Training Loss: 2.022570, Validation Loss: 2.022918\n",
      "Epoch : [2079/8999], Training Loss: 2.022568, Validation Loss: 2.022907\n",
      "Epoch : [2080/8999], Training Loss: 2.022662, Validation Loss: 2.022697\n",
      "Epoch : [2081/8999], Training Loss: 2.022597, Validation Loss: 2.022861\n",
      "Epoch : [2082/8999], Training Loss: 2.022652, Validation Loss: 2.022732\n",
      "Epoch : [2083/8999], Training Loss: 2.022637, Validation Loss: 2.022704\n",
      "Epoch : [2084/8999], Training Loss: 2.022630, Validation Loss: 2.022539\n",
      "Epoch : [2085/8999], Training Loss: 2.022573, Validation Loss: 2.022755\n",
      "Epoch : [2086/8999], Training Loss: 2.022643, Validation Loss: 2.022529\n",
      "Epoch : [2087/8999], Training Loss: 2.022720, Validation Loss: 2.022419\n",
      "Epoch : [2088/8999], Training Loss: 2.022633, Validation Loss: 2.022801\n",
      "Epoch : [2089/8999], Training Loss: 2.022749, Validation Loss: 2.022532\n",
      "Epoch : [2090/8999], Training Loss: 2.022705, Validation Loss: 2.022753\n",
      "Epoch : [2091/8999], Training Loss: 2.022617, Validation Loss: 2.022622\n",
      "Epoch : [2092/8999], Training Loss: 2.022601, Validation Loss: 2.022744\n",
      "Epoch : [2093/8999], Training Loss: 2.022559, Validation Loss: 2.022898\n",
      "Epoch : [2094/8999], Training Loss: 2.022502, Validation Loss: 2.022723\n",
      "Epoch : [2095/8999], Training Loss: 2.022567, Validation Loss: 2.022492\n",
      "Epoch : [2096/8999], Training Loss: 2.022556, Validation Loss: 2.022603\n",
      "Epoch : [2097/8999], Training Loss: 2.022546, Validation Loss: 2.022447\n",
      "Epoch : [2098/8999], Training Loss: 2.022494, Validation Loss: 2.022452\n",
      "Epoch : [2099/8999], Training Loss: 2.022506, Validation Loss: 2.022685\n",
      "Epoch : [2100/8999], Training Loss: 2.022760, Validation Loss: 2.022498\n",
      "Epoch : [2101/8999], Training Loss: 2.022615, Validation Loss: 2.022700\n",
      "Epoch : [2102/8999], Training Loss: 2.022531, Validation Loss: 2.022699\n",
      "Epoch : [2103/8999], Training Loss: 2.022531, Validation Loss: 2.022537\n",
      "Epoch : [2104/8999], Training Loss: 2.022414, Validation Loss: 2.022626\n",
      "Epoch : [2105/8999], Training Loss: 2.022413, Validation Loss: 2.022539\n",
      "Epoch : [2106/8999], Training Loss: 2.022418, Validation Loss: 2.022481\n",
      "Epoch : [2107/8999], Training Loss: 2.022404, Validation Loss: 2.022336\n",
      "Epoch : [2108/8999], Training Loss: 2.022332, Validation Loss: 2.022360\n",
      "Epoch : [2109/8999], Training Loss: 2.022484, Validation Loss: 2.022343\n",
      "Epoch : [2110/8999], Training Loss: 2.022347, Validation Loss: 2.022345\n",
      "Epoch : [2111/8999], Training Loss: 2.022532, Validation Loss: 2.022300\n",
      "Epoch : [2112/8999], Training Loss: 2.022424, Validation Loss: 2.022414\n",
      "Epoch : [2113/8999], Training Loss: 2.022458, Validation Loss: 2.022181\n",
      "Epoch : [2114/8999], Training Loss: 2.022362, Validation Loss: 2.022409\n",
      "Epoch : [2115/8999], Training Loss: 2.022401, Validation Loss: 2.022301\n",
      "Epoch : [2116/8999], Training Loss: 2.022366, Validation Loss: 2.022489\n",
      "Epoch : [2117/8999], Training Loss: 2.022342, Validation Loss: 2.022341\n",
      "Epoch : [2118/8999], Training Loss: 2.022289, Validation Loss: 2.022387\n",
      "Epoch : [2119/8999], Training Loss: 2.022308, Validation Loss: 2.022312\n",
      "Epoch : [2120/8999], Training Loss: 2.022357, Validation Loss: 2.022368\n",
      "Epoch : [2121/8999], Training Loss: 2.022322, Validation Loss: 2.022458\n",
      "Epoch : [2122/8999], Training Loss: 2.022344, Validation Loss: 2.022301\n",
      "Epoch : [2123/8999], Training Loss: 2.022306, Validation Loss: 2.022253\n",
      "Epoch : [2124/8999], Training Loss: 2.022323, Validation Loss: 2.022233\n",
      "Epoch : [2125/8999], Training Loss: 2.022331, Validation Loss: 2.022223\n",
      "Epoch : [2126/8999], Training Loss: 2.022290, Validation Loss: 2.022309\n",
      "Epoch : [2127/8999], Training Loss: 2.022506, Validation Loss: 2.022461\n",
      "Epoch : [2128/8999], Training Loss: 2.022376, Validation Loss: 2.022589\n",
      "Epoch : [2129/8999], Training Loss: 2.022409, Validation Loss: 2.022487\n",
      "Epoch : [2130/8999], Training Loss: 2.022379, Validation Loss: 2.022442\n",
      "Epoch : [2131/8999], Training Loss: 2.022297, Validation Loss: 2.022470\n",
      "Epoch : [2132/8999], Training Loss: 2.022267, Validation Loss: 2.022443\n",
      "Epoch : [2133/8999], Training Loss: 2.022403, Validation Loss: 2.022338\n",
      "Epoch : [2134/8999], Training Loss: 2.022407, Validation Loss: 2.022422\n",
      "Epoch : [2135/8999], Training Loss: 2.022275, Validation Loss: 2.022247\n",
      "Epoch : [2136/8999], Training Loss: 2.022258, Validation Loss: 2.022345\n",
      "Epoch : [2137/8999], Training Loss: 2.022280, Validation Loss: 2.022305\n",
      "Epoch : [2138/8999], Training Loss: 2.022189, Validation Loss: 2.022173\n",
      "Epoch : [2139/8999], Training Loss: 2.022301, Validation Loss: 2.022155\n",
      "Epoch : [2140/8999], Training Loss: 2.022187, Validation Loss: 2.022198\n",
      "Epoch : [2141/8999], Training Loss: 2.022164, Validation Loss: 2.022070\n",
      "Epoch : [2142/8999], Training Loss: 2.022159, Validation Loss: 2.022039\n",
      "Epoch : [2143/8999], Training Loss: 2.022121, Validation Loss: 2.022092\n",
      "Epoch : [2144/8999], Training Loss: 2.022144, Validation Loss: 2.022095\n",
      "Epoch : [2145/8999], Training Loss: 2.022239, Validation Loss: 2.022142\n",
      "Epoch : [2146/8999], Training Loss: 2.022117, Validation Loss: 2.022170\n",
      "Epoch : [2147/8999], Training Loss: 2.022142, Validation Loss: 2.022067\n",
      "Epoch : [2148/8999], Training Loss: 2.022189, Validation Loss: 2.022256\n",
      "Epoch : [2149/8999], Training Loss: 2.022114, Validation Loss: 2.022311\n",
      "Epoch : [2150/8999], Training Loss: 2.022277, Validation Loss: 2.022146\n",
      "Epoch : [2151/8999], Training Loss: 2.022248, Validation Loss: 2.022431\n",
      "Epoch : [2152/8999], Training Loss: 2.022429, Validation Loss: 2.022264\n",
      "Epoch : [2153/8999], Training Loss: 2.022375, Validation Loss: 2.022219\n",
      "Epoch : [2154/8999], Training Loss: 2.022296, Validation Loss: 2.022174\n",
      "Epoch : [2155/8999], Training Loss: 2.022157, Validation Loss: 2.022023\n",
      "Epoch : [2156/8999], Training Loss: 2.022188, Validation Loss: 2.022198\n",
      "Epoch : [2157/8999], Training Loss: 2.022203, Validation Loss: 2.021933\n",
      "Epoch : [2158/8999], Training Loss: 2.022034, Validation Loss: 2.021919\n",
      "Epoch : [2159/8999], Training Loss: 2.022042, Validation Loss: 2.022043\n",
      "Epoch : [2160/8999], Training Loss: 2.022220, Validation Loss: 2.022066\n",
      "Epoch : [2161/8999], Training Loss: 2.022170, Validation Loss: 2.022052\n",
      "Epoch : [2162/8999], Training Loss: 2.022106, Validation Loss: 2.022041\n",
      "Epoch : [2163/8999], Training Loss: 2.022101, Validation Loss: 2.021989\n",
      "Epoch : [2164/8999], Training Loss: 2.022091, Validation Loss: 2.021986\n",
      "Epoch : [2165/8999], Training Loss: 2.022021, Validation Loss: 2.022078\n",
      "Epoch : [2166/8999], Training Loss: 2.021957, Validation Loss: 2.021960\n",
      "Epoch : [2167/8999], Training Loss: 2.022000, Validation Loss: 2.022008\n",
      "Epoch : [2168/8999], Training Loss: 2.022038, Validation Loss: 2.022399\n",
      "Epoch : [2169/8999], Training Loss: 2.022221, Validation Loss: 2.022180\n",
      "Epoch : [2170/8999], Training Loss: 2.022038, Validation Loss: 2.022197\n",
      "Epoch : [2171/8999], Training Loss: 2.021999, Validation Loss: 2.022088\n",
      "Epoch : [2172/8999], Training Loss: 2.021967, Validation Loss: 2.022154\n",
      "Epoch : [2173/8999], Training Loss: 2.021987, Validation Loss: 2.021958\n",
      "Epoch : [2174/8999], Training Loss: 2.021999, Validation Loss: 2.022269\n",
      "Epoch : [2175/8999], Training Loss: 2.021979, Validation Loss: 2.022026\n",
      "Epoch : [2176/8999], Training Loss: 2.022086, Validation Loss: 2.022069\n",
      "Epoch : [2177/8999], Training Loss: 2.021958, Validation Loss: 2.022067\n",
      "Epoch : [2178/8999], Training Loss: 2.022068, Validation Loss: 2.021960\n",
      "Epoch : [2179/8999], Training Loss: 2.022027, Validation Loss: 2.022079\n",
      "Epoch : [2180/8999], Training Loss: 2.021909, Validation Loss: 2.021936\n",
      "Epoch : [2181/8999], Training Loss: 2.021954, Validation Loss: 2.021746\n",
      "Epoch : [2182/8999], Training Loss: 2.021838, Validation Loss: 2.021827\n",
      "Epoch : [2183/8999], Training Loss: 2.021819, Validation Loss: 2.021748\n",
      "Epoch : [2184/8999], Training Loss: 2.021854, Validation Loss: 2.021742\n",
      "Epoch : [2185/8999], Training Loss: 2.021892, Validation Loss: 2.021936\n",
      "Epoch : [2186/8999], Training Loss: 2.021869, Validation Loss: 2.021774\n",
      "Epoch : [2187/8999], Training Loss: 2.021873, Validation Loss: 2.021730\n",
      "Epoch : [2188/8999], Training Loss: 2.021865, Validation Loss: 2.021871\n",
      "Epoch : [2189/8999], Training Loss: 2.021959, Validation Loss: 2.021798\n",
      "Epoch : [2190/8999], Training Loss: 2.021973, Validation Loss: 2.021983\n",
      "Epoch : [2191/8999], Training Loss: 2.021892, Validation Loss: 2.021695\n",
      "Epoch : [2192/8999], Training Loss: 2.021937, Validation Loss: 2.021738\n",
      "Epoch : [2193/8999], Training Loss: 2.021751, Validation Loss: 2.021685\n",
      "Epoch : [2194/8999], Training Loss: 2.021815, Validation Loss: 2.021779\n",
      "Epoch : [2195/8999], Training Loss: 2.021765, Validation Loss: 2.021767\n",
      "Epoch : [2196/8999], Training Loss: 2.021871, Validation Loss: 2.022207\n",
      "Epoch : [2197/8999], Training Loss: 2.021892, Validation Loss: 2.021804\n",
      "Epoch : [2198/8999], Training Loss: 2.021866, Validation Loss: 2.021897\n",
      "Epoch : [2199/8999], Training Loss: 2.021770, Validation Loss: 2.021733\n",
      "Epoch : [2200/8999], Training Loss: 2.021773, Validation Loss: 2.021792\n",
      "Epoch : [2201/8999], Training Loss: 2.021763, Validation Loss: 2.021829\n",
      "Epoch : [2202/8999], Training Loss: 2.021755, Validation Loss: 2.021863\n",
      "Epoch : [2203/8999], Training Loss: 2.021764, Validation Loss: 2.021759\n",
      "Epoch : [2204/8999], Training Loss: 2.021791, Validation Loss: 2.021888\n",
      "Epoch : [2205/8999], Training Loss: 2.021794, Validation Loss: 2.021607\n",
      "Epoch : [2206/8999], Training Loss: 2.021693, Validation Loss: 2.021844\n",
      "Epoch : [2207/8999], Training Loss: 2.021693, Validation Loss: 2.021625\n",
      "Epoch : [2208/8999], Training Loss: 2.021715, Validation Loss: 2.021721\n",
      "Epoch : [2209/8999], Training Loss: 2.021702, Validation Loss: 2.021759\n",
      "Epoch : [2210/8999], Training Loss: 2.021716, Validation Loss: 2.021906\n",
      "Epoch : [2211/8999], Training Loss: 2.021684, Validation Loss: 2.021583\n",
      "Epoch : [2212/8999], Training Loss: 2.021673, Validation Loss: 2.021667\n",
      "Epoch : [2213/8999], Training Loss: 2.021691, Validation Loss: 2.021641\n",
      "Epoch : [2214/8999], Training Loss: 2.021742, Validation Loss: 2.021942\n",
      "Epoch : [2215/8999], Training Loss: 2.021792, Validation Loss: 2.021695\n",
      "Epoch : [2216/8999], Training Loss: 2.021795, Validation Loss: 2.021708\n",
      "Epoch : [2217/8999], Training Loss: 2.021784, Validation Loss: 2.021582\n",
      "Epoch : [2218/8999], Training Loss: 2.021722, Validation Loss: 2.021558\n",
      "Epoch : [2219/8999], Training Loss: 2.021632, Validation Loss: 2.021552\n",
      "Epoch : [2220/8999], Training Loss: 2.021760, Validation Loss: 2.021630\n",
      "Epoch : [2221/8999], Training Loss: 2.021774, Validation Loss: 2.021618\n",
      "Epoch : [2222/8999], Training Loss: 2.021726, Validation Loss: 2.021758\n",
      "Epoch : [2223/8999], Training Loss: 2.021748, Validation Loss: 2.021696\n",
      "Epoch : [2224/8999], Training Loss: 2.021725, Validation Loss: 2.021666\n",
      "Epoch : [2225/8999], Training Loss: 2.021703, Validation Loss: 2.021661\n",
      "Epoch : [2226/8999], Training Loss: 2.021731, Validation Loss: 2.021618\n",
      "Epoch : [2227/8999], Training Loss: 2.021672, Validation Loss: 2.021642\n",
      "Epoch : [2228/8999], Training Loss: 2.021691, Validation Loss: 2.021728\n",
      "Epoch : [2229/8999], Training Loss: 2.021783, Validation Loss: 2.021811\n",
      "Epoch : [2230/8999], Training Loss: 2.021670, Validation Loss: 2.021741\n",
      "Epoch : [2231/8999], Training Loss: 2.021653, Validation Loss: 2.021644\n",
      "Epoch : [2232/8999], Training Loss: 2.021655, Validation Loss: 2.021629\n",
      "Epoch : [2233/8999], Training Loss: 2.021637, Validation Loss: 2.021749\n",
      "Epoch : [2234/8999], Training Loss: 2.021639, Validation Loss: 2.021681\n",
      "Epoch : [2235/8999], Training Loss: 2.021567, Validation Loss: 2.021642\n",
      "Epoch : [2236/8999], Training Loss: 2.021575, Validation Loss: 2.021644\n",
      "Epoch : [2237/8999], Training Loss: 2.021573, Validation Loss: 2.021890\n",
      "Epoch : [2238/8999], Training Loss: 2.021614, Validation Loss: 2.021664\n",
      "Epoch : [2239/8999], Training Loss: 2.021694, Validation Loss: 2.022042\n",
      "Epoch : [2240/8999], Training Loss: 2.021636, Validation Loss: 2.021955\n",
      "Epoch : [2241/8999], Training Loss: 2.021836, Validation Loss: 2.021873\n",
      "Epoch : [2242/8999], Training Loss: 2.021706, Validation Loss: 2.022084\n",
      "Epoch : [2243/8999], Training Loss: 2.021928, Validation Loss: 2.021863\n",
      "Epoch : [2244/8999], Training Loss: 2.021824, Validation Loss: 2.022245\n",
      "Epoch : [2245/8999], Training Loss: 2.021739, Validation Loss: 2.021882\n",
      "Epoch : [2246/8999], Training Loss: 2.021940, Validation Loss: 2.021965\n",
      "Epoch : [2247/8999], Training Loss: 2.021737, Validation Loss: 2.021922\n",
      "Epoch : [2248/8999], Training Loss: 2.021788, Validation Loss: 2.021767\n",
      "Epoch : [2249/8999], Training Loss: 2.021739, Validation Loss: 2.021855\n",
      "Epoch : [2250/8999], Training Loss: 2.021686, Validation Loss: 2.021683\n",
      "Epoch : [2251/8999], Training Loss: 2.021702, Validation Loss: 2.021784\n",
      "Epoch : [2252/8999], Training Loss: 2.021608, Validation Loss: 2.021903\n",
      "Epoch : [2253/8999], Training Loss: 2.021593, Validation Loss: 2.021761\n",
      "Epoch : [2254/8999], Training Loss: 2.021613, Validation Loss: 2.021698\n",
      "Epoch : [2255/8999], Training Loss: 2.021590, Validation Loss: 2.021671\n",
      "Epoch : [2256/8999], Training Loss: 2.021567, Validation Loss: 2.021678\n",
      "Epoch : [2257/8999], Training Loss: 2.021594, Validation Loss: 2.021539\n",
      "Epoch : [2258/8999], Training Loss: 2.021654, Validation Loss: 2.021925\n",
      "Epoch : [2259/8999], Training Loss: 2.021736, Validation Loss: 2.021686\n",
      "Epoch : [2260/8999], Training Loss: 2.021781, Validation Loss: 2.021708\n",
      "Epoch : [2261/8999], Training Loss: 2.021593, Validation Loss: 2.021573\n",
      "Epoch : [2262/8999], Training Loss: 2.021592, Validation Loss: 2.021569\n",
      "Epoch : [2263/8999], Training Loss: 2.021710, Validation Loss: 2.021672\n",
      "Epoch : [2264/8999], Training Loss: 2.021681, Validation Loss: 2.021529\n",
      "Epoch : [2265/8999], Training Loss: 2.021619, Validation Loss: 2.021704\n",
      "Epoch : [2266/8999], Training Loss: 2.021594, Validation Loss: 2.021584\n",
      "Epoch : [2267/8999], Training Loss: 2.021544, Validation Loss: 2.021525\n",
      "Epoch : [2268/8999], Training Loss: 2.021735, Validation Loss: 2.021535\n",
      "Epoch : [2269/8999], Training Loss: 2.021579, Validation Loss: 2.021550\n",
      "Epoch : [2270/8999], Training Loss: 2.021578, Validation Loss: 2.021469\n",
      "Epoch : [2271/8999], Training Loss: 2.021605, Validation Loss: 2.021624\n",
      "Epoch : [2272/8999], Training Loss: 2.021671, Validation Loss: 2.021606\n",
      "Epoch : [2273/8999], Training Loss: 2.021733, Validation Loss: 2.021552\n",
      "Epoch : [2274/8999], Training Loss: 2.021713, Validation Loss: 2.021552\n",
      "Epoch : [2275/8999], Training Loss: 2.021631, Validation Loss: 2.021537\n",
      "Epoch : [2276/8999], Training Loss: 2.021838, Validation Loss: 2.021624\n",
      "Epoch : [2277/8999], Training Loss: 2.021662, Validation Loss: 2.021814\n",
      "Epoch : [2278/8999], Training Loss: 2.021816, Validation Loss: 2.021693\n",
      "Epoch : [2279/8999], Training Loss: 2.021595, Validation Loss: 2.021571\n",
      "Epoch : [2280/8999], Training Loss: 2.021652, Validation Loss: 2.021528\n",
      "Epoch : [2281/8999], Training Loss: 2.021538, Validation Loss: 2.021634\n",
      "Epoch : [2282/8999], Training Loss: 2.021691, Validation Loss: 2.021739\n",
      "Epoch : [2283/8999], Training Loss: 2.021639, Validation Loss: 2.021720\n",
      "Epoch : [2284/8999], Training Loss: 2.021760, Validation Loss: 2.021527\n",
      "Epoch : [2285/8999], Training Loss: 2.021582, Validation Loss: 2.021734\n",
      "Epoch : [2286/8999], Training Loss: 2.021543, Validation Loss: 2.021474\n",
      "Epoch : [2287/8999], Training Loss: 2.021436, Validation Loss: 2.021508\n",
      "Epoch : [2288/8999], Training Loss: 2.021482, Validation Loss: 2.021624\n",
      "Epoch : [2289/8999], Training Loss: 2.021458, Validation Loss: 2.021473\n",
      "Epoch : [2290/8999], Training Loss: 2.021552, Validation Loss: 2.021683\n",
      "Epoch : [2291/8999], Training Loss: 2.021554, Validation Loss: 2.021642\n",
      "Epoch : [2292/8999], Training Loss: 2.021643, Validation Loss: 2.021534\n",
      "Epoch : [2293/8999], Training Loss: 2.021438, Validation Loss: 2.021635\n",
      "Epoch : [2294/8999], Training Loss: 2.021555, Validation Loss: 2.021872\n",
      "Epoch : [2295/8999], Training Loss: 2.021510, Validation Loss: 2.021368\n",
      "Epoch : [2296/8999], Training Loss: 2.021540, Validation Loss: 2.021657\n",
      "Epoch : [2297/8999], Training Loss: 2.021538, Validation Loss: 2.021656\n",
      "Epoch : [2298/8999], Training Loss: 2.021484, Validation Loss: 2.021476\n",
      "Epoch : [2299/8999], Training Loss: 2.021506, Validation Loss: 2.021442\n",
      "Epoch : [2300/8999], Training Loss: 2.021470, Validation Loss: 2.021312\n",
      "Epoch : [2301/8999], Training Loss: 2.021703, Validation Loss: 2.021290\n",
      "Epoch : [2302/8999], Training Loss: 2.021327, Validation Loss: 2.021248\n",
      "Epoch : [2303/8999], Training Loss: 2.021458, Validation Loss: 2.021397\n",
      "Epoch : [2304/8999], Training Loss: 2.021335, Validation Loss: 2.021194\n",
      "Epoch : [2305/8999], Training Loss: 2.021438, Validation Loss: 2.021360\n",
      "Epoch : [2306/8999], Training Loss: 2.021410, Validation Loss: 2.021329\n",
      "Epoch : [2307/8999], Training Loss: 2.021431, Validation Loss: 2.021469\n",
      "Epoch : [2308/8999], Training Loss: 2.021503, Validation Loss: 2.021431\n",
      "Epoch : [2309/8999], Training Loss: 2.021463, Validation Loss: 2.021408\n",
      "Epoch : [2310/8999], Training Loss: 2.021457, Validation Loss: 2.021424\n",
      "Epoch : [2311/8999], Training Loss: 2.021337, Validation Loss: 2.021190\n",
      "Epoch : [2312/8999], Training Loss: 2.021433, Validation Loss: 2.021456\n",
      "Epoch : [2313/8999], Training Loss: 2.021371, Validation Loss: 2.021214\n",
      "Epoch : [2314/8999], Training Loss: 2.021313, Validation Loss: 2.021281\n",
      "Epoch : [2315/8999], Training Loss: 2.021427, Validation Loss: 2.021208\n",
      "Epoch : [2316/8999], Training Loss: 2.021411, Validation Loss: 2.021261\n",
      "Epoch : [2317/8999], Training Loss: 2.021520, Validation Loss: 2.021206\n",
      "Epoch : [2318/8999], Training Loss: 2.021297, Validation Loss: 2.021180\n",
      "Epoch : [2319/8999], Training Loss: 2.021333, Validation Loss: 2.021275\n",
      "Epoch : [2320/8999], Training Loss: 2.021291, Validation Loss: 2.021203\n",
      "Epoch : [2321/8999], Training Loss: 2.021274, Validation Loss: 2.021237\n",
      "Epoch : [2322/8999], Training Loss: 2.021297, Validation Loss: 2.021356\n",
      "Epoch : [2323/8999], Training Loss: 2.021321, Validation Loss: 2.021474\n",
      "Epoch : [2324/8999], Training Loss: 2.021385, Validation Loss: 2.021391\n",
      "Epoch : [2325/8999], Training Loss: 2.021274, Validation Loss: 2.021275\n",
      "Epoch : [2326/8999], Training Loss: 2.021307, Validation Loss: 2.021246\n",
      "Epoch : [2327/8999], Training Loss: 2.021353, Validation Loss: 2.021345\n",
      "Epoch : [2328/8999], Training Loss: 2.021315, Validation Loss: 2.021362\n",
      "Epoch : [2329/8999], Training Loss: 2.021792, Validation Loss: 2.021289\n",
      "Epoch : [2330/8999], Training Loss: 2.021263, Validation Loss: 2.021267\n",
      "Epoch : [2331/8999], Training Loss: 2.021374, Validation Loss: 2.021195\n",
      "Epoch : [2332/8999], Training Loss: 2.021213, Validation Loss: 2.021187\n",
      "Epoch : [2333/8999], Training Loss: 2.021299, Validation Loss: 2.021363\n",
      "Epoch : [2334/8999], Training Loss: 2.021299, Validation Loss: 2.021197\n",
      "Epoch : [2335/8999], Training Loss: 2.021321, Validation Loss: 2.021365\n",
      "Epoch : [2336/8999], Training Loss: 2.021278, Validation Loss: 2.021180\n",
      "Epoch : [2337/8999], Training Loss: 2.021203, Validation Loss: 2.021209\n",
      "Epoch : [2338/8999], Training Loss: 2.021270, Validation Loss: 2.021252\n",
      "Epoch : [2339/8999], Training Loss: 2.021319, Validation Loss: 2.021271\n",
      "Epoch : [2340/8999], Training Loss: 2.021307, Validation Loss: 2.021434\n",
      "Epoch : [2341/8999], Training Loss: 2.021341, Validation Loss: 2.021245\n",
      "Epoch : [2342/8999], Training Loss: 2.021234, Validation Loss: 2.021326\n",
      "Epoch : [2343/8999], Training Loss: 2.021239, Validation Loss: 2.021349\n",
      "Epoch : [2344/8999], Training Loss: 2.021313, Validation Loss: 2.021169\n",
      "Epoch : [2345/8999], Training Loss: 2.021351, Validation Loss: 2.021385\n",
      "Epoch : [2346/8999], Training Loss: 2.021301, Validation Loss: 2.021220\n",
      "Epoch : [2347/8999], Training Loss: 2.021457, Validation Loss: 2.021080\n",
      "Epoch : [2348/8999], Training Loss: 2.021310, Validation Loss: 2.021477\n",
      "Epoch : [2349/8999], Training Loss: 2.021360, Validation Loss: 2.021206\n",
      "Epoch : [2350/8999], Training Loss: 2.021220, Validation Loss: 2.021092\n",
      "Epoch : [2351/8999], Training Loss: 2.021312, Validation Loss: 2.021446\n",
      "Epoch : [2352/8999], Training Loss: 2.021234, Validation Loss: 2.021070\n",
      "Epoch : [2353/8999], Training Loss: 2.021439, Validation Loss: 2.021270\n",
      "Epoch : [2354/8999], Training Loss: 2.021152, Validation Loss: 2.021181\n",
      "Epoch : [2355/8999], Training Loss: 2.021135, Validation Loss: 2.021091\n",
      "Epoch : [2356/8999], Training Loss: 2.021080, Validation Loss: 2.021123\n",
      "Epoch : [2357/8999], Training Loss: 2.021129, Validation Loss: 2.021132\n",
      "Epoch : [2358/8999], Training Loss: 2.021116, Validation Loss: 2.021152\n",
      "Epoch : [2359/8999], Training Loss: 2.021241, Validation Loss: 2.021134\n",
      "Epoch : [2360/8999], Training Loss: 2.021267, Validation Loss: 2.021206\n",
      "Epoch : [2361/8999], Training Loss: 2.021217, Validation Loss: 2.021016\n",
      "Epoch : [2362/8999], Training Loss: 2.021164, Validation Loss: 2.021033\n",
      "Epoch : [2363/8999], Training Loss: 2.021112, Validation Loss: 2.021214\n",
      "Epoch : [2364/8999], Training Loss: 2.021205, Validation Loss: 2.021245\n",
      "Epoch : [2365/8999], Training Loss: 2.021170, Validation Loss: 2.021115\n",
      "Epoch : [2366/8999], Training Loss: 2.021171, Validation Loss: 2.021053\n",
      "Epoch : [2367/8999], Training Loss: 2.021058, Validation Loss: 2.020998\n",
      "Epoch : [2368/8999], Training Loss: 2.021161, Validation Loss: 2.021188\n",
      "Epoch : [2369/8999], Training Loss: 2.021147, Validation Loss: 2.020947\n",
      "Epoch : [2370/8999], Training Loss: 2.020971, Validation Loss: 2.020964\n",
      "Epoch : [2371/8999], Training Loss: 2.021044, Validation Loss: 2.021177\n",
      "Epoch : [2372/8999], Training Loss: 2.021166, Validation Loss: 2.020915\n",
      "Epoch : [2373/8999], Training Loss: 2.021112, Validation Loss: 2.021136\n",
      "Epoch : [2374/8999], Training Loss: 2.021252, Validation Loss: 2.021034\n",
      "Epoch : [2375/8999], Training Loss: 2.021218, Validation Loss: 2.021080\n",
      "Epoch : [2376/8999], Training Loss: 2.021078, Validation Loss: 2.020934\n",
      "Epoch : [2377/8999], Training Loss: 2.020993, Validation Loss: 2.021156\n",
      "Epoch : [2378/8999], Training Loss: 2.021172, Validation Loss: 2.021171\n",
      "Epoch : [2379/8999], Training Loss: 2.021241, Validation Loss: 2.021028\n",
      "Epoch : [2380/8999], Training Loss: 2.021135, Validation Loss: 2.020874\n",
      "Epoch : [2381/8999], Training Loss: 2.020970, Validation Loss: 2.020910\n",
      "Epoch : [2382/8999], Training Loss: 2.020998, Validation Loss: 2.021075\n",
      "Epoch : [2383/8999], Training Loss: 2.021029, Validation Loss: 2.020832\n",
      "Epoch : [2384/8999], Training Loss: 2.021033, Validation Loss: 2.020974\n",
      "Epoch : [2385/8999], Training Loss: 2.020962, Validation Loss: 2.020898\n",
      "Epoch : [2386/8999], Training Loss: 2.021170, Validation Loss: 2.020896\n",
      "Epoch : [2387/8999], Training Loss: 2.021012, Validation Loss: 2.020852\n",
      "Epoch : [2388/8999], Training Loss: 2.021137, Validation Loss: 2.021063\n",
      "Epoch : [2389/8999], Training Loss: 2.020964, Validation Loss: 2.020777\n",
      "Epoch : [2390/8999], Training Loss: 2.020929, Validation Loss: 2.020751\n",
      "Epoch : [2391/8999], Training Loss: 2.020911, Validation Loss: 2.020882\n",
      "Epoch : [2392/8999], Training Loss: 2.020817, Validation Loss: 2.020906\n",
      "Epoch : [2393/8999], Training Loss: 2.020981, Validation Loss: 2.021011\n",
      "Epoch : [2394/8999], Training Loss: 2.020842, Validation Loss: 2.020955\n",
      "Epoch : [2395/8999], Training Loss: 2.020976, Validation Loss: 2.020804\n",
      "Epoch : [2396/8999], Training Loss: 2.021044, Validation Loss: 2.020934\n",
      "Epoch : [2397/8999], Training Loss: 2.020800, Validation Loss: 2.020876\n",
      "Epoch : [2398/8999], Training Loss: 2.020868, Validation Loss: 2.020751\n",
      "Epoch : [2399/8999], Training Loss: 2.020902, Validation Loss: 2.021042\n",
      "Epoch : [2400/8999], Training Loss: 2.021063, Validation Loss: 2.021084\n",
      "Epoch : [2401/8999], Training Loss: 2.020894, Validation Loss: 2.021049\n",
      "Epoch : [2402/8999], Training Loss: 2.020894, Validation Loss: 2.020691\n",
      "Epoch : [2403/8999], Training Loss: 2.020799, Validation Loss: 2.021117\n",
      "Epoch : [2404/8999], Training Loss: 2.020843, Validation Loss: 2.021032\n",
      "Epoch : [2405/8999], Training Loss: 2.020699, Validation Loss: 2.020919\n",
      "Epoch : [2406/8999], Training Loss: 2.020780, Validation Loss: 2.020759\n",
      "Epoch : [2407/8999], Training Loss: 2.020764, Validation Loss: 2.021144\n",
      "Epoch : [2408/8999], Training Loss: 2.020829, Validation Loss: 2.020861\n",
      "Epoch : [2409/8999], Training Loss: 2.020785, Validation Loss: 2.020699\n",
      "Epoch : [2410/8999], Training Loss: 2.020724, Validation Loss: 2.020912\n",
      "Epoch : [2411/8999], Training Loss: 2.020678, Validation Loss: 2.020904\n",
      "Epoch : [2412/8999], Training Loss: 2.020587, Validation Loss: 2.020808\n",
      "Epoch : [2413/8999], Training Loss: 2.020696, Validation Loss: 2.020781\n",
      "Epoch : [2414/8999], Training Loss: 2.020701, Validation Loss: 2.021107\n",
      "Epoch : [2415/8999], Training Loss: 2.020741, Validation Loss: 2.020969\n",
      "Epoch : [2416/8999], Training Loss: 2.020696, Validation Loss: 2.020713\n",
      "Epoch : [2417/8999], Training Loss: 2.020762, Validation Loss: 2.020755\n",
      "Epoch : [2418/8999], Training Loss: 2.020717, Validation Loss: 2.020949\n",
      "Epoch : [2419/8999], Training Loss: 2.020760, Validation Loss: 2.020679\n",
      "Epoch : [2420/8999], Training Loss: 2.020871, Validation Loss: 2.020849\n",
      "Epoch : [2421/8999], Training Loss: 2.020728, Validation Loss: 2.020880\n",
      "Epoch : [2422/8999], Training Loss: 2.020701, Validation Loss: 2.021008\n",
      "Epoch : [2423/8999], Training Loss: 2.020729, Validation Loss: 2.020796\n",
      "Epoch : [2424/8999], Training Loss: 2.020659, Validation Loss: 2.020658\n",
      "Epoch : [2425/8999], Training Loss: 2.020734, Validation Loss: 2.020695\n",
      "Epoch : [2426/8999], Training Loss: 2.020708, Validation Loss: 2.020674\n",
      "Epoch : [2427/8999], Training Loss: 2.020621, Validation Loss: 2.020703\n",
      "Epoch : [2428/8999], Training Loss: 2.020694, Validation Loss: 2.020860\n",
      "Epoch : [2429/8999], Training Loss: 2.020657, Validation Loss: 2.020820\n",
      "Epoch : [2430/8999], Training Loss: 2.020638, Validation Loss: 2.020573\n",
      "Epoch : [2431/8999], Training Loss: 2.020541, Validation Loss: 2.020658\n",
      "Epoch : [2432/8999], Training Loss: 2.020693, Validation Loss: 2.020591\n",
      "Epoch : [2433/8999], Training Loss: 2.020564, Validation Loss: 2.020519\n",
      "Epoch : [2434/8999], Training Loss: 2.020603, Validation Loss: 2.020747\n",
      "Epoch : [2435/8999], Training Loss: 2.020643, Validation Loss: 2.020618\n",
      "Epoch : [2436/8999], Training Loss: 2.020570, Validation Loss: 2.020467\n",
      "Epoch : [2437/8999], Training Loss: 2.020456, Validation Loss: 2.020491\n",
      "Epoch : [2438/8999], Training Loss: 2.020602, Validation Loss: 2.020456\n",
      "Epoch : [2439/8999], Training Loss: 2.020514, Validation Loss: 2.020469\n",
      "Epoch : [2440/8999], Training Loss: 2.020690, Validation Loss: 2.020764\n",
      "Epoch : [2441/8999], Training Loss: 2.020574, Validation Loss: 2.020630\n",
      "Epoch : [2442/8999], Training Loss: 2.020616, Validation Loss: 2.020460\n",
      "Epoch : [2443/8999], Training Loss: 2.020575, Validation Loss: 2.020485\n",
      "Epoch : [2444/8999], Training Loss: 2.020475, Validation Loss: 2.020632\n",
      "Epoch : [2445/8999], Training Loss: 2.020935, Validation Loss: 2.020565\n",
      "Epoch : [2446/8999], Training Loss: 2.020588, Validation Loss: 2.020587\n",
      "Epoch : [2447/8999], Training Loss: 2.020618, Validation Loss: 2.020450\n",
      "Epoch : [2448/8999], Training Loss: 2.020560, Validation Loss: 2.020424\n",
      "Epoch : [2449/8999], Training Loss: 2.020480, Validation Loss: 2.020363\n",
      "Epoch : [2450/8999], Training Loss: 2.020509, Validation Loss: 2.020385\n",
      "Epoch : [2451/8999], Training Loss: 2.020526, Validation Loss: 2.020532\n",
      "Epoch : [2452/8999], Training Loss: 2.020500, Validation Loss: 2.020411\n",
      "Epoch : [2453/8999], Training Loss: 2.020586, Validation Loss: 2.020358\n",
      "Epoch : [2454/8999], Training Loss: 2.020555, Validation Loss: 2.020580\n",
      "Epoch : [2455/8999], Training Loss: 2.020595, Validation Loss: 2.020564\n",
      "Epoch : [2456/8999], Training Loss: 2.020573, Validation Loss: 2.020432\n",
      "Epoch : [2457/8999], Training Loss: 2.020508, Validation Loss: 2.020811\n",
      "Epoch : [2458/8999], Training Loss: 2.020812, Validation Loss: 2.020521\n",
      "Epoch : [2459/8999], Training Loss: 2.020565, Validation Loss: 2.020459\n",
      "Epoch : [2460/8999], Training Loss: 2.020518, Validation Loss: 2.020643\n",
      "Epoch : [2461/8999], Training Loss: 2.020687, Validation Loss: 2.020734\n",
      "Epoch : [2462/8999], Training Loss: 2.020581, Validation Loss: 2.020514\n",
      "Epoch : [2463/8999], Training Loss: 2.020456, Validation Loss: 2.020423\n",
      "Epoch : [2464/8999], Training Loss: 2.020351, Validation Loss: 2.020451\n",
      "Epoch : [2465/8999], Training Loss: 2.020415, Validation Loss: 2.020493\n",
      "Epoch : [2466/8999], Training Loss: 2.020540, Validation Loss: 2.020436\n",
      "Epoch : [2467/8999], Training Loss: 2.020496, Validation Loss: 2.020741\n",
      "Epoch : [2468/8999], Training Loss: 2.020449, Validation Loss: 2.020476\n",
      "Epoch : [2469/8999], Training Loss: 2.020632, Validation Loss: 2.020448\n",
      "Epoch : [2470/8999], Training Loss: 2.020572, Validation Loss: 2.020788\n",
      "Epoch : [2471/8999], Training Loss: 2.020572, Validation Loss: 2.020339\n",
      "Epoch : [2472/8999], Training Loss: 2.020571, Validation Loss: 2.020608\n",
      "Epoch : [2473/8999], Training Loss: 2.020376, Validation Loss: 2.020591\n",
      "Epoch : [2474/8999], Training Loss: 2.020453, Validation Loss: 2.020678\n",
      "Epoch : [2475/8999], Training Loss: 2.020404, Validation Loss: 2.020575\n",
      "Epoch : [2476/8999], Training Loss: 2.020341, Validation Loss: 2.020366\n",
      "Epoch : [2477/8999], Training Loss: 2.020456, Validation Loss: 2.020485\n",
      "Epoch : [2478/8999], Training Loss: 2.020514, Validation Loss: 2.020973\n",
      "Epoch : [2479/8999], Training Loss: 2.020532, Validation Loss: 2.020381\n",
      "Epoch : [2480/8999], Training Loss: 2.020489, Validation Loss: 2.020580\n",
      "Epoch : [2481/8999], Training Loss: 2.020343, Validation Loss: 2.020484\n",
      "Epoch : [2482/8999], Training Loss: 2.020406, Validation Loss: 2.020492\n",
      "Epoch : [2483/8999], Training Loss: 2.020392, Validation Loss: 2.020420\n",
      "Epoch : [2484/8999], Training Loss: 2.020432, Validation Loss: 2.020765\n",
      "Epoch : [2485/8999], Training Loss: 2.020379, Validation Loss: 2.020537\n",
      "Epoch : [2486/8999], Training Loss: 2.020336, Validation Loss: 2.020371\n",
      "Epoch : [2487/8999], Training Loss: 2.020459, Validation Loss: 2.020739\n",
      "Epoch : [2488/8999], Training Loss: 2.020478, Validation Loss: 2.020380\n",
      "Epoch : [2489/8999], Training Loss: 2.020404, Validation Loss: 2.020535\n",
      "Epoch : [2490/8999], Training Loss: 2.020412, Validation Loss: 2.020896\n",
      "Epoch : [2491/8999], Training Loss: 2.020465, Validation Loss: 2.020256\n",
      "Epoch : [2492/8999], Training Loss: 2.020696, Validation Loss: 2.020500\n",
      "Epoch : [2493/8999], Training Loss: 2.020609, Validation Loss: 2.020614\n",
      "Epoch : [2494/8999], Training Loss: 2.020415, Validation Loss: 2.020205\n",
      "Epoch : [2495/8999], Training Loss: 2.020352, Validation Loss: 2.020508\n",
      "Epoch : [2496/8999], Training Loss: 2.020345, Validation Loss: 2.020504\n",
      "Epoch : [2497/8999], Training Loss: 2.020361, Validation Loss: 2.020221\n",
      "Epoch : [2498/8999], Training Loss: 2.020498, Validation Loss: 2.020453\n",
      "Epoch : [2499/8999], Training Loss: 2.020383, Validation Loss: 2.020417\n",
      "Epoch : [2500/8999], Training Loss: 2.020330, Validation Loss: 2.020218\n",
      "Epoch : [2501/8999], Training Loss: 2.020429, Validation Loss: 2.020413\n",
      "Epoch : [2502/8999], Training Loss: 2.020391, Validation Loss: 2.020391\n",
      "Epoch : [2503/8999], Training Loss: 2.020613, Validation Loss: 2.020224\n",
      "Epoch : [2504/8999], Training Loss: 2.020760, Validation Loss: 2.020391\n",
      "Epoch : [2505/8999], Training Loss: 2.020472, Validation Loss: 2.020227\n",
      "Epoch : [2506/8999], Training Loss: 2.020317, Validation Loss: 2.020307\n",
      "Epoch : [2507/8999], Training Loss: 2.020376, Validation Loss: 2.020279\n",
      "Epoch : [2508/8999], Training Loss: 2.020270, Validation Loss: 2.020260\n",
      "Epoch : [2509/8999], Training Loss: 2.020327, Validation Loss: 2.020326\n",
      "Epoch : [2510/8999], Training Loss: 2.020319, Validation Loss: 2.020325\n",
      "Epoch : [2511/8999], Training Loss: 2.020333, Validation Loss: 2.020229\n",
      "Epoch : [2512/8999], Training Loss: 2.020359, Validation Loss: 2.020307\n",
      "Epoch : [2513/8999], Training Loss: 2.020393, Validation Loss: 2.020343\n",
      "Epoch : [2514/8999], Training Loss: 2.020439, Validation Loss: 2.020375\n",
      "Epoch : [2515/8999], Training Loss: 2.020340, Validation Loss: 2.020216\n",
      "Epoch : [2516/8999], Training Loss: 2.020549, Validation Loss: 2.020329\n",
      "Epoch : [2517/8999], Training Loss: 2.020416, Validation Loss: 2.020344\n",
      "Epoch : [2518/8999], Training Loss: 2.020382, Validation Loss: 2.020262\n",
      "Epoch : [2519/8999], Training Loss: 2.020236, Validation Loss: 2.020137\n",
      "Epoch : [2520/8999], Training Loss: 2.020222, Validation Loss: 2.020051\n",
      "Epoch : [2521/8999], Training Loss: 2.020164, Validation Loss: 2.020104\n",
      "Epoch : [2522/8999], Training Loss: 2.020228, Validation Loss: 2.020262\n",
      "Epoch : [2523/8999], Training Loss: 2.020286, Validation Loss: 2.020212\n",
      "Epoch : [2524/8999], Training Loss: 2.020300, Validation Loss: 2.020134\n",
      "Epoch : [2525/8999], Training Loss: 2.020222, Validation Loss: 2.020327\n",
      "Epoch : [2526/8999], Training Loss: 2.020286, Validation Loss: 2.020156\n",
      "Epoch : [2527/8999], Training Loss: 2.020148, Validation Loss: 2.020128\n",
      "Epoch : [2528/8999], Training Loss: 2.020290, Validation Loss: 2.020171\n",
      "Epoch : [2529/8999], Training Loss: 2.020240, Validation Loss: 2.020123\n",
      "Epoch : [2530/8999], Training Loss: 2.020155, Validation Loss: 2.020131\n",
      "Epoch : [2531/8999], Training Loss: 2.020279, Validation Loss: 2.020241\n",
      "Epoch : [2532/8999], Training Loss: 2.020145, Validation Loss: 2.020249\n",
      "Epoch : [2533/8999], Training Loss: 2.020379, Validation Loss: 2.020125\n",
      "Epoch : [2534/8999], Training Loss: 2.020308, Validation Loss: 2.020352\n",
      "Epoch : [2535/8999], Training Loss: 2.020229, Validation Loss: 2.020294\n",
      "Epoch : [2536/8999], Training Loss: 2.020316, Validation Loss: 2.020070\n",
      "Epoch : [2537/8999], Training Loss: 2.020194, Validation Loss: 2.020165\n",
      "Epoch : [2538/8999], Training Loss: 2.020262, Validation Loss: 2.020351\n",
      "Epoch : [2539/8999], Training Loss: 2.020248, Validation Loss: 2.020211\n",
      "Epoch : [2540/8999], Training Loss: 2.020218, Validation Loss: 2.020117\n",
      "Epoch : [2541/8999], Training Loss: 2.020192, Validation Loss: 2.020371\n",
      "Epoch : [2542/8999], Training Loss: 2.020403, Validation Loss: 2.020335\n",
      "Epoch : [2543/8999], Training Loss: 2.020291, Validation Loss: 2.020309\n",
      "Epoch : [2544/8999], Training Loss: 2.020316, Validation Loss: 2.020446\n",
      "Epoch : [2545/8999], Training Loss: 2.020313, Validation Loss: 2.020131\n",
      "Epoch : [2546/8999], Training Loss: 2.020154, Validation Loss: 2.020104\n",
      "Epoch : [2547/8999], Training Loss: 2.020148, Validation Loss: 2.020127\n",
      "Epoch : [2548/8999], Training Loss: 2.020244, Validation Loss: 2.020339\n",
      "Epoch : [2549/8999], Training Loss: 2.020345, Validation Loss: 2.020505\n",
      "Epoch : [2550/8999], Training Loss: 2.020228, Validation Loss: 2.020270\n",
      "Epoch : [2551/8999], Training Loss: 2.020247, Validation Loss: 2.020182\n",
      "Epoch : [2552/8999], Training Loss: 2.020388, Validation Loss: 2.020486\n",
      "Epoch : [2553/8999], Training Loss: 2.020447, Validation Loss: 2.020409\n",
      "Epoch : [2554/8999], Training Loss: 2.020163, Validation Loss: 2.020335\n",
      "Epoch : [2555/8999], Training Loss: 2.020224, Validation Loss: 2.020317\n",
      "Epoch : [2556/8999], Training Loss: 2.020249, Validation Loss: 2.020172\n",
      "Epoch : [2557/8999], Training Loss: 2.020209, Validation Loss: 2.020568\n",
      "Epoch : [2558/8999], Training Loss: 2.020296, Validation Loss: 2.020310\n",
      "Epoch : [2559/8999], Training Loss: 2.020246, Validation Loss: 2.020558\n",
      "Epoch : [2560/8999], Training Loss: 2.020331, Validation Loss: 2.020532\n",
      "Epoch : [2561/8999], Training Loss: 2.020161, Validation Loss: 2.020350\n",
      "Epoch : [2562/8999], Training Loss: 2.020109, Validation Loss: 2.020189\n",
      "Epoch : [2563/8999], Training Loss: 2.020074, Validation Loss: 2.020592\n",
      "Epoch : [2564/8999], Training Loss: 2.020197, Validation Loss: 2.020132\n",
      "Epoch : [2565/8999], Training Loss: 2.020143, Validation Loss: 2.020231\n",
      "Epoch : [2566/8999], Training Loss: 2.020280, Validation Loss: 2.020385\n",
      "Epoch : [2567/8999], Training Loss: 2.020131, Validation Loss: 2.020509\n",
      "Epoch : [2568/8999], Training Loss: 2.020229, Validation Loss: 2.020242\n",
      "Epoch : [2569/8999], Training Loss: 2.020200, Validation Loss: 2.020245\n",
      "Epoch : [2570/8999], Training Loss: 2.020240, Validation Loss: 2.020334\n",
      "Epoch : [2571/8999], Training Loss: 2.020230, Validation Loss: 2.020342\n",
      "Epoch : [2572/8999], Training Loss: 2.020239, Validation Loss: 2.020381\n",
      "Epoch : [2573/8999], Training Loss: 2.020258, Validation Loss: 2.020222\n",
      "Epoch : [2574/8999], Training Loss: 2.020298, Validation Loss: 2.020385\n",
      "Epoch : [2575/8999], Training Loss: 2.020101, Validation Loss: 2.020260\n",
      "Epoch : [2576/8999], Training Loss: 2.020155, Validation Loss: 2.020395\n",
      "Epoch : [2577/8999], Training Loss: 2.020263, Validation Loss: 2.020243\n",
      "Epoch : [2578/8999], Training Loss: 2.020225, Validation Loss: 2.020230\n",
      "Epoch : [2579/8999], Training Loss: 2.020123, Validation Loss: 2.020294\n",
      "Epoch : [2580/8999], Training Loss: 2.020224, Validation Loss: 2.020121\n",
      "Epoch : [2581/8999], Training Loss: 2.020117, Validation Loss: 2.020276\n",
      "Epoch : [2582/8999], Training Loss: 2.020084, Validation Loss: 2.020358\n",
      "Epoch : [2583/8999], Training Loss: 2.020319, Validation Loss: 2.020266\n",
      "Epoch : [2584/8999], Training Loss: 2.020186, Validation Loss: 2.020351\n",
      "Epoch : [2585/8999], Training Loss: 2.020337, Validation Loss: 2.020567\n",
      "Epoch : [2586/8999], Training Loss: 2.020307, Validation Loss: 2.020192\n",
      "Epoch : [2587/8999], Training Loss: 2.020156, Validation Loss: 2.020254\n",
      "Epoch : [2588/8999], Training Loss: 2.020097, Validation Loss: 2.020225\n",
      "Epoch : [2589/8999], Training Loss: 2.020409, Validation Loss: 2.020355\n",
      "Epoch : [2590/8999], Training Loss: 2.020234, Validation Loss: 2.020162\n",
      "Epoch : [2591/8999], Training Loss: 2.020099, Validation Loss: 2.020215\n",
      "Epoch : [2592/8999], Training Loss: 2.020265, Validation Loss: 2.020528\n",
      "Epoch : [2593/8999], Training Loss: 2.020220, Validation Loss: 2.020239\n",
      "Epoch : [2594/8999], Training Loss: 2.020299, Validation Loss: 2.020418\n",
      "Epoch : [2595/8999], Training Loss: 2.020159, Validation Loss: 2.020228\n",
      "Epoch : [2596/8999], Training Loss: 2.020354, Validation Loss: 2.020209\n",
      "Epoch : [2597/8999], Training Loss: 2.020070, Validation Loss: 2.020196\n",
      "Epoch : [2598/8999], Training Loss: 2.020237, Validation Loss: 2.020147\n",
      "Epoch : [2599/8999], Training Loss: 2.020292, Validation Loss: 2.020376\n",
      "Epoch : [2600/8999], Training Loss: 2.020216, Validation Loss: 2.020073\n",
      "Epoch : [2601/8999], Training Loss: 2.020310, Validation Loss: 2.020518\n",
      "Epoch : [2602/8999], Training Loss: 2.020265, Validation Loss: 2.020202\n",
      "Epoch : [2603/8999], Training Loss: 2.020174, Validation Loss: 2.020229\n",
      "Epoch : [2604/8999], Training Loss: 2.020150, Validation Loss: 2.020155\n",
      "Epoch : [2605/8999], Training Loss: 2.020183, Validation Loss: 2.020238\n",
      "Epoch : [2606/8999], Training Loss: 2.020302, Validation Loss: 2.020333\n",
      "Epoch : [2607/8999], Training Loss: 2.020393, Validation Loss: 2.020187\n",
      "Epoch : [2608/8999], Training Loss: 2.020408, Validation Loss: 2.020114\n",
      "Epoch : [2609/8999], Training Loss: 2.020173, Validation Loss: 2.020257\n",
      "Epoch : [2610/8999], Training Loss: 2.020128, Validation Loss: 2.020008\n",
      "Epoch : [2611/8999], Training Loss: 2.020215, Validation Loss: 2.020064\n",
      "Epoch : [2612/8999], Training Loss: 2.020164, Validation Loss: 2.020039\n",
      "Epoch : [2613/8999], Training Loss: 2.020245, Validation Loss: 2.020245\n",
      "Epoch : [2614/8999], Training Loss: 2.020449, Validation Loss: 2.020125\n",
      "Epoch : [2615/8999], Training Loss: 2.020050, Validation Loss: 2.019967\n",
      "Epoch : [2616/8999], Training Loss: 2.019988, Validation Loss: 2.019904\n",
      "Epoch : [2617/8999], Training Loss: 2.019981, Validation Loss: 2.020014\n",
      "Epoch : [2618/8999], Training Loss: 2.020067, Validation Loss: 2.020159\n",
      "Epoch : [2619/8999], Training Loss: 2.020137, Validation Loss: 2.019976\n",
      "Epoch : [2620/8999], Training Loss: 2.020084, Validation Loss: 2.020246\n",
      "Epoch : [2621/8999], Training Loss: 2.020099, Validation Loss: 2.020115\n",
      "Epoch : [2622/8999], Training Loss: 2.020156, Validation Loss: 2.020100\n",
      "Epoch : [2623/8999], Training Loss: 2.020201, Validation Loss: 2.020131\n",
      "Epoch : [2624/8999], Training Loss: 2.020144, Validation Loss: 2.020006\n",
      "Epoch : [2625/8999], Training Loss: 2.020047, Validation Loss: 2.020074\n",
      "Epoch : [2626/8999], Training Loss: 2.020282, Validation Loss: 2.020189\n",
      "Epoch : [2627/8999], Training Loss: 2.020127, Validation Loss: 2.020040\n",
      "Epoch : [2628/8999], Training Loss: 2.020078, Validation Loss: 2.020188\n",
      "Epoch : [2629/8999], Training Loss: 2.020051, Validation Loss: 2.019965\n",
      "Epoch : [2630/8999], Training Loss: 2.020142, Validation Loss: 2.020160\n",
      "Epoch : [2631/8999], Training Loss: 2.020040, Validation Loss: 2.020205\n",
      "Epoch : [2632/8999], Training Loss: 2.020288, Validation Loss: 2.020043\n",
      "Epoch : [2633/8999], Training Loss: 2.020092, Validation Loss: 2.020126\n",
      "Epoch : [2634/8999], Training Loss: 2.020209, Validation Loss: 2.020300\n",
      "Epoch : [2635/8999], Training Loss: 2.020377, Validation Loss: 2.020274\n",
      "Epoch : [2636/8999], Training Loss: 2.020193, Validation Loss: 2.020407\n",
      "Epoch : [2637/8999], Training Loss: 2.020126, Validation Loss: 2.020424\n",
      "Epoch : [2638/8999], Training Loss: 2.020218, Validation Loss: 2.020177\n",
      "Epoch : [2639/8999], Training Loss: 2.020074, Validation Loss: 2.020938\n",
      "Epoch : [2640/8999], Training Loss: 2.020174, Validation Loss: 2.020089\n",
      "Epoch : [2641/8999], Training Loss: 2.020335, Validation Loss: 2.020631\n",
      "Epoch : [2642/8999], Training Loss: 2.020274, Validation Loss: 2.020917\n",
      "Epoch : [2643/8999], Training Loss: 2.020333, Validation Loss: 2.020249\n",
      "Epoch : [2644/8999], Training Loss: 2.020712, Validation Loss: 2.020755\n",
      "Epoch : [2645/8999], Training Loss: 2.020295, Validation Loss: 2.020552\n",
      "Epoch : [2646/8999], Training Loss: 2.020281, Validation Loss: 2.020325\n",
      "Epoch : [2647/8999], Training Loss: 2.020183, Validation Loss: 2.021137\n",
      "Epoch : [2648/8999], Training Loss: 2.020280, Validation Loss: 2.020191\n",
      "Epoch : [2649/8999], Training Loss: 2.020247, Validation Loss: 2.020378\n",
      "Epoch : [2650/8999], Training Loss: 2.020152, Validation Loss: 2.020797\n",
      "Epoch : [2651/8999], Training Loss: 2.020078, Validation Loss: 2.020436\n",
      "Epoch : [2652/8999], Training Loss: 2.020210, Validation Loss: 2.020412\n",
      "Epoch : [2653/8999], Training Loss: 2.020323, Validation Loss: 2.021164\n",
      "Epoch : [2654/8999], Training Loss: 2.020636, Validation Loss: 2.020263\n",
      "Epoch : [2655/8999], Training Loss: 2.020518, Validation Loss: 2.020777\n",
      "Epoch : [2656/8999], Training Loss: 2.020258, Validation Loss: 2.020815\n",
      "Epoch : [2657/8999], Training Loss: 2.020296, Validation Loss: 2.020626\n",
      "Epoch : [2658/8999], Training Loss: 2.020406, Validation Loss: 2.021094\n",
      "Epoch : [2659/8999], Training Loss: 2.020042, Validation Loss: 2.020459\n",
      "Epoch : [2660/8999], Training Loss: 2.020422, Validation Loss: 2.020668\n",
      "Epoch : [2661/8999], Training Loss: 2.020359, Validation Loss: 2.021114\n",
      "Epoch : [2662/8999], Training Loss: 2.020279, Validation Loss: 2.020309\n",
      "Epoch : [2663/8999], Training Loss: 2.020794, Validation Loss: 2.021029\n",
      "Epoch : [2664/8999], Training Loss: 2.020380, Validation Loss: 2.020685\n",
      "Epoch : [2665/8999], Training Loss: 2.020536, Validation Loss: 2.020687\n",
      "Epoch : [2666/8999], Training Loss: 2.020192, Validation Loss: 2.020813\n",
      "Epoch : [2667/8999], Training Loss: 2.020190, Validation Loss: 2.020160\n",
      "Epoch : [2668/8999], Training Loss: 2.020326, Validation Loss: 2.020385\n",
      "Epoch : [2669/8999], Training Loss: 2.020313, Validation Loss: 2.020621\n",
      "Epoch : [2670/8999], Training Loss: 2.020156, Validation Loss: 2.020387\n",
      "Epoch : [2671/8999], Training Loss: 2.020201, Validation Loss: 2.020206\n",
      "Epoch : [2672/8999], Training Loss: 2.020200, Validation Loss: 2.020408\n",
      "Epoch : [2673/8999], Training Loss: 2.020108, Validation Loss: 2.020406\n",
      "Epoch : [2674/8999], Training Loss: 2.020148, Validation Loss: 2.020328\n",
      "Epoch : [2675/8999], Training Loss: 2.020386, Validation Loss: 2.020345\n",
      "Epoch : [2676/8999], Training Loss: 2.020302, Validation Loss: 2.020318\n",
      "Epoch : [2677/8999], Training Loss: 2.020262, Validation Loss: 2.020206\n",
      "Epoch : [2678/8999], Training Loss: 2.020282, Validation Loss: 2.020183\n",
      "Epoch : [2679/8999], Training Loss: 2.020140, Validation Loss: 2.020174\n",
      "Epoch : [2680/8999], Training Loss: 2.020207, Validation Loss: 2.020186\n",
      "Epoch : [2681/8999], Training Loss: 2.020048, Validation Loss: 2.019986\n",
      "Epoch : [2682/8999], Training Loss: 2.020079, Validation Loss: 2.020107\n",
      "Epoch : [2683/8999], Training Loss: 2.020236, Validation Loss: 2.020201\n",
      "Epoch : [2684/8999], Training Loss: 2.020230, Validation Loss: 2.020009\n",
      "Epoch : [2685/8999], Training Loss: 2.020254, Validation Loss: 2.020220\n",
      "Epoch : [2686/8999], Training Loss: 2.020125, Validation Loss: 2.020137\n",
      "Epoch : [2687/8999], Training Loss: 2.020197, Validation Loss: 2.019956\n",
      "Epoch : [2688/8999], Training Loss: 2.020078, Validation Loss: 2.020196\n",
      "Epoch : [2689/8999], Training Loss: 2.020151, Validation Loss: 2.019993\n",
      "Epoch : [2690/8999], Training Loss: 2.019995, Validation Loss: 2.019944\n",
      "Epoch : [2691/8999], Training Loss: 2.020145, Validation Loss: 2.019960\n",
      "Epoch : [2692/8999], Training Loss: 2.019963, Validation Loss: 2.019904\n",
      "Epoch : [2693/8999], Training Loss: 2.019998, Validation Loss: 2.019969\n",
      "Epoch : [2694/8999], Training Loss: 2.019903, Validation Loss: 2.019934\n",
      "Epoch : [2695/8999], Training Loss: 2.020178, Validation Loss: 2.020012\n",
      "Epoch : [2696/8999], Training Loss: 2.020129, Validation Loss: 2.020017\n",
      "Epoch : [2697/8999], Training Loss: 2.020224, Validation Loss: 2.020078\n",
      "Epoch : [2698/8999], Training Loss: 2.020252, Validation Loss: 2.020121\n",
      "Epoch : [2699/8999], Training Loss: 2.020016, Validation Loss: 2.019956\n",
      "Epoch : [2700/8999], Training Loss: 2.019990, Validation Loss: 2.020230\n",
      "Epoch : [2701/8999], Training Loss: 2.020614, Validation Loss: 2.020365\n",
      "Epoch : [2702/8999], Training Loss: 2.020762, Validation Loss: 2.020676\n",
      "Epoch : [2703/8999], Training Loss: 2.020837, Validation Loss: 2.020728\n",
      "Epoch : [2704/8999], Training Loss: 2.020207, Validation Loss: 2.019967\n",
      "Epoch : [2705/8999], Training Loss: 2.020176, Validation Loss: 2.020238\n",
      "Epoch : [2706/8999], Training Loss: 2.020048, Validation Loss: 2.020022\n",
      "Epoch : [2707/8999], Training Loss: 2.020017, Validation Loss: 2.020022\n",
      "Epoch : [2708/8999], Training Loss: 2.019961, Validation Loss: 2.020023\n",
      "Epoch : [2709/8999], Training Loss: 2.020114, Validation Loss: 2.020241\n",
      "Epoch : [2710/8999], Training Loss: 2.020043, Validation Loss: 2.019990\n",
      "Epoch : [2711/8999], Training Loss: 2.019982, Validation Loss: 2.019825\n",
      "Epoch : [2712/8999], Training Loss: 2.019976, Validation Loss: 2.020065\n",
      "Epoch : [2713/8999], Training Loss: 2.020022, Validation Loss: 2.019916\n",
      "Epoch : [2714/8999], Training Loss: 2.019897, Validation Loss: 2.020269\n",
      "Epoch : [2715/8999], Training Loss: 2.020081, Validation Loss: 2.020013\n",
      "Epoch : [2716/8999], Training Loss: 2.020033, Validation Loss: 2.020094\n",
      "Epoch : [2717/8999], Training Loss: 2.019952, Validation Loss: 2.019944\n",
      "Epoch : [2718/8999], Training Loss: 2.019958, Validation Loss: 2.020088\n",
      "Epoch : [2719/8999], Training Loss: 2.020057, Validation Loss: 2.020328\n",
      "Epoch : [2720/8999], Training Loss: 2.019958, Validation Loss: 2.020296\n",
      "Epoch : [2721/8999], Training Loss: 2.020104, Validation Loss: 2.019878\n",
      "Epoch : [2722/8999], Training Loss: 2.020131, Validation Loss: 2.020568\n",
      "Epoch : [2723/8999], Training Loss: 2.020087, Validation Loss: 2.019841\n",
      "Epoch : [2724/8999], Training Loss: 2.019878, Validation Loss: 2.020429\n",
      "Epoch : [2725/8999], Training Loss: 2.020009, Validation Loss: 2.019915\n",
      "Epoch : [2726/8999], Training Loss: 2.019968, Validation Loss: 2.020142\n",
      "Epoch : [2727/8999], Training Loss: 2.019958, Validation Loss: 2.019857\n",
      "Epoch : [2728/8999], Training Loss: 2.019908, Validation Loss: 2.020783\n",
      "Epoch : [2729/8999], Training Loss: 2.020068, Validation Loss: 2.020074\n",
      "Epoch : [2730/8999], Training Loss: 2.020195, Validation Loss: 2.020088\n",
      "Epoch : [2731/8999], Training Loss: 2.020148, Validation Loss: 2.020429\n",
      "Epoch : [2732/8999], Training Loss: 2.020035, Validation Loss: 2.020117\n",
      "Epoch : [2733/8999], Training Loss: 2.019913, Validation Loss: 2.020330\n",
      "Epoch : [2734/8999], Training Loss: 2.019895, Validation Loss: 2.019981\n",
      "Epoch : [2735/8999], Training Loss: 2.019853, Validation Loss: 2.020311\n",
      "Epoch : [2736/8999], Training Loss: 2.020153, Validation Loss: 2.020320\n",
      "Epoch : [2737/8999], Training Loss: 2.020030, Validation Loss: 2.020198\n",
      "Epoch : [2738/8999], Training Loss: 2.019920, Validation Loss: 2.020091\n",
      "Epoch : [2739/8999], Training Loss: 2.020051, Validation Loss: 2.020455\n",
      "Epoch : [2740/8999], Training Loss: 2.019829, Validation Loss: 2.020049\n",
      "Epoch : [2741/8999], Training Loss: 2.019831, Validation Loss: 2.020333\n",
      "Epoch : [2742/8999], Training Loss: 2.019923, Validation Loss: 2.020136\n",
      "Epoch : [2743/8999], Training Loss: 2.019838, Validation Loss: 2.020559\n",
      "Epoch : [2744/8999], Training Loss: 2.019971, Validation Loss: 2.020333\n",
      "Epoch : [2745/8999], Training Loss: 2.019968, Validation Loss: 2.020531\n",
      "Epoch : [2746/8999], Training Loss: 2.020093, Validation Loss: 2.019916\n",
      "Epoch : [2747/8999], Training Loss: 2.020103, Validation Loss: 2.020417\n",
      "Epoch : [2748/8999], Training Loss: 2.019880, Validation Loss: 2.020133\n",
      "Epoch : [2749/8999], Training Loss: 2.019905, Validation Loss: 2.020610\n",
      "Epoch : [2750/8999], Training Loss: 2.020112, Validation Loss: 2.020078\n",
      "Epoch : [2751/8999], Training Loss: 2.020232, Validation Loss: 2.020267\n",
      "Epoch : [2752/8999], Training Loss: 2.019961, Validation Loss: 2.020215\n",
      "Epoch : [2753/8999], Training Loss: 2.019986, Validation Loss: 2.020408\n",
      "Epoch : [2754/8999], Training Loss: 2.019966, Validation Loss: 2.019979\n",
      "Epoch : [2755/8999], Training Loss: 2.020046, Validation Loss: 2.020051\n",
      "Epoch : [2756/8999], Training Loss: 2.019929, Validation Loss: 2.020220\n",
      "Epoch : [2757/8999], Training Loss: 2.019944, Validation Loss: 2.020268\n",
      "Epoch : [2758/8999], Training Loss: 2.019849, Validation Loss: 2.020115\n",
      "Epoch : [2759/8999], Training Loss: 2.019894, Validation Loss: 2.020318\n",
      "Epoch : [2760/8999], Training Loss: 2.019934, Validation Loss: 2.020444\n",
      "Epoch : [2761/8999], Training Loss: 2.020030, Validation Loss: 2.020018\n",
      "Epoch : [2762/8999], Training Loss: 2.020011, Validation Loss: 2.020611\n",
      "Epoch : [2763/8999], Training Loss: 2.019925, Validation Loss: 2.020032\n",
      "Epoch : [2764/8999], Training Loss: 2.019897, Validation Loss: 2.020177\n",
      "Epoch : [2765/8999], Training Loss: 2.019897, Validation Loss: 2.020167\n",
      "Epoch : [2766/8999], Training Loss: 2.019904, Validation Loss: 2.020165\n",
      "Epoch : [2767/8999], Training Loss: 2.019992, Validation Loss: 2.020036\n",
      "Epoch : [2768/8999], Training Loss: 2.020200, Validation Loss: 2.020293\n",
      "Epoch : [2769/8999], Training Loss: 2.020091, Validation Loss: 2.019928\n",
      "Epoch : [2770/8999], Training Loss: 2.020157, Validation Loss: 2.020166\n",
      "Epoch : [2771/8999], Training Loss: 2.019942, Validation Loss: 2.020135\n",
      "Epoch : [2772/8999], Training Loss: 2.019904, Validation Loss: 2.019898\n",
      "Epoch : [2773/8999], Training Loss: 2.019844, Validation Loss: 2.019764\n",
      "Epoch : [2774/8999], Training Loss: 2.019955, Validation Loss: 2.020074\n",
      "Epoch : [2775/8999], Training Loss: 2.019976, Validation Loss: 2.019882\n",
      "Epoch : [2776/8999], Training Loss: 2.020051, Validation Loss: 2.020086\n",
      "Epoch : [2777/8999], Training Loss: 2.019987, Validation Loss: 2.019840\n",
      "Epoch : [2778/8999], Training Loss: 2.019913, Validation Loss: 2.019929\n",
      "Epoch : [2779/8999], Training Loss: 2.019840, Validation Loss: 2.019895\n",
      "Epoch : [2780/8999], Training Loss: 2.019916, Validation Loss: 2.019948\n",
      "Epoch : [2781/8999], Training Loss: 2.019891, Validation Loss: 2.019844\n",
      "Epoch : [2782/8999], Training Loss: 2.019872, Validation Loss: 2.019884\n",
      "Epoch : [2783/8999], Training Loss: 2.019887, Validation Loss: 2.019875\n",
      "Epoch : [2784/8999], Training Loss: 2.019844, Validation Loss: 2.020034\n",
      "Epoch : [2785/8999], Training Loss: 2.019997, Validation Loss: 2.019842\n",
      "Epoch : [2786/8999], Training Loss: 2.019965, Validation Loss: 2.019930\n",
      "Epoch : [2787/8999], Training Loss: 2.020051, Validation Loss: 2.019982\n",
      "Epoch : [2788/8999], Training Loss: 2.020049, Validation Loss: 2.019890\n",
      "Epoch : [2789/8999], Training Loss: 2.020052, Validation Loss: 2.019889\n",
      "Epoch : [2790/8999], Training Loss: 2.019945, Validation Loss: 2.019866\n",
      "Epoch : [2791/8999], Training Loss: 2.019872, Validation Loss: 2.019761\n",
      "Epoch : [2792/8999], Training Loss: 2.019844, Validation Loss: 2.019690\n",
      "Epoch : [2793/8999], Training Loss: 2.019705, Validation Loss: 2.019642\n",
      "Epoch : [2794/8999], Training Loss: 2.019766, Validation Loss: 2.019782\n",
      "Epoch : [2795/8999], Training Loss: 2.019968, Validation Loss: 2.019790\n",
      "Epoch : [2796/8999], Training Loss: 2.019855, Validation Loss: 2.020149\n",
      "Epoch : [2797/8999], Training Loss: 2.020128, Validation Loss: 2.019813\n",
      "Epoch : [2798/8999], Training Loss: 2.019897, Validation Loss: 2.019892\n",
      "Epoch : [2799/8999], Training Loss: 2.019851, Validation Loss: 2.019705\n",
      "Epoch : [2800/8999], Training Loss: 2.019932, Validation Loss: 2.019802\n",
      "Epoch : [2801/8999], Training Loss: 2.019807, Validation Loss: 2.019798\n",
      "Epoch : [2802/8999], Training Loss: 2.019937, Validation Loss: 2.019744\n",
      "Epoch : [2803/8999], Training Loss: 2.019697, Validation Loss: 2.019746\n",
      "Epoch : [2804/8999], Training Loss: 2.019742, Validation Loss: 2.019879\n",
      "Epoch : [2805/8999], Training Loss: 2.020277, Validation Loss: 2.019907\n",
      "Epoch : [2806/8999], Training Loss: 2.020157, Validation Loss: 2.020297\n",
      "Epoch : [2807/8999], Training Loss: 2.019899, Validation Loss: 2.020120\n",
      "Epoch : [2808/8999], Training Loss: 2.019913, Validation Loss: 2.019960\n",
      "Epoch : [2809/8999], Training Loss: 2.019819, Validation Loss: 2.019866\n",
      "Epoch : [2810/8999], Training Loss: 2.020035, Validation Loss: 2.020418\n",
      "Epoch : [2811/8999], Training Loss: 2.019804, Validation Loss: 2.019934\n",
      "Epoch : [2812/8999], Training Loss: 2.020069, Validation Loss: 2.020012\n",
      "Epoch : [2813/8999], Training Loss: 2.019829, Validation Loss: 2.019963\n",
      "Epoch : [2814/8999], Training Loss: 2.019862, Validation Loss: 2.020806\n",
      "Epoch : [2815/8999], Training Loss: 2.020493, Validation Loss: 2.020056\n",
      "Epoch : [2816/8999], Training Loss: 2.020192, Validation Loss: 2.020563\n",
      "Epoch : [2817/8999], Training Loss: 2.020030, Validation Loss: 2.020632\n",
      "Epoch : [2818/8999], Training Loss: 2.019998, Validation Loss: 2.020244\n",
      "Epoch : [2819/8999], Training Loss: 2.020010, Validation Loss: 2.020448\n",
      "Epoch : [2820/8999], Training Loss: 2.019901, Validation Loss: 2.020309\n",
      "Epoch : [2821/8999], Training Loss: 2.019875, Validation Loss: 2.020048\n",
      "Epoch : [2822/8999], Training Loss: 2.020127, Validation Loss: 2.020612\n",
      "Epoch : [2823/8999], Training Loss: 2.020016, Validation Loss: 2.020936\n",
      "Epoch : [2824/8999], Training Loss: 2.020559, Validation Loss: 2.020423\n",
      "Epoch : [2825/8999], Training Loss: 2.020000, Validation Loss: 2.020277\n",
      "Epoch : [2826/8999], Training Loss: 2.020011, Validation Loss: 2.020309\n",
      "Epoch : [2827/8999], Training Loss: 2.019992, Validation Loss: 2.021031\n",
      "Epoch : [2828/8999], Training Loss: 2.020130, Validation Loss: 2.020877\n",
      "Epoch : [2829/8999], Training Loss: 2.020053, Validation Loss: 2.020293\n",
      "Epoch : [2830/8999], Training Loss: 2.020035, Validation Loss: 2.020642\n",
      "Epoch : [2831/8999], Training Loss: 2.020156, Validation Loss: 2.021177\n",
      "Epoch : [2832/8999], Training Loss: 2.019949, Validation Loss: 2.020102\n",
      "Epoch : [2833/8999], Training Loss: 2.020520, Validation Loss: 2.020590\n",
      "Epoch : [2834/8999], Training Loss: 2.020199, Validation Loss: 2.020729\n",
      "Epoch : [2835/8999], Training Loss: 2.020142, Validation Loss: 2.020253\n",
      "Epoch : [2836/8999], Training Loss: 2.020155, Validation Loss: 2.020913\n",
      "Epoch : [2837/8999], Training Loss: 2.019951, Validation Loss: 2.020446\n",
      "Epoch : [2838/8999], Training Loss: 2.019986, Validation Loss: 2.020129\n",
      "Epoch : [2839/8999], Training Loss: 2.020116, Validation Loss: 2.020901\n",
      "Epoch : [2840/8999], Training Loss: 2.019915, Validation Loss: 2.020514\n",
      "Epoch : [2841/8999], Training Loss: 2.020018, Validation Loss: 2.020088\n",
      "Epoch : [2842/8999], Training Loss: 2.020204, Validation Loss: 2.020661\n",
      "Epoch : [2843/8999], Training Loss: 2.020052, Validation Loss: 2.020686\n",
      "Epoch : [2844/8999], Training Loss: 2.019940, Validation Loss: 2.020002\n",
      "Epoch : [2845/8999], Training Loss: 2.019999, Validation Loss: 2.020203\n",
      "Epoch : [2846/8999], Training Loss: 2.020006, Validation Loss: 2.020641\n",
      "Epoch : [2847/8999], Training Loss: 2.019874, Validation Loss: 2.020318\n",
      "Epoch : [2848/8999], Training Loss: 2.020030, Validation Loss: 2.020045\n",
      "Epoch : [2849/8999], Training Loss: 2.020047, Validation Loss: 2.020272\n",
      "Epoch : [2850/8999], Training Loss: 2.019975, Validation Loss: 2.020724\n",
      "Epoch : [2851/8999], Training Loss: 2.019805, Validation Loss: 2.020187\n",
      "Epoch : [2852/8999], Training Loss: 2.019928, Validation Loss: 2.019981\n",
      "Epoch : [2853/8999], Training Loss: 2.019936, Validation Loss: 2.020388\n",
      "Epoch : [2854/8999], Training Loss: 2.019970, Validation Loss: 2.020307\n",
      "Epoch : [2855/8999], Training Loss: 2.020013, Validation Loss: 2.020153\n",
      "Epoch : [2856/8999], Training Loss: 2.020202, Validation Loss: 2.020106\n",
      "Epoch : [2857/8999], Training Loss: 2.019984, Validation Loss: 2.020394\n",
      "Epoch : [2858/8999], Training Loss: 2.020163, Validation Loss: 2.020136\n",
      "Epoch : [2859/8999], Training Loss: 2.020215, Validation Loss: 2.020087\n",
      "Epoch : [2860/8999], Training Loss: 2.020293, Validation Loss: 2.020555\n",
      "Epoch : [2861/8999], Training Loss: 2.020108, Validation Loss: 2.020036\n",
      "Epoch : [2862/8999], Training Loss: 2.020277, Validation Loss: 2.019989\n",
      "Epoch : [2863/8999], Training Loss: 2.020208, Validation Loss: 2.020046\n",
      "Epoch : [2864/8999], Training Loss: 2.020003, Validation Loss: 2.020149\n",
      "Epoch : [2865/8999], Training Loss: 2.020386, Validation Loss: 2.020182\n",
      "Epoch : [2866/8999], Training Loss: 2.019986, Validation Loss: 2.019787\n",
      "Epoch : [2867/8999], Training Loss: 2.019887, Validation Loss: 2.019748\n",
      "Epoch : [2868/8999], Training Loss: 2.019993, Validation Loss: 2.020016\n",
      "Epoch : [2869/8999], Training Loss: 2.019867, Validation Loss: 2.019715\n",
      "Epoch : [2870/8999], Training Loss: 2.019794, Validation Loss: 2.019896\n",
      "Epoch : [2871/8999], Training Loss: 2.020118, Validation Loss: 2.020122\n",
      "Epoch : [2872/8999], Training Loss: 2.019943, Validation Loss: 2.019755\n",
      "Epoch : [2873/8999], Training Loss: 2.019993, Validation Loss: 2.020114\n",
      "Epoch : [2874/8999], Training Loss: 2.020589, Validation Loss: 2.020223\n",
      "Epoch : [2875/8999], Training Loss: 2.020111, Validation Loss: 2.019812\n",
      "Epoch : [2876/8999], Training Loss: 2.019859, Validation Loss: 2.019908\n",
      "Epoch : [2877/8999], Training Loss: 2.020155, Validation Loss: 2.019820\n",
      "Epoch : [2878/8999], Training Loss: 2.019902, Validation Loss: 2.019811\n",
      "Epoch : [2879/8999], Training Loss: 2.019980, Validation Loss: 2.020215\n",
      "Epoch : [2880/8999], Training Loss: 2.020268, Validation Loss: 2.020038\n",
      "Epoch : [2881/8999], Training Loss: 2.020091, Validation Loss: 2.019954\n",
      "Epoch : [2882/8999], Training Loss: 2.019960, Validation Loss: 2.019796\n",
      "Epoch : [2883/8999], Training Loss: 2.019814, Validation Loss: 2.019665\n",
      "Epoch : [2884/8999], Training Loss: 2.019699, Validation Loss: 2.019666\n",
      "Epoch : [2885/8999], Training Loss: 2.019746, Validation Loss: 2.019845\n",
      "Epoch : [2886/8999], Training Loss: 2.019804, Validation Loss: 2.019846\n",
      "Epoch : [2887/8999], Training Loss: 2.019744, Validation Loss: 2.019781\n",
      "Epoch : [2888/8999], Training Loss: 2.019707, Validation Loss: 2.019865\n",
      "Epoch : [2889/8999], Training Loss: 2.019949, Validation Loss: 2.019986\n",
      "Epoch : [2890/8999], Training Loss: 2.019968, Validation Loss: 2.020377\n",
      "Epoch : [2891/8999], Training Loss: 2.020077, Validation Loss: 2.019773\n",
      "Epoch : [2892/8999], Training Loss: 2.019745, Validation Loss: 2.019820\n",
      "Epoch : [2893/8999], Training Loss: 2.019807, Validation Loss: 2.020262\n",
      "Epoch : [2894/8999], Training Loss: 2.019806, Validation Loss: 2.019818\n",
      "Epoch : [2895/8999], Training Loss: 2.019864, Validation Loss: 2.019761\n",
      "Epoch : [2896/8999], Training Loss: 2.019822, Validation Loss: 2.020061\n",
      "Epoch : [2897/8999], Training Loss: 2.019805, Validation Loss: 2.019861\n",
      "Epoch : [2898/8999], Training Loss: 2.019639, Validation Loss: 2.019887\n",
      "Epoch : [2899/8999], Training Loss: 2.019735, Validation Loss: 2.019611\n",
      "Epoch : [2900/8999], Training Loss: 2.019844, Validation Loss: 2.020582\n",
      "Epoch : [2901/8999], Training Loss: 2.019841, Validation Loss: 2.020126\n",
      "Epoch : [2902/8999], Training Loss: 2.019828, Validation Loss: 2.019757\n",
      "Epoch : [2903/8999], Training Loss: 2.019729, Validation Loss: 2.019839\n",
      "Epoch : [2904/8999], Training Loss: 2.019816, Validation Loss: 2.020588\n",
      "Epoch : [2905/8999], Training Loss: 2.019985, Validation Loss: 2.020047\n",
      "Epoch : [2906/8999], Training Loss: 2.019830, Validation Loss: 2.019743\n",
      "Epoch : [2907/8999], Training Loss: 2.019716, Validation Loss: 2.020551\n",
      "Epoch : [2908/8999], Training Loss: 2.019805, Validation Loss: 2.019920\n",
      "Epoch : [2909/8999], Training Loss: 2.020234, Validation Loss: 2.019990\n",
      "Epoch : [2910/8999], Training Loss: 2.020172, Validation Loss: 2.020384\n",
      "Epoch : [2911/8999], Training Loss: 2.019974, Validation Loss: 2.020273\n",
      "Epoch : [2912/8999], Training Loss: 2.019823, Validation Loss: 2.019939\n",
      "Epoch : [2913/8999], Training Loss: 2.019928, Validation Loss: 2.021181\n",
      "Epoch : [2914/8999], Training Loss: 2.020165, Validation Loss: 2.019975\n",
      "Epoch : [2915/8999], Training Loss: 2.019820, Validation Loss: 2.019868\n",
      "Epoch : [2916/8999], Training Loss: 2.019758, Validation Loss: 2.020409\n",
      "Epoch : [2917/8999], Training Loss: 2.019731, Validation Loss: 2.020392\n",
      "Epoch : [2918/8999], Training Loss: 2.019856, Validation Loss: 2.019973\n",
      "Epoch : [2919/8999], Training Loss: 2.019904, Validation Loss: 2.019861\n",
      "Epoch : [2920/8999], Training Loss: 2.019757, Validation Loss: 2.020895\n",
      "Epoch : [2921/8999], Training Loss: 2.019804, Validation Loss: 2.020115\n",
      "Epoch : [2922/8999], Training Loss: 2.019656, Validation Loss: 2.019938\n",
      "Epoch : [2923/8999], Training Loss: 2.019957, Validation Loss: 2.020811\n",
      "Epoch : [2924/8999], Training Loss: 2.019928, Validation Loss: 2.020103\n",
      "Epoch : [2925/8999], Training Loss: 2.019809, Validation Loss: 2.020114\n",
      "Epoch : [2926/8999], Training Loss: 2.019699, Validation Loss: 2.020252\n",
      "Epoch : [2927/8999], Training Loss: 2.019759, Validation Loss: 2.020465\n",
      "Epoch : [2928/8999], Training Loss: 2.019748, Validation Loss: 2.019913\n",
      "Epoch : [2929/8999], Training Loss: 2.019796, Validation Loss: 2.020348\n",
      "Epoch : [2930/8999], Training Loss: 2.019790, Validation Loss: 2.020210\n",
      "Epoch : [2931/8999], Training Loss: 2.019698, Validation Loss: 2.020344\n",
      "Epoch : [2932/8999], Training Loss: 2.019707, Validation Loss: 2.020484\n",
      "Epoch : [2933/8999], Training Loss: 2.019984, Validation Loss: 2.020020\n",
      "Epoch : [2934/8999], Training Loss: 2.019968, Validation Loss: 2.021022\n",
      "Epoch : [2935/8999], Training Loss: 2.019933, Validation Loss: 2.020178\n",
      "Epoch : [2936/8999], Training Loss: 2.019787, Validation Loss: 2.020148\n",
      "Epoch : [2937/8999], Training Loss: 2.019810, Validation Loss: 2.019833\n",
      "Epoch : [2938/8999], Training Loss: 2.019869, Validation Loss: 2.020767\n",
      "Epoch : [2939/8999], Training Loss: 2.019826, Validation Loss: 2.020114\n",
      "Epoch : [2940/8999], Training Loss: 2.019712, Validation Loss: 2.019869\n",
      "Epoch : [2941/8999], Training Loss: 2.020076, Validation Loss: 2.020604\n",
      "Epoch : [2942/8999], Training Loss: 2.019782, Validation Loss: 2.020456\n",
      "Epoch : [2943/8999], Training Loss: 2.019702, Validation Loss: 2.020421\n",
      "Epoch : [2944/8999], Training Loss: 2.019733, Validation Loss: 2.019940\n",
      "Epoch : [2945/8999], Training Loss: 2.019743, Validation Loss: 2.020196\n",
      "Epoch : [2946/8999], Training Loss: 2.019609, Validation Loss: 2.019954\n",
      "Epoch : [2947/8999], Training Loss: 2.019664, Validation Loss: 2.020060\n",
      "Epoch : [2948/8999], Training Loss: 2.019620, Validation Loss: 2.020222\n",
      "Epoch : [2949/8999], Training Loss: 2.019828, Validation Loss: 2.020255\n",
      "Epoch : [2950/8999], Training Loss: 2.019729, Validation Loss: 2.020041\n",
      "Epoch : [2951/8999], Training Loss: 2.019699, Validation Loss: 2.019935\n",
      "Epoch : [2952/8999], Training Loss: 2.019700, Validation Loss: 2.020348\n",
      "Epoch : [2953/8999], Training Loss: 2.019760, Validation Loss: 2.020143\n",
      "Epoch : [2954/8999], Training Loss: 2.019669, Validation Loss: 2.019971\n",
      "Epoch : [2955/8999], Training Loss: 2.019758, Validation Loss: 2.019965\n",
      "Epoch : [2956/8999], Training Loss: 2.019583, Validation Loss: 2.020328\n",
      "Epoch : [2957/8999], Training Loss: 2.019738, Validation Loss: 2.019997\n",
      "Epoch : [2958/8999], Training Loss: 2.019768, Validation Loss: 2.019769\n",
      "Epoch : [2959/8999], Training Loss: 2.020019, Validation Loss: 2.020164\n",
      "Epoch : [2960/8999], Training Loss: 2.019747, Validation Loss: 2.020006\n",
      "Epoch : [2961/8999], Training Loss: 2.019666, Validation Loss: 2.020109\n",
      "Epoch : [2962/8999], Training Loss: 2.019566, Validation Loss: 2.020088\n",
      "Epoch : [2963/8999], Training Loss: 2.019712, Validation Loss: 2.019858\n",
      "Epoch : [2964/8999], Training Loss: 2.019544, Validation Loss: 2.019999\n",
      "Epoch : [2965/8999], Training Loss: 2.019624, Validation Loss: 2.019825\n",
      "Epoch : [2966/8999], Training Loss: 2.019650, Validation Loss: 2.020027\n",
      "Epoch : [2967/8999], Training Loss: 2.019665, Validation Loss: 2.019922\n",
      "Epoch : [2968/8999], Training Loss: 2.019727, Validation Loss: 2.020078\n",
      "Epoch : [2969/8999], Training Loss: 2.019792, Validation Loss: 2.019756\n",
      "Epoch : [2970/8999], Training Loss: 2.019659, Validation Loss: 2.019637\n",
      "Epoch : [2971/8999], Training Loss: 2.019728, Validation Loss: 2.020041\n",
      "Epoch : [2972/8999], Training Loss: 2.019864, Validation Loss: 2.019750\n",
      "Epoch : [2973/8999], Training Loss: 2.019693, Validation Loss: 2.019880\n",
      "Epoch : [2974/8999], Training Loss: 2.019820, Validation Loss: 2.019784\n",
      "Epoch : [2975/8999], Training Loss: 2.019787, Validation Loss: 2.020132\n",
      "Epoch : [2976/8999], Training Loss: 2.019915, Validation Loss: 2.019768\n",
      "Epoch : [2977/8999], Training Loss: 2.019695, Validation Loss: 2.019745\n",
      "Epoch : [2978/8999], Training Loss: 2.019661, Validation Loss: 2.019607\n",
      "Epoch : [2979/8999], Training Loss: 2.019607, Validation Loss: 2.019679\n",
      "Epoch : [2980/8999], Training Loss: 2.019947, Validation Loss: 2.019924\n",
      "Epoch : [2981/8999], Training Loss: 2.019733, Validation Loss: 2.019831\n",
      "Epoch : [2982/8999], Training Loss: 2.019819, Validation Loss: 2.019778\n",
      "Epoch : [2983/8999], Training Loss: 2.019826, Validation Loss: 2.019737\n",
      "Epoch : [2984/8999], Training Loss: 2.019690, Validation Loss: 2.019793\n",
      "Epoch : [2985/8999], Training Loss: 2.019780, Validation Loss: 2.019766\n",
      "Epoch : [2986/8999], Training Loss: 2.019637, Validation Loss: 2.019869\n",
      "Epoch : [2987/8999], Training Loss: 2.019727, Validation Loss: 2.019598\n",
      "Epoch : [2988/8999], Training Loss: 2.019512, Validation Loss: 2.019555\n",
      "Epoch : [2989/8999], Training Loss: 2.019582, Validation Loss: 2.019724\n",
      "Epoch : [2990/8999], Training Loss: 2.019712, Validation Loss: 2.019763\n",
      "Epoch : [2991/8999], Training Loss: 2.019638, Validation Loss: 2.019886\n",
      "Epoch : [2992/8999], Training Loss: 2.019599, Validation Loss: 2.019772\n",
      "Epoch : [2993/8999], Training Loss: 2.019789, Validation Loss: 2.019795\n",
      "Epoch : [2994/8999], Training Loss: 2.019830, Validation Loss: 2.019706\n",
      "Epoch : [2995/8999], Training Loss: 2.019673, Validation Loss: 2.019887\n",
      "Epoch : [2996/8999], Training Loss: 2.019840, Validation Loss: 2.019693\n",
      "Epoch : [2997/8999], Training Loss: 2.019631, Validation Loss: 2.019803\n",
      "Epoch : [2998/8999], Training Loss: 2.019738, Validation Loss: 2.019599\n",
      "Epoch : [2999/8999], Training Loss: 2.019604, Validation Loss: 2.019784\n",
      "Epoch : [3000/8999], Training Loss: 2.019557, Validation Loss: 2.019747\n",
      "Epoch : [3001/8999], Training Loss: 2.019547, Validation Loss: 2.019807\n",
      "Epoch : [3002/8999], Training Loss: 2.019789, Validation Loss: 2.019655\n",
      "Epoch : [3003/8999], Training Loss: 2.019850, Validation Loss: 2.019944\n",
      "Epoch : [3004/8999], Training Loss: 2.019718, Validation Loss: 2.020080\n",
      "Epoch : [3005/8999], Training Loss: 2.019677, Validation Loss: 2.019823\n",
      "Epoch : [3006/8999], Training Loss: 2.019978, Validation Loss: 2.020101\n",
      "Epoch : [3007/8999], Training Loss: 2.019828, Validation Loss: 2.020366\n",
      "Epoch : [3008/8999], Training Loss: 2.019760, Validation Loss: 2.019867\n",
      "Epoch : [3009/8999], Training Loss: 2.019567, Validation Loss: 2.019939\n",
      "Epoch : [3010/8999], Training Loss: 2.019553, Validation Loss: 2.019991\n",
      "Epoch : [3011/8999], Training Loss: 2.019750, Validation Loss: 2.019808\n",
      "Epoch : [3012/8999], Training Loss: 2.019795, Validation Loss: 2.020175\n",
      "Epoch : [3013/8999], Training Loss: 2.019627, Validation Loss: 2.020738\n",
      "Epoch : [3014/8999], Training Loss: 2.019944, Validation Loss: 2.019857\n",
      "Epoch : [3015/8999], Training Loss: 2.019904, Validation Loss: 2.020143\n",
      "Epoch : [3016/8999], Training Loss: 2.019629, Validation Loss: 2.020362\n",
      "Epoch : [3017/8999], Training Loss: 2.019704, Validation Loss: 2.019944\n",
      "Epoch : [3018/8999], Training Loss: 2.019853, Validation Loss: 2.019912\n",
      "Epoch : [3019/8999], Training Loss: 2.019825, Validation Loss: 2.020608\n",
      "Epoch : [3020/8999], Training Loss: 2.019787, Validation Loss: 2.020472\n",
      "Epoch : [3021/8999], Training Loss: 2.019798, Validation Loss: 2.020020\n",
      "Epoch : [3022/8999], Training Loss: 2.019822, Validation Loss: 2.020637\n",
      "Epoch : [3023/8999], Training Loss: 2.019826, Validation Loss: 2.020319\n",
      "Epoch : [3024/8999], Training Loss: 2.019633, Validation Loss: 2.020574\n",
      "Epoch : [3025/8999], Training Loss: 2.019642, Validation Loss: 2.019968\n",
      "Epoch : [3026/8999], Training Loss: 2.019786, Validation Loss: 2.020203\n",
      "Epoch : [3027/8999], Training Loss: 2.020004, Validation Loss: 2.021143\n",
      "Epoch : [3028/8999], Training Loss: 2.019797, Validation Loss: 2.020823\n",
      "Epoch : [3029/8999], Training Loss: 2.019898, Validation Loss: 2.020316\n",
      "Epoch : [3030/8999], Training Loss: 2.019953, Validation Loss: 2.020628\n",
      "Epoch : [3031/8999], Training Loss: 2.020016, Validation Loss: 2.021532\n",
      "Epoch : [3032/8999], Training Loss: 2.020171, Validation Loss: 2.020427\n",
      "Epoch : [3033/8999], Training Loss: 2.021272, Validation Loss: 2.021371\n",
      "Epoch : [3034/8999], Training Loss: 2.020269, Validation Loss: 2.020744\n",
      "Epoch : [3035/8999], Training Loss: 2.020368, Validation Loss: 2.020818\n",
      "Epoch : [3036/8999], Training Loss: 2.020056, Validation Loss: 2.021084\n",
      "Epoch : [3037/8999], Training Loss: 2.020250, Validation Loss: 2.020595\n",
      "Epoch : [3038/8999], Training Loss: 2.020275, Validation Loss: 2.021086\n",
      "Epoch : [3039/8999], Training Loss: 2.019987, Validation Loss: 2.020034\n",
      "Epoch : [3040/8999], Training Loss: 2.020546, Validation Loss: 2.021100\n",
      "Epoch : [3041/8999], Training Loss: 2.020072, Validation Loss: 2.020940\n",
      "Epoch : [3042/8999], Training Loss: 2.020458, Validation Loss: 2.020468\n",
      "Epoch : [3043/8999], Training Loss: 2.020168, Validation Loss: 2.020829\n",
      "Epoch : [3044/8999], Training Loss: 2.019927, Validation Loss: 2.020134\n",
      "Epoch : [3045/8999], Training Loss: 2.020276, Validation Loss: 2.020380\n",
      "Epoch : [3046/8999], Training Loss: 2.020172, Validation Loss: 2.021041\n",
      "Epoch : [3047/8999], Training Loss: 2.020077, Validation Loss: 2.020893\n",
      "Epoch : [3048/8999], Training Loss: 2.019939, Validation Loss: 2.020294\n",
      "Epoch : [3049/8999], Training Loss: 2.020039, Validation Loss: 2.020497\n",
      "Epoch : [3050/8999], Training Loss: 2.020128, Validation Loss: 2.020932\n",
      "Epoch : [3051/8999], Training Loss: 2.020179, Validation Loss: 2.020267\n",
      "Epoch : [3052/8999], Training Loss: 2.019900, Validation Loss: 2.020186\n",
      "Epoch : [3053/8999], Training Loss: 2.020153, Validation Loss: 2.020681\n",
      "Epoch : [3054/8999], Training Loss: 2.020022, Validation Loss: 2.020375\n",
      "Epoch : [3055/8999], Training Loss: 2.019940, Validation Loss: 2.020056\n",
      "Epoch : [3056/8999], Training Loss: 2.020097, Validation Loss: 2.020163\n",
      "Epoch : [3057/8999], Training Loss: 2.020137, Validation Loss: 2.020638\n",
      "Epoch : [3058/8999], Training Loss: 2.019902, Validation Loss: 2.019989\n",
      "Epoch : [3059/8999], Training Loss: 2.020006, Validation Loss: 2.020298\n",
      "Epoch : [3060/8999], Training Loss: 2.019857, Validation Loss: 2.020241\n",
      "Epoch : [3061/8999], Training Loss: 2.019779, Validation Loss: 2.019958\n",
      "Epoch : [3062/8999], Training Loss: 2.019870, Validation Loss: 2.019992\n",
      "Epoch : [3063/8999], Training Loss: 2.019871, Validation Loss: 2.020076\n",
      "Epoch : [3064/8999], Training Loss: 2.019937, Validation Loss: 2.020422\n",
      "Epoch : [3065/8999], Training Loss: 2.019973, Validation Loss: 2.020254\n",
      "Epoch : [3066/8999], Training Loss: 2.019931, Validation Loss: 2.019922\n",
      "Epoch : [3067/8999], Training Loss: 2.019926, Validation Loss: 2.020098\n",
      "Epoch : [3068/8999], Training Loss: 2.019847, Validation Loss: 2.020101\n",
      "Epoch : [3069/8999], Training Loss: 2.019912, Validation Loss: 2.020049\n",
      "Epoch : [3070/8999], Training Loss: 2.019711, Validation Loss: 2.019967\n",
      "Epoch : [3071/8999], Training Loss: 2.019879, Validation Loss: 2.019826\n",
      "Epoch : [3072/8999], Training Loss: 2.019884, Validation Loss: 2.019976\n",
      "Epoch : [3073/8999], Training Loss: 2.019985, Validation Loss: 2.019878\n",
      "Epoch : [3074/8999], Training Loss: 2.019843, Validation Loss: 2.019971\n",
      "Epoch : [3075/8999], Training Loss: 2.019917, Validation Loss: 2.020132\n",
      "Epoch : [3076/8999], Training Loss: 2.019923, Validation Loss: 2.019992\n",
      "Epoch : [3077/8999], Training Loss: 2.019854, Validation Loss: 2.019767\n",
      "Epoch : [3078/8999], Training Loss: 2.019892, Validation Loss: 2.020143\n",
      "Epoch : [3079/8999], Training Loss: 2.020095, Validation Loss: 2.020064\n",
      "Epoch : [3080/8999], Training Loss: 2.019835, Validation Loss: 2.020155\n",
      "Epoch : [3081/8999], Training Loss: 2.020007, Validation Loss: 2.020227\n",
      "Epoch : [3082/8999], Training Loss: 2.020069, Validation Loss: 2.019962\n",
      "Epoch : [3083/8999], Training Loss: 2.020055, Validation Loss: 2.020172\n",
      "Epoch : [3084/8999], Training Loss: 2.019703, Validation Loss: 2.020145\n",
      "Epoch : [3085/8999], Training Loss: 2.019974, Validation Loss: 2.020142\n",
      "Epoch : [3086/8999], Training Loss: 2.020124, Validation Loss: 2.020393\n",
      "Epoch : [3087/8999], Training Loss: 2.019975, Validation Loss: 2.019892\n",
      "Epoch : [3088/8999], Training Loss: 2.019863, Validation Loss: 2.019943\n",
      "Epoch : [3089/8999], Training Loss: 2.019804, Validation Loss: 2.020348\n",
      "Epoch : [3090/8999], Training Loss: 2.019895, Validation Loss: 2.020408\n",
      "Epoch : [3091/8999], Training Loss: 2.020005, Validation Loss: 2.019981\n",
      "Epoch : [3092/8999], Training Loss: 2.019781, Validation Loss: 2.020729\n",
      "Epoch : [3093/8999], Training Loss: 2.020037, Validation Loss: 2.019876\n",
      "Epoch : [3094/8999], Training Loss: 2.019863, Validation Loss: 2.020218\n",
      "Epoch : [3095/8999], Training Loss: 2.019728, Validation Loss: 2.020652\n",
      "Epoch : [3096/8999], Training Loss: 2.019851, Validation Loss: 2.020109\n",
      "Epoch : [3097/8999], Training Loss: 2.019651, Validation Loss: 2.019996\n",
      "Epoch : [3098/8999], Training Loss: 2.019877, Validation Loss: 2.020942\n",
      "Epoch : [3099/8999], Training Loss: 2.019990, Validation Loss: 2.020943\n",
      "Epoch : [3100/8999], Training Loss: 2.019826, Validation Loss: 2.021123\n",
      "Epoch : [3101/8999], Training Loss: 2.019869, Validation Loss: 2.020186\n",
      "Epoch : [3102/8999], Training Loss: 2.019956, Validation Loss: 2.020949\n",
      "Epoch : [3103/8999], Training Loss: 2.019947, Validation Loss: 2.020601\n",
      "Epoch : [3104/8999], Training Loss: 2.019908, Validation Loss: 2.020354\n",
      "Epoch : [3105/8999], Training Loss: 2.019907, Validation Loss: 2.021157\n",
      "Epoch : [3106/8999], Training Loss: 2.019893, Validation Loss: 2.020855\n",
      "Epoch : [3107/8999], Training Loss: 2.019798, Validation Loss: 2.020512\n",
      "Epoch : [3108/8999], Training Loss: 2.019739, Validation Loss: 2.020620\n",
      "Epoch : [3109/8999], Training Loss: 2.019866, Validation Loss: 2.020570\n",
      "Epoch : [3110/8999], Training Loss: 2.019714, Validation Loss: 2.020650\n",
      "Epoch : [3111/8999], Training Loss: 2.019696, Validation Loss: 2.020321\n",
      "Epoch : [3112/8999], Training Loss: 2.019795, Validation Loss: 2.020857\n",
      "Epoch : [3113/8999], Training Loss: 2.019995, Validation Loss: 2.020958\n",
      "Epoch : [3114/8999], Training Loss: 2.020135, Validation Loss: 2.021309\n",
      "Epoch : [3115/8999], Training Loss: 2.020077, Validation Loss: 2.020457\n",
      "Epoch : [3116/8999], Training Loss: 2.020031, Validation Loss: 2.021026\n",
      "Epoch : [3117/8999], Training Loss: 2.020022, Validation Loss: 2.020971\n",
      "Epoch : [3118/8999], Training Loss: 2.020021, Validation Loss: 2.020243\n",
      "Epoch : [3119/8999], Training Loss: 2.019791, Validation Loss: 2.020811\n",
      "Epoch : [3120/8999], Training Loss: 2.020016, Validation Loss: 2.021439\n",
      "Epoch : [3121/8999], Training Loss: 2.020112, Validation Loss: 2.020495\n",
      "Epoch : [3122/8999], Training Loss: 2.020009, Validation Loss: 2.020423\n",
      "Epoch : [3123/8999], Training Loss: 2.019748, Validation Loss: 2.020930\n",
      "Epoch : [3124/8999], Training Loss: 2.019716, Validation Loss: 2.020626\n",
      "Epoch : [3125/8999], Training Loss: 2.019787, Validation Loss: 2.020230\n",
      "Epoch : [3126/8999], Training Loss: 2.019731, Validation Loss: 2.020008\n",
      "Epoch : [3127/8999], Training Loss: 2.060471, Validation Loss: 2.054278\n",
      "Epoch : [3128/8999], Training Loss: 2.043030, Validation Loss: 2.047056\n",
      "Epoch : [3129/8999], Training Loss: 2.047742, Validation Loss: 2.050360\n",
      "Epoch : [3130/8999], Training Loss: 2.039392, Validation Loss: 2.040762\n",
      "Epoch : [3131/8999], Training Loss: 2.050779, Validation Loss: 2.042299\n",
      "Epoch : [3132/8999], Training Loss: 2.049476, Validation Loss: 2.049654\n",
      "Epoch : [3133/8999], Training Loss: 2.048140, Validation Loss: 2.051093\n",
      "Epoch : [3134/8999], Training Loss: 2.050465, Validation Loss: 2.042648\n",
      "Epoch : [3135/8999], Training Loss: 2.062432, Validation Loss: 2.056898\n",
      "Epoch : [3136/8999], Training Loss: 2.053765, Validation Loss: 2.041526\n",
      "Epoch : [3137/8999], Training Loss: 2.039322, Validation Loss: 2.034806\n",
      "Epoch : [3138/8999], Training Loss: 2.043468, Validation Loss: 2.046384\n",
      "Epoch : [3139/8999], Training Loss: 2.043407, Validation Loss: 2.033097\n",
      "Epoch : [3140/8999], Training Loss: 2.040551, Validation Loss: 2.036763\n",
      "Epoch : [3141/8999], Training Loss: 2.037626, Validation Loss: 2.029719\n",
      "Epoch : [3142/8999], Training Loss: 2.029943, Validation Loss: 2.023220\n",
      "Epoch : [3143/8999], Training Loss: 2.029442, Validation Loss: 2.045061\n",
      "Epoch : [3144/8999], Training Loss: 2.039380, Validation Loss: 2.036601\n",
      "Epoch : [3145/8999], Training Loss: 2.040640, Validation Loss: 2.028248\n",
      "Epoch : [3146/8999], Training Loss: 2.032082, Validation Loss: 2.026459\n",
      "Epoch : [3147/8999], Training Loss: 2.029586, Validation Loss: 2.025743\n",
      "Epoch : [3148/8999], Training Loss: 2.027401, Validation Loss: 2.038266\n",
      "Epoch : [3149/8999], Training Loss: 2.038610, Validation Loss: 2.049035\n",
      "Epoch : [3150/8999], Training Loss: 2.051111, Validation Loss: 2.031154\n",
      "Epoch : [3151/8999], Training Loss: 2.031200, Validation Loss: 2.040213\n",
      "Epoch : [3152/8999], Training Loss: 2.039158, Validation Loss: 2.043174\n",
      "Epoch : [3153/8999], Training Loss: 2.037555, Validation Loss: 2.033563\n",
      "Epoch : [3154/8999], Training Loss: 2.054108, Validation Loss: 2.055916\n",
      "Epoch : [3155/8999], Training Loss: 2.045607, Validation Loss: 2.039151\n",
      "Epoch : [3156/8999], Training Loss: 2.028509, Validation Loss: 2.028674\n",
      "Epoch : [3157/8999], Training Loss: 2.037342, Validation Loss: 2.037349\n",
      "Epoch : [3158/8999], Training Loss: 2.041686, Validation Loss: 2.040760\n",
      "Epoch : [3159/8999], Training Loss: 2.039043, Validation Loss: 2.035867\n",
      "Epoch : [3160/8999], Training Loss: 2.047507, Validation Loss: 2.050595\n",
      "Epoch : [3161/8999], Training Loss: 2.043832, Validation Loss: 2.028059\n",
      "Epoch : [3162/8999], Training Loss: 2.029492, Validation Loss: 2.050267\n",
      "Epoch : [3163/8999], Training Loss: 2.067561, Validation Loss: 2.069377\n",
      "Epoch : [3164/8999], Training Loss: 2.069516, Validation Loss: 2.057449\n",
      "Epoch : [3165/8999], Training Loss: 2.066915, Validation Loss: 2.078285\n",
      "Epoch : [3166/8999], Training Loss: 2.066356, Validation Loss: 2.059993\n",
      "Epoch : [3167/8999], Training Loss: 2.050991, Validation Loss: 2.041672\n",
      "Epoch : [3168/8999], Training Loss: 2.041413, Validation Loss: 2.029806\n",
      "Epoch : [3169/8999], Training Loss: 2.031450, Validation Loss: 2.030526\n",
      "Epoch : [3170/8999], Training Loss: 2.035791, Validation Loss: 2.041456\n",
      "Epoch : [3171/8999], Training Loss: 2.042843, Validation Loss: 2.050582\n",
      "Epoch : [3172/8999], Training Loss: 2.042368, Validation Loss: 2.048961\n",
      "Epoch : [3173/8999], Training Loss: 2.043293, Validation Loss: 2.026389\n",
      "Epoch : [3174/8999], Training Loss: 2.031461, Validation Loss: 2.046336\n",
      "Epoch : [3175/8999], Training Loss: 2.041140, Validation Loss: 2.029199\n",
      "Epoch : [3176/8999], Training Loss: 2.031424, Validation Loss: 2.024870\n",
      "Epoch : [3177/8999], Training Loss: 2.026118, Validation Loss: 2.025138\n",
      "Epoch : [3178/8999], Training Loss: 2.025042, Validation Loss: 2.023024\n",
      "Epoch : [3179/8999], Training Loss: 2.024605, Validation Loss: 2.035352\n",
      "Epoch : [3180/8999], Training Loss: 2.040066, Validation Loss: 2.035777\n",
      "Epoch : [3181/8999], Training Loss: 2.036001, Validation Loss: 2.044551\n",
      "Epoch : [3182/8999], Training Loss: 2.042995, Validation Loss: 2.048383\n",
      "Epoch : [3183/8999], Training Loss: 2.039788, Validation Loss: 2.025918\n",
      "Epoch : [3184/8999], Training Loss: 2.034152, Validation Loss: 2.045701\n",
      "Epoch : [3185/8999], Training Loss: 2.032907, Validation Loss: 2.029203\n",
      "Epoch : [3186/8999], Training Loss: 2.036791, Validation Loss: 2.054362\n",
      "Epoch : [3187/8999], Training Loss: 2.052443, Validation Loss: 2.034561\n",
      "Epoch : [3188/8999], Training Loss: 2.029182, Validation Loss: 2.030722\n",
      "Epoch : [3189/8999], Training Loss: 2.038294, Validation Loss: 2.030753\n",
      "Epoch : [3190/8999], Training Loss: 2.037560, Validation Loss: 2.042772\n",
      "Epoch : [3191/8999], Training Loss: 2.046942, Validation Loss: 2.032985\n",
      "Epoch : [3192/8999], Training Loss: 2.034186, Validation Loss: 2.035672\n",
      "Epoch : [3193/8999], Training Loss: 2.029488, Validation Loss: 2.043201\n",
      "Epoch : [3194/8999], Training Loss: 2.034135, Validation Loss: 2.045196\n",
      "Epoch : [3195/8999], Training Loss: 2.034059, Validation Loss: 2.027795\n",
      "Epoch : [3196/8999], Training Loss: 2.029060, Validation Loss: 2.025001\n",
      "Epoch : [3197/8999], Training Loss: 2.024672, Validation Loss: 2.021970\n",
      "Epoch : [3198/8999], Training Loss: 2.022209, Validation Loss: 2.027244\n",
      "Epoch : [3199/8999], Training Loss: 2.025937, Validation Loss: 2.021142\n",
      "Epoch : [3200/8999], Training Loss: 2.021794, Validation Loss: 2.021371\n",
      "Epoch : [3201/8999], Training Loss: 2.026328, Validation Loss: 2.027838\n",
      "Epoch : [3202/8999], Training Loss: 2.028694, Validation Loss: 2.021251\n",
      "Epoch : [3203/8999], Training Loss: 2.022127, Validation Loss: 2.021175\n",
      "Epoch : [3204/8999], Training Loss: 2.027977, Validation Loss: 2.030636\n",
      "Epoch : [3205/8999], Training Loss: 2.030611, Validation Loss: 2.040874\n",
      "Epoch : [3206/8999], Training Loss: 2.043056, Validation Loss: 2.048633\n",
      "Epoch : [3207/8999], Training Loss: 2.038551, Validation Loss: 2.032975\n",
      "Epoch : [3208/8999], Training Loss: 2.034047, Validation Loss: 2.032313\n",
      "Epoch : [3209/8999], Training Loss: 2.026286, Validation Loss: 2.022056\n",
      "Epoch : [3210/8999], Training Loss: 2.024559, Validation Loss: 2.022798\n",
      "Epoch : [3211/8999], Training Loss: 2.023778, Validation Loss: 2.021070\n",
      "Epoch : [3212/8999], Training Loss: 2.022043, Validation Loss: 2.024573\n",
      "Epoch : [3213/8999], Training Loss: 2.033725, Validation Loss: 2.031852\n",
      "Epoch : [3214/8999], Training Loss: 2.031226, Validation Loss: 2.027867\n",
      "Epoch : [3215/8999], Training Loss: 2.028905, Validation Loss: 2.021563\n",
      "Epoch : [3216/8999], Training Loss: 2.023778, Validation Loss: 2.024326\n",
      "Epoch : [3217/8999], Training Loss: 2.022876, Validation Loss: 2.021249\n",
      "Epoch : [3218/8999], Training Loss: 2.019876, Validation Loss: 2.020042\n",
      "Epoch : [3219/8999], Training Loss: 2.019166, Validation Loss: 2.018885\n",
      "Epoch : [3220/8999], Training Loss: 2.018947, Validation Loss: 2.019204\n",
      "Epoch : [3221/8999], Training Loss: 2.019162, Validation Loss: 2.018962\n",
      "Epoch : [3222/8999], Training Loss: 2.018849, Validation Loss: 2.018251\n",
      "Epoch : [3223/8999], Training Loss: 2.018407, Validation Loss: 2.018452\n",
      "Epoch : [3224/8999], Training Loss: 2.018315, Validation Loss: 2.018408\n",
      "Epoch : [3225/8999], Training Loss: 2.018333, Validation Loss: 2.018276\n",
      "Epoch : [3226/8999], Training Loss: 2.018228, Validation Loss: 2.018142\n",
      "Epoch : [3227/8999], Training Loss: 2.018130, Validation Loss: 2.018133\n",
      "Epoch : [3228/8999], Training Loss: 2.018151, Validation Loss: 2.018127\n",
      "Epoch : [3229/8999], Training Loss: 2.018034, Validation Loss: 2.017969\n",
      "Epoch : [3230/8999], Training Loss: 2.017969, Validation Loss: 2.017897\n",
      "Epoch : [3231/8999], Training Loss: 2.017925, Validation Loss: 2.018128\n",
      "Epoch : [3232/8999], Training Loss: 2.018001, Validation Loss: 2.017958\n",
      "Epoch : [3233/8999], Training Loss: 2.017853, Validation Loss: 2.018030\n",
      "Epoch : [3234/8999], Training Loss: 2.017787, Validation Loss: 2.018511\n",
      "Epoch : [3235/8999], Training Loss: 2.018127, Validation Loss: 2.017859\n",
      "Epoch : [3236/8999], Training Loss: 2.017771, Validation Loss: 2.017805\n",
      "Epoch : [3237/8999], Training Loss: 2.017982, Validation Loss: 2.017975\n",
      "Epoch : [3238/8999], Training Loss: 2.017650, Validation Loss: 2.017633\n",
      "Epoch : [3239/8999], Training Loss: 2.017709, Validation Loss: 2.017847\n",
      "Epoch : [3240/8999], Training Loss: 2.017774, Validation Loss: 2.017719\n",
      "Epoch : [3241/8999], Training Loss: 2.017778, Validation Loss: 2.017364\n",
      "Epoch : [3242/8999], Training Loss: 2.017533, Validation Loss: 2.017525\n",
      "Epoch : [3243/8999], Training Loss: 2.017652, Validation Loss: 2.017540\n",
      "Epoch : [3244/8999], Training Loss: 2.017489, Validation Loss: 2.017458\n",
      "Epoch : [3245/8999], Training Loss: 2.017561, Validation Loss: 2.017522\n",
      "Epoch : [3246/8999], Training Loss: 2.017606, Validation Loss: 2.017515\n",
      "Epoch : [3247/8999], Training Loss: 2.017518, Validation Loss: 2.017554\n",
      "Epoch : [3248/8999], Training Loss: 2.017476, Validation Loss: 2.017555\n",
      "Epoch : [3249/8999], Training Loss: 2.017470, Validation Loss: 2.017724\n",
      "Epoch : [3250/8999], Training Loss: 2.017599, Validation Loss: 2.017860\n",
      "Epoch : [3251/8999], Training Loss: 2.017636, Validation Loss: 2.017782\n",
      "Epoch : [3252/8999], Training Loss: 2.017633, Validation Loss: 2.017741\n",
      "Epoch : [3253/8999], Training Loss: 2.017637, Validation Loss: 2.017538\n",
      "Epoch : [3254/8999], Training Loss: 2.017469, Validation Loss: 2.017445\n",
      "Epoch : [3255/8999], Training Loss: 2.017576, Validation Loss: 2.017434\n",
      "Epoch : [3256/8999], Training Loss: 2.017499, Validation Loss: 2.017409\n",
      "Epoch : [3257/8999], Training Loss: 2.017408, Validation Loss: 2.017265\n",
      "Epoch : [3258/8999], Training Loss: 2.017435, Validation Loss: 2.017639\n",
      "Epoch : [3259/8999], Training Loss: 2.017508, Validation Loss: 2.017787\n",
      "Epoch : [3260/8999], Training Loss: 2.017603, Validation Loss: 2.017771\n",
      "Epoch : [3261/8999], Training Loss: 2.017610, Validation Loss: 2.017608\n",
      "Epoch : [3262/8999], Training Loss: 2.017314, Validation Loss: 2.017526\n",
      "Epoch : [3263/8999], Training Loss: 2.017359, Validation Loss: 2.017403\n",
      "Epoch : [3264/8999], Training Loss: 2.017353, Validation Loss: 2.017545\n",
      "Epoch : [3265/8999], Training Loss: 2.017318, Validation Loss: 2.017555\n",
      "Epoch : [3266/8999], Training Loss: 2.017290, Validation Loss: 2.017560\n",
      "Epoch : [3267/8999], Training Loss: 2.017440, Validation Loss: 2.017611\n",
      "Epoch : [3268/8999], Training Loss: 2.017469, Validation Loss: 2.017560\n",
      "Epoch : [3269/8999], Training Loss: 2.017177, Validation Loss: 2.017715\n",
      "Epoch : [3270/8999], Training Loss: 2.017479, Validation Loss: 2.017228\n",
      "Epoch : [3271/8999], Training Loss: 2.017299, Validation Loss: 2.017844\n",
      "Epoch : [3272/8999], Training Loss: 2.017611, Validation Loss: 2.017247\n",
      "Epoch : [3273/8999], Training Loss: 2.017229, Validation Loss: 2.017235\n",
      "Epoch : [3274/8999], Training Loss: 2.017372, Validation Loss: 2.017754\n",
      "Epoch : [3275/8999], Training Loss: 2.017306, Validation Loss: 2.017257\n",
      "Epoch : [3276/8999], Training Loss: 2.017175, Validation Loss: 2.017057\n",
      "Epoch : [3277/8999], Training Loss: 2.017233, Validation Loss: 2.017082\n",
      "Epoch : [3278/8999], Training Loss: 2.017187, Validation Loss: 2.017226\n",
      "Epoch : [3279/8999], Training Loss: 2.017202, Validation Loss: 2.017163\n",
      "Epoch : [3280/8999], Training Loss: 2.017112, Validation Loss: 2.017101\n",
      "Epoch : [3281/8999], Training Loss: 2.017148, Validation Loss: 2.017026\n",
      "Epoch : [3282/8999], Training Loss: 2.017162, Validation Loss: 2.016848\n",
      "Epoch : [3283/8999], Training Loss: 2.017109, Validation Loss: 2.016985\n",
      "Epoch : [3284/8999], Training Loss: 2.017060, Validation Loss: 2.016997\n",
      "Epoch : [3285/8999], Training Loss: 2.017008, Validation Loss: 2.017130\n",
      "Epoch : [3286/8999], Training Loss: 2.017121, Validation Loss: 2.017193\n",
      "Epoch : [3287/8999], Training Loss: 2.017293, Validation Loss: 2.016960\n",
      "Epoch : [3288/8999], Training Loss: 2.016987, Validation Loss: 2.016992\n",
      "Epoch : [3289/8999], Training Loss: 2.017272, Validation Loss: 2.017148\n",
      "Epoch : [3290/8999], Training Loss: 2.017140, Validation Loss: 2.016861\n",
      "Epoch : [3291/8999], Training Loss: 2.017008, Validation Loss: 2.016930\n",
      "Epoch : [3292/8999], Training Loss: 2.017070, Validation Loss: 2.016885\n",
      "Epoch : [3293/8999], Training Loss: 2.017083, Validation Loss: 2.016838\n",
      "Epoch : [3294/8999], Training Loss: 2.016959, Validation Loss: 2.016835\n",
      "Epoch : [3295/8999], Training Loss: 2.017161, Validation Loss: 2.017028\n",
      "Epoch : [3296/8999], Training Loss: 2.017046, Validation Loss: 2.016974\n",
      "Epoch : [3297/8999], Training Loss: 2.017072, Validation Loss: 2.017081\n",
      "Epoch : [3298/8999], Training Loss: 2.016933, Validation Loss: 2.016814\n",
      "Epoch : [3299/8999], Training Loss: 2.016843, Validation Loss: 2.016862\n",
      "Epoch : [3300/8999], Training Loss: 2.016936, Validation Loss: 2.016823\n",
      "Epoch : [3301/8999], Training Loss: 2.016839, Validation Loss: 2.016732\n",
      "Epoch : [3302/8999], Training Loss: 2.017055, Validation Loss: 2.016837\n",
      "Epoch : [3303/8999], Training Loss: 2.016961, Validation Loss: 2.016692\n",
      "Epoch : [3304/8999], Training Loss: 2.016914, Validation Loss: 2.016858\n",
      "Epoch : [3305/8999], Training Loss: 2.017000, Validation Loss: 2.016717\n",
      "Epoch : [3306/8999], Training Loss: 2.016802, Validation Loss: 2.016773\n",
      "Epoch : [3307/8999], Training Loss: 2.016803, Validation Loss: 2.016732\n",
      "Epoch : [3308/8999], Training Loss: 2.016818, Validation Loss: 2.016686\n",
      "Epoch : [3309/8999], Training Loss: 2.016591, Validation Loss: 2.016599\n",
      "Epoch : [3310/8999], Training Loss: 2.016809, Validation Loss: 2.016492\n",
      "Epoch : [3311/8999], Training Loss: 2.016780, Validation Loss: 2.016739\n",
      "Epoch : [3312/8999], Training Loss: 2.016828, Validation Loss: 2.016779\n",
      "Epoch : [3313/8999], Training Loss: 2.016647, Validation Loss: 2.016529\n",
      "Epoch : [3314/8999], Training Loss: 2.016893, Validation Loss: 2.016995\n",
      "Epoch : [3315/8999], Training Loss: 2.016818, Validation Loss: 2.017069\n",
      "Epoch : [3316/8999], Training Loss: 2.016825, Validation Loss: 2.016814\n",
      "Epoch : [3317/8999], Training Loss: 2.016833, Validation Loss: 2.017157\n",
      "Epoch : [3318/8999], Training Loss: 2.016738, Validation Loss: 2.016673\n",
      "Epoch : [3319/8999], Training Loss: 2.016849, Validation Loss: 2.016731\n",
      "Epoch : [3320/8999], Training Loss: 2.016765, Validation Loss: 2.016940\n",
      "Epoch : [3321/8999], Training Loss: 2.016602, Validation Loss: 2.016467\n",
      "Epoch : [3322/8999], Training Loss: 2.016613, Validation Loss: 2.016641\n",
      "Epoch : [3323/8999], Training Loss: 2.016532, Validation Loss: 2.016719\n",
      "Epoch : [3324/8999], Training Loss: 2.016637, Validation Loss: 2.016687\n",
      "Epoch : [3325/8999], Training Loss: 2.016532, Validation Loss: 2.016764\n",
      "Epoch : [3326/8999], Training Loss: 2.016682, Validation Loss: 2.016705\n",
      "Epoch : [3327/8999], Training Loss: 2.016559, Validation Loss: 2.016618\n",
      "Epoch : [3328/8999], Training Loss: 2.016680, Validation Loss: 2.016918\n",
      "Epoch : [3329/8999], Training Loss: 2.016734, Validation Loss: 2.017031\n",
      "Epoch : [3330/8999], Training Loss: 2.016659, Validation Loss: 2.016618\n",
      "Epoch : [3331/8999], Training Loss: 2.016656, Validation Loss: 2.016642\n",
      "Epoch : [3332/8999], Training Loss: 2.016605, Validation Loss: 2.016884\n",
      "Epoch : [3333/8999], Training Loss: 2.016667, Validation Loss: 2.016796\n",
      "Epoch : [3334/8999], Training Loss: 2.016684, Validation Loss: 2.016617\n",
      "Epoch : [3335/8999], Training Loss: 2.016592, Validation Loss: 2.016776\n",
      "Epoch : [3336/8999], Training Loss: 2.016697, Validation Loss: 2.016795\n",
      "Epoch : [3337/8999], Training Loss: 2.016552, Validation Loss: 2.016796\n",
      "Epoch : [3338/8999], Training Loss: 2.016542, Validation Loss: 2.016570\n",
      "Epoch : [3339/8999], Training Loss: 2.016655, Validation Loss: 2.016715\n",
      "Epoch : [3340/8999], Training Loss: 2.016830, Validation Loss: 2.017206\n",
      "Epoch : [3341/8999], Training Loss: 2.016648, Validation Loss: 2.016427\n",
      "Epoch : [3342/8999], Training Loss: 2.016583, Validation Loss: 2.016573\n",
      "Epoch : [3343/8999], Training Loss: 2.016840, Validation Loss: 2.017368\n",
      "Epoch : [3344/8999], Training Loss: 2.016888, Validation Loss: 2.016491\n",
      "Epoch : [3345/8999], Training Loss: 2.016480, Validation Loss: 2.016502\n",
      "Epoch : [3346/8999], Training Loss: 2.016546, Validation Loss: 2.016450\n",
      "Epoch : [3347/8999], Training Loss: 2.016432, Validation Loss: 2.016569\n",
      "Epoch : [3348/8999], Training Loss: 2.016668, Validation Loss: 2.016988\n",
      "Epoch : [3349/8999], Training Loss: 2.016823, Validation Loss: 2.016497\n",
      "Epoch : [3350/8999], Training Loss: 2.016697, Validation Loss: 2.016502\n",
      "Epoch : [3351/8999], Training Loss: 2.016476, Validation Loss: 2.016794\n",
      "Epoch : [3352/8999], Training Loss: 2.016710, Validation Loss: 2.016605\n",
      "Epoch : [3353/8999], Training Loss: 2.016750, Validation Loss: 2.016641\n",
      "Epoch : [3354/8999], Training Loss: 2.016879, Validation Loss: 2.016536\n",
      "Epoch : [3355/8999], Training Loss: 2.016669, Validation Loss: 2.016851\n",
      "Epoch : [3356/8999], Training Loss: 2.016706, Validation Loss: 2.016523\n",
      "Epoch : [3357/8999], Training Loss: 2.016642, Validation Loss: 2.016526\n",
      "Epoch : [3358/8999], Training Loss: 2.016538, Validation Loss: 2.016761\n",
      "Epoch : [3359/8999], Training Loss: 2.016655, Validation Loss: 2.016477\n",
      "Epoch : [3360/8999], Training Loss: 2.016538, Validation Loss: 2.016892\n",
      "Epoch : [3361/8999], Training Loss: 2.016660, Validation Loss: 2.016613\n",
      "Epoch : [3362/8999], Training Loss: 2.016483, Validation Loss: 2.016566\n",
      "Epoch : [3363/8999], Training Loss: 2.016572, Validation Loss: 2.016778\n",
      "Epoch : [3364/8999], Training Loss: 2.016458, Validation Loss: 2.016754\n",
      "Epoch : [3365/8999], Training Loss: 2.016658, Validation Loss: 2.016362\n",
      "Epoch : [3366/8999], Training Loss: 2.016562, Validation Loss: 2.016871\n",
      "Epoch : [3367/8999], Training Loss: 2.016656, Validation Loss: 2.016714\n",
      "Epoch : [3368/8999], Training Loss: 2.016603, Validation Loss: 2.016501\n",
      "Epoch : [3369/8999], Training Loss: 2.016501, Validation Loss: 2.016547\n",
      "Epoch : [3370/8999], Training Loss: 2.016735, Validation Loss: 2.017272\n",
      "Epoch : [3371/8999], Training Loss: 2.016586, Validation Loss: 2.016644\n",
      "Epoch : [3372/8999], Training Loss: 2.016785, Validation Loss: 2.016839\n",
      "Epoch : [3373/8999], Training Loss: 2.016693, Validation Loss: 2.017074\n",
      "Epoch : [3374/8999], Training Loss: 2.016881, Validation Loss: 2.017836\n",
      "Epoch : [3375/8999], Training Loss: 2.017210, Validation Loss: 2.017278\n",
      "Epoch : [3376/8999], Training Loss: 2.017076, Validation Loss: 2.017207\n",
      "Epoch : [3377/8999], Training Loss: 2.016951, Validation Loss: 2.017125\n",
      "Epoch : [3378/8999], Training Loss: 2.016825, Validation Loss: 2.016817\n",
      "Epoch : [3379/8999], Training Loss: 2.017038, Validation Loss: 2.016780\n",
      "Epoch : [3380/8999], Training Loss: 2.016751, Validation Loss: 2.016738\n",
      "Epoch : [3381/8999], Training Loss: 2.016790, Validation Loss: 2.016648\n",
      "Epoch : [3382/8999], Training Loss: 2.016746, Validation Loss: 2.016528\n",
      "Epoch : [3383/8999], Training Loss: 2.016754, Validation Loss: 2.016487\n",
      "Epoch : [3384/8999], Training Loss: 2.016678, Validation Loss: 2.016445\n",
      "Epoch : [3385/8999], Training Loss: 2.016693, Validation Loss: 2.016557\n",
      "Epoch : [3386/8999], Training Loss: 2.016440, Validation Loss: 2.016320\n",
      "Epoch : [3387/8999], Training Loss: 2.016397, Validation Loss: 2.016413\n",
      "Epoch : [3388/8999], Training Loss: 2.016486, Validation Loss: 2.016426\n",
      "Epoch : [3389/8999], Training Loss: 2.016462, Validation Loss: 2.016758\n",
      "Epoch : [3390/8999], Training Loss: 2.016717, Validation Loss: 2.016554\n",
      "Epoch : [3391/8999], Training Loss: 2.016429, Validation Loss: 2.016504\n",
      "Epoch : [3392/8999], Training Loss: 2.016593, Validation Loss: 2.016560\n",
      "Epoch : [3393/8999], Training Loss: 2.016530, Validation Loss: 2.016262\n",
      "Epoch : [3394/8999], Training Loss: 2.016342, Validation Loss: 2.016395\n",
      "Epoch : [3395/8999], Training Loss: 2.016306, Validation Loss: 2.016405\n",
      "Epoch : [3396/8999], Training Loss: 2.016452, Validation Loss: 2.016362\n",
      "Epoch : [3397/8999], Training Loss: 2.016497, Validation Loss: 2.016561\n",
      "Epoch : [3398/8999], Training Loss: 2.016393, Validation Loss: 2.016147\n",
      "Epoch : [3399/8999], Training Loss: 2.016229, Validation Loss: 2.016496\n",
      "Epoch : [3400/8999], Training Loss: 2.016465, Validation Loss: 2.016643\n",
      "Epoch : [3401/8999], Training Loss: 2.016505, Validation Loss: 2.016294\n",
      "Epoch : [3402/8999], Training Loss: 2.016298, Validation Loss: 2.016270\n",
      "Epoch : [3403/8999], Training Loss: 2.016151, Validation Loss: 2.016406\n",
      "Epoch : [3404/8999], Training Loss: 2.016309, Validation Loss: 2.016494\n",
      "Epoch : [3405/8999], Training Loss: 2.016342, Validation Loss: 2.016449\n",
      "Epoch : [3406/8999], Training Loss: 2.016423, Validation Loss: 2.016408\n",
      "Epoch : [3407/8999], Training Loss: 2.016322, Validation Loss: 2.016341\n",
      "Epoch : [3408/8999], Training Loss: 2.016313, Validation Loss: 2.016237\n",
      "Epoch : [3409/8999], Training Loss: 2.016465, Validation Loss: 2.016334\n",
      "Epoch : [3410/8999], Training Loss: 2.016559, Validation Loss: 2.016782\n",
      "Epoch : [3411/8999], Training Loss: 2.016451, Validation Loss: 2.016318\n",
      "Epoch : [3412/8999], Training Loss: 2.016233, Validation Loss: 2.016122\n",
      "Epoch : [3413/8999], Training Loss: 2.016277, Validation Loss: 2.016305\n",
      "Epoch : [3414/8999], Training Loss: 2.016363, Validation Loss: 2.016477\n",
      "Epoch : [3415/8999], Training Loss: 2.016511, Validation Loss: 2.016245\n",
      "Epoch : [3416/8999], Training Loss: 2.016243, Validation Loss: 2.016510\n",
      "Epoch : [3417/8999], Training Loss: 2.016475, Validation Loss: 2.016474\n",
      "Epoch : [3418/8999], Training Loss: 2.016251, Validation Loss: 2.016143\n",
      "Epoch : [3419/8999], Training Loss: 2.016153, Validation Loss: 2.016021\n",
      "Epoch : [3420/8999], Training Loss: 2.016147, Validation Loss: 2.016078\n",
      "Epoch : [3421/8999], Training Loss: 2.016171, Validation Loss: 2.016548\n",
      "Epoch : [3422/8999], Training Loss: 2.016700, Validation Loss: 2.016508\n",
      "Epoch : [3423/8999], Training Loss: 2.016670, Validation Loss: 2.016247\n",
      "Epoch : [3424/8999], Training Loss: 2.016466, Validation Loss: 2.016577\n",
      "Epoch : [3425/8999], Training Loss: 2.016798, Validation Loss: 2.016497\n",
      "Epoch : [3426/8999], Training Loss: 2.016388, Validation Loss: 2.016143\n",
      "Epoch : [3427/8999], Training Loss: 2.016364, Validation Loss: 2.016274\n",
      "Epoch : [3428/8999], Training Loss: 2.016497, Validation Loss: 2.016607\n",
      "Epoch : [3429/8999], Training Loss: 2.016455, Validation Loss: 2.016384\n",
      "Epoch : [3430/8999], Training Loss: 2.016214, Validation Loss: 2.016113\n",
      "Epoch : [3431/8999], Training Loss: 2.016229, Validation Loss: 2.016236\n",
      "Epoch : [3432/8999], Training Loss: 2.016380, Validation Loss: 2.016442\n",
      "Epoch : [3433/8999], Training Loss: 2.016345, Validation Loss: 2.016300\n",
      "Epoch : [3434/8999], Training Loss: 2.016201, Validation Loss: 2.016293\n",
      "Epoch : [3435/8999], Training Loss: 2.016363, Validation Loss: 2.016212\n",
      "Epoch : [3436/8999], Training Loss: 2.016150, Validation Loss: 2.016167\n",
      "Epoch : [3437/8999], Training Loss: 2.016194, Validation Loss: 2.015998\n",
      "Epoch : [3438/8999], Training Loss: 2.016223, Validation Loss: 2.016432\n",
      "Epoch : [3439/8999], Training Loss: 2.016299, Validation Loss: 2.016037\n",
      "Epoch : [3440/8999], Training Loss: 2.016221, Validation Loss: 2.016348\n",
      "Epoch : [3441/8999], Training Loss: 2.016150, Validation Loss: 2.016183\n",
      "Epoch : [3442/8999], Training Loss: 2.016293, Validation Loss: 2.016282\n",
      "Epoch : [3443/8999], Training Loss: 2.016454, Validation Loss: 2.016251\n",
      "Epoch : [3444/8999], Training Loss: 2.016298, Validation Loss: 2.016182\n",
      "Epoch : [3445/8999], Training Loss: 2.016365, Validation Loss: 2.016126\n",
      "Epoch : [3446/8999], Training Loss: 2.016340, Validation Loss: 2.016110\n",
      "Epoch : [3447/8999], Training Loss: 2.016250, Validation Loss: 2.016153\n",
      "Epoch : [3448/8999], Training Loss: 2.016112, Validation Loss: 2.016330\n",
      "Epoch : [3449/8999], Training Loss: 2.016059, Validation Loss: 2.016112\n",
      "Epoch : [3450/8999], Training Loss: 2.016108, Validation Loss: 2.016113\n",
      "Epoch : [3451/8999], Training Loss: 2.016366, Validation Loss: 2.016959\n",
      "Epoch : [3452/8999], Training Loss: 2.016342, Validation Loss: 2.016098\n",
      "Epoch : [3453/8999], Training Loss: 2.016036, Validation Loss: 2.015994\n",
      "Epoch : [3454/8999], Training Loss: 2.016029, Validation Loss: 2.016395\n",
      "Epoch : [3455/8999], Training Loss: 2.016028, Validation Loss: 2.016076\n",
      "Epoch : [3456/8999], Training Loss: 2.016136, Validation Loss: 2.016128\n",
      "Epoch : [3457/8999], Training Loss: 2.016386, Validation Loss: 2.016804\n",
      "Epoch : [3458/8999], Training Loss: 2.016306, Validation Loss: 2.016725\n",
      "Epoch : [3459/8999], Training Loss: 2.016270, Validation Loss: 2.016157\n",
      "Epoch : [3460/8999], Training Loss: 2.016226, Validation Loss: 2.016373\n",
      "Epoch : [3461/8999], Training Loss: 2.016273, Validation Loss: 2.016604\n",
      "Epoch : [3462/8999], Training Loss: 2.016208, Validation Loss: 2.016141\n",
      "Epoch : [3463/8999], Training Loss: 2.016198, Validation Loss: 2.017118\n",
      "Epoch : [3464/8999], Training Loss: 2.016390, Validation Loss: 2.016517\n",
      "Epoch : [3465/8999], Training Loss: 2.016429, Validation Loss: 2.016197\n",
      "Epoch : [3466/8999], Training Loss: 2.016388, Validation Loss: 2.016982\n",
      "Epoch : [3467/8999], Training Loss: 2.016258, Validation Loss: 2.016287\n",
      "Epoch : [3468/8999], Training Loss: 2.016163, Validation Loss: 2.016138\n",
      "Epoch : [3469/8999], Training Loss: 2.016256, Validation Loss: 2.016585\n",
      "Epoch : [3470/8999], Training Loss: 2.016274, Validation Loss: 2.016376\n",
      "Epoch : [3471/8999], Training Loss: 2.016335, Validation Loss: 2.016318\n",
      "Epoch : [3472/8999], Training Loss: 2.016317, Validation Loss: 2.016728\n",
      "Epoch : [3473/8999], Training Loss: 2.016274, Validation Loss: 2.016255\n",
      "Epoch : [3474/8999], Training Loss: 2.016209, Validation Loss: 2.016563\n",
      "Epoch : [3475/8999], Training Loss: 2.016298, Validation Loss: 2.016745\n",
      "Epoch : [3476/8999], Training Loss: 2.016330, Validation Loss: 2.016337\n",
      "Epoch : [3477/8999], Training Loss: 2.016311, Validation Loss: 2.016842\n",
      "Epoch : [3478/8999], Training Loss: 2.016218, Validation Loss: 2.016316\n",
      "Epoch : [3479/8999], Training Loss: 2.016326, Validation Loss: 2.016298\n",
      "Epoch : [3480/8999], Training Loss: 2.016203, Validation Loss: 2.016399\n",
      "Epoch : [3481/8999], Training Loss: 2.016316, Validation Loss: 2.016291\n",
      "Epoch : [3482/8999], Training Loss: 2.016526, Validation Loss: 2.016647\n",
      "Epoch : [3483/8999], Training Loss: 2.016816, Validation Loss: 2.016221\n",
      "Epoch : [3484/8999], Training Loss: 2.016430, Validation Loss: 2.016236\n",
      "Epoch : [3485/8999], Training Loss: 2.016249, Validation Loss: 2.016229\n",
      "Epoch : [3486/8999], Training Loss: 2.016485, Validation Loss: 2.016421\n",
      "Epoch : [3487/8999], Training Loss: 2.016371, Validation Loss: 2.016111\n",
      "Epoch : [3488/8999], Training Loss: 2.016160, Validation Loss: 2.016319\n",
      "Epoch : [3489/8999], Training Loss: 2.016109, Validation Loss: 2.016174\n",
      "Epoch : [3490/8999], Training Loss: 2.016311, Validation Loss: 2.016050\n",
      "Epoch : [3491/8999], Training Loss: 2.016073, Validation Loss: 2.016066\n",
      "Epoch : [3492/8999], Training Loss: 2.016209, Validation Loss: 2.016307\n",
      "Epoch : [3493/8999], Training Loss: 2.016258, Validation Loss: 2.016324\n",
      "Epoch : [3494/8999], Training Loss: 2.016257, Validation Loss: 2.016082\n",
      "Epoch : [3495/8999], Training Loss: 2.016271, Validation Loss: 2.016130\n",
      "Epoch : [3496/8999], Training Loss: 2.016130, Validation Loss: 2.016106\n",
      "Epoch : [3497/8999], Training Loss: 2.016082, Validation Loss: 2.016256\n",
      "Epoch : [3498/8999], Training Loss: 2.016178, Validation Loss: 2.016228\n",
      "Epoch : [3499/8999], Training Loss: 2.016287, Validation Loss: 2.016333\n",
      "Epoch : [3500/8999], Training Loss: 2.016109, Validation Loss: 2.016535\n",
      "Epoch : [3501/8999], Training Loss: 2.016261, Validation Loss: 2.016216\n",
      "Epoch : [3502/8999], Training Loss: 2.016001, Validation Loss: 2.016135\n",
      "Epoch : [3503/8999], Training Loss: 2.016095, Validation Loss: 2.016161\n",
      "Epoch : [3504/8999], Training Loss: 2.016154, Validation Loss: 2.016386\n",
      "Epoch : [3505/8999], Training Loss: 2.016283, Validation Loss: 2.016163\n",
      "Epoch : [3506/8999], Training Loss: 2.016050, Validation Loss: 2.016007\n",
      "Epoch : [3507/8999], Training Loss: 2.016047, Validation Loss: 2.016243\n",
      "Epoch : [3508/8999], Training Loss: 2.015993, Validation Loss: 2.016645\n",
      "Epoch : [3509/8999], Training Loss: 2.016025, Validation Loss: 2.015946\n",
      "Epoch : [3510/8999], Training Loss: 2.016132, Validation Loss: 2.016049\n",
      "Epoch : [3511/8999], Training Loss: 2.016098, Validation Loss: 2.017000\n",
      "Epoch : [3512/8999], Training Loss: 2.016274, Validation Loss: 2.016029\n",
      "Epoch : [3513/8999], Training Loss: 2.015937, Validation Loss: 2.015869\n",
      "Epoch : [3514/8999], Training Loss: 2.016082, Validation Loss: 2.016544\n",
      "Epoch : [3515/8999], Training Loss: 2.016095, Validation Loss: 2.016538\n",
      "Epoch : [3516/8999], Training Loss: 2.016064, Validation Loss: 2.016128\n",
      "Epoch : [3517/8999], Training Loss: 2.015966, Validation Loss: 2.016025\n",
      "Epoch : [3518/8999], Training Loss: 2.015957, Validation Loss: 2.016035\n",
      "Epoch : [3519/8999], Training Loss: 2.015966, Validation Loss: 2.016874\n",
      "Epoch : [3520/8999], Training Loss: 2.016172, Validation Loss: 2.016494\n",
      "Epoch : [3521/8999], Training Loss: 2.016097, Validation Loss: 2.016030\n",
      "Epoch : [3522/8999], Training Loss: 2.016101, Validation Loss: 2.016163\n",
      "Epoch : [3523/8999], Training Loss: 2.016010, Validation Loss: 2.016667\n",
      "Epoch : [3524/8999], Training Loss: 2.016086, Validation Loss: 2.016213\n",
      "Epoch : [3525/8999], Training Loss: 2.016181, Validation Loss: 2.016169\n",
      "Epoch : [3526/8999], Training Loss: 2.016127, Validation Loss: 2.016821\n",
      "Epoch : [3527/8999], Training Loss: 2.016223, Validation Loss: 2.017053\n",
      "Epoch : [3528/8999], Training Loss: 2.016370, Validation Loss: 2.016114\n",
      "Epoch : [3529/8999], Training Loss: 2.016386, Validation Loss: 2.017016\n",
      "Epoch : [3530/8999], Training Loss: 2.016188, Validation Loss: 2.016345\n",
      "Epoch : [3531/8999], Training Loss: 2.016114, Validation Loss: 2.016062\n",
      "Epoch : [3532/8999], Training Loss: 2.016077, Validation Loss: 2.016228\n",
      "Epoch : [3533/8999], Training Loss: 2.016011, Validation Loss: 2.016987\n",
      "Epoch : [3534/8999], Training Loss: 2.016053, Validation Loss: 2.016337\n",
      "Epoch : [3535/8999], Training Loss: 2.016209, Validation Loss: 2.016111\n",
      "Epoch : [3536/8999], Training Loss: 2.016084, Validation Loss: 2.016472\n",
      "Epoch : [3537/8999], Training Loss: 2.016208, Validation Loss: 2.016700\n",
      "Epoch : [3538/8999], Training Loss: 2.016082, Validation Loss: 2.016410\n",
      "Epoch : [3539/8999], Training Loss: 2.016007, Validation Loss: 2.015975\n",
      "Epoch : [3540/8999], Training Loss: 2.016123, Validation Loss: 2.016532\n",
      "Epoch : [3541/8999], Training Loss: 2.016188, Validation Loss: 2.016488\n",
      "Epoch : [3542/8999], Training Loss: 2.015975, Validation Loss: 2.016381\n",
      "Epoch : [3543/8999], Training Loss: 2.016048, Validation Loss: 2.016304\n",
      "Epoch : [3544/8999], Training Loss: 2.016155, Validation Loss: 2.016712\n",
      "Epoch : [3545/8999], Training Loss: 2.016053, Validation Loss: 2.016170\n",
      "Epoch : [3546/8999], Training Loss: 2.016115, Validation Loss: 2.016168\n",
      "Epoch : [3547/8999], Training Loss: 2.015919, Validation Loss: 2.016291\n",
      "Epoch : [3548/8999], Training Loss: 2.016048, Validation Loss: 2.016696\n",
      "Epoch : [3549/8999], Training Loss: 2.016029, Validation Loss: 2.016029\n",
      "Epoch : [3550/8999], Training Loss: 2.016166, Validation Loss: 2.016259\n",
      "Epoch : [3551/8999], Training Loss: 2.015895, Validation Loss: 2.016674\n",
      "Epoch : [3552/8999], Training Loss: 2.016052, Validation Loss: 2.016243\n",
      "Epoch : [3553/8999], Training Loss: 2.015937, Validation Loss: 2.016345\n",
      "Epoch : [3554/8999], Training Loss: 2.015928, Validation Loss: 2.016443\n",
      "Epoch : [3555/8999], Training Loss: 2.015962, Validation Loss: 2.015950\n",
      "Epoch : [3556/8999], Training Loss: 2.016337, Validation Loss: 2.016730\n",
      "Epoch : [3557/8999], Training Loss: 2.016126, Validation Loss: 2.015957\n",
      "Epoch : [3558/8999], Training Loss: 2.015967, Validation Loss: 2.016154\n",
      "Epoch : [3559/8999], Training Loss: 2.016176, Validation Loss: 2.016517\n",
      "Epoch : [3560/8999], Training Loss: 2.016065, Validation Loss: 2.015959\n",
      "Epoch : [3561/8999], Training Loss: 2.016114, Validation Loss: 2.016207\n",
      "Epoch : [3562/8999], Training Loss: 2.016139, Validation Loss: 2.015968\n",
      "Epoch : [3563/8999], Training Loss: 2.016061, Validation Loss: 2.016248\n",
      "Epoch : [3564/8999], Training Loss: 2.016162, Validation Loss: 2.016007\n",
      "Epoch : [3565/8999], Training Loss: 2.015970, Validation Loss: 2.015815\n",
      "Epoch : [3566/8999], Training Loss: 2.015988, Validation Loss: 2.016422\n",
      "Epoch : [3567/8999], Training Loss: 2.016056, Validation Loss: 2.016073\n",
      "Epoch : [3568/8999], Training Loss: 2.016102, Validation Loss: 2.016301\n",
      "Epoch : [3569/8999], Training Loss: 2.016068, Validation Loss: 2.015912\n",
      "Epoch : [3570/8999], Training Loss: 2.015995, Validation Loss: 2.015932\n",
      "Epoch : [3571/8999], Training Loss: 2.015936, Validation Loss: 2.015881\n",
      "Epoch : [3572/8999], Training Loss: 2.016017, Validation Loss: 2.016021\n",
      "Epoch : [3573/8999], Training Loss: 2.016016, Validation Loss: 2.016184\n",
      "Epoch : [3574/8999], Training Loss: 2.015893, Validation Loss: 2.016145\n",
      "Epoch : [3575/8999], Training Loss: 2.016128, Validation Loss: 2.016117\n",
      "Epoch : [3576/8999], Training Loss: 2.015962, Validation Loss: 2.015960\n",
      "Epoch : [3577/8999], Training Loss: 2.015934, Validation Loss: 2.015954\n",
      "Epoch : [3578/8999], Training Loss: 2.016331, Validation Loss: 2.017196\n",
      "Epoch : [3579/8999], Training Loss: 2.016661, Validation Loss: 2.016143\n",
      "Epoch : [3580/8999], Training Loss: 2.016213, Validation Loss: 2.016204\n",
      "Epoch : [3581/8999], Training Loss: 2.016031, Validation Loss: 2.016055\n",
      "Epoch : [3582/8999], Training Loss: 2.015994, Validation Loss: 2.016135\n",
      "Epoch : [3583/8999], Training Loss: 2.016093, Validation Loss: 2.016184\n",
      "Epoch : [3584/8999], Training Loss: 2.016154, Validation Loss: 2.016125\n",
      "Epoch : [3585/8999], Training Loss: 2.016226, Validation Loss: 2.015956\n",
      "Epoch : [3586/8999], Training Loss: 2.016051, Validation Loss: 2.015961\n",
      "Epoch : [3587/8999], Training Loss: 2.016076, Validation Loss: 2.017089\n",
      "Epoch : [3588/8999], Training Loss: 2.016175, Validation Loss: 2.016240\n",
      "Epoch : [3589/8999], Training Loss: 2.016063, Validation Loss: 2.016155\n",
      "Epoch : [3590/8999], Training Loss: 2.015929, Validation Loss: 2.016563\n",
      "Epoch : [3591/8999], Training Loss: 2.016164, Validation Loss: 2.017120\n",
      "Epoch : [3592/8999], Training Loss: 2.016334, Validation Loss: 2.016070\n",
      "Epoch : [3593/8999], Training Loss: 2.015995, Validation Loss: 2.016768\n",
      "Epoch : [3594/8999], Training Loss: 2.016110, Validation Loss: 2.016722\n",
      "Epoch : [3595/8999], Training Loss: 2.016159, Validation Loss: 2.016114\n",
      "Epoch : [3596/8999], Training Loss: 2.015903, Validation Loss: 2.016465\n",
      "Epoch : [3597/8999], Training Loss: 2.016017, Validation Loss: 2.017049\n",
      "Epoch : [3598/8999], Training Loss: 2.016017, Validation Loss: 2.016290\n",
      "Epoch : [3599/8999], Training Loss: 2.016044, Validation Loss: 2.016674\n",
      "Epoch : [3600/8999], Training Loss: 2.016109, Validation Loss: 2.016713\n",
      "Epoch : [3601/8999], Training Loss: 2.015966, Validation Loss: 2.017093\n",
      "Epoch : [3602/8999], Training Loss: 2.016216, Validation Loss: 2.017000\n",
      "Epoch : [3603/8999], Training Loss: 2.016126, Validation Loss: 2.016681\n",
      "Epoch : [3604/8999], Training Loss: 2.016231, Validation Loss: 2.017098\n",
      "Epoch : [3605/8999], Training Loss: 2.016303, Validation Loss: 2.017502\n",
      "Epoch : [3606/8999], Training Loss: 2.016607, Validation Loss: 2.016761\n",
      "Epoch : [3607/8999], Training Loss: 2.016576, Validation Loss: 2.017298\n",
      "Epoch : [3608/8999], Training Loss: 2.016735, Validation Loss: 2.017032\n",
      "Epoch : [3609/8999], Training Loss: 2.016954, Validation Loss: 2.017261\n",
      "Epoch : [3610/8999], Training Loss: 2.016845, Validation Loss: 2.016970\n",
      "Epoch : [3611/8999], Training Loss: 2.016788, Validation Loss: 2.016696\n",
      "Epoch : [3612/8999], Training Loss: 2.016889, Validation Loss: 2.016731\n",
      "Epoch : [3613/8999], Training Loss: 2.017082, Validation Loss: 2.017321\n",
      "Epoch : [3614/8999], Training Loss: 2.017663, Validation Loss: 2.017132\n",
      "Epoch : [3615/8999], Training Loss: 2.016861, Validation Loss: 2.016201\n",
      "Epoch : [3616/8999], Training Loss: 2.016306, Validation Loss: 2.016407\n",
      "Epoch : [3617/8999], Training Loss: 2.016382, Validation Loss: 2.016217\n",
      "Epoch : [3618/8999], Training Loss: 2.016139, Validation Loss: 2.015988\n",
      "Epoch : [3619/8999], Training Loss: 2.015911, Validation Loss: 2.015965\n",
      "Epoch : [3620/8999], Training Loss: 2.016005, Validation Loss: 2.016217\n",
      "Epoch : [3621/8999], Training Loss: 2.015928, Validation Loss: 2.016012\n",
      "Epoch : [3622/8999], Training Loss: 2.015966, Validation Loss: 2.015825\n",
      "Epoch : [3623/8999], Training Loss: 2.015785, Validation Loss: 2.016267\n",
      "Epoch : [3624/8999], Training Loss: 2.015871, Validation Loss: 2.015706\n",
      "Epoch : [3625/8999], Training Loss: 2.015769, Validation Loss: 2.015748\n",
      "Epoch : [3626/8999], Training Loss: 2.015693, Validation Loss: 2.015726\n",
      "Epoch : [3627/8999], Training Loss: 2.015791, Validation Loss: 2.016206\n",
      "Epoch : [3628/8999], Training Loss: 2.015905, Validation Loss: 2.015789\n",
      "Epoch : [3629/8999], Training Loss: 2.015982, Validation Loss: 2.015711\n",
      "Epoch : [3630/8999], Training Loss: 2.015966, Validation Loss: 2.015958\n",
      "Epoch : [3631/8999], Training Loss: 2.015948, Validation Loss: 2.016187\n",
      "Epoch : [3632/8999], Training Loss: 2.016000, Validation Loss: 2.015870\n",
      "Epoch : [3633/8999], Training Loss: 2.015769, Validation Loss: 2.015779\n",
      "Epoch : [3634/8999], Training Loss: 2.015846, Validation Loss: 2.015906\n",
      "Epoch : [3635/8999], Training Loss: 2.015837, Validation Loss: 2.016003\n",
      "Epoch : [3636/8999], Training Loss: 2.015737, Validation Loss: 2.015664\n",
      "Epoch : [3637/8999], Training Loss: 2.015779, Validation Loss: 2.015910\n",
      "Epoch : [3638/8999], Training Loss: 2.015827, Validation Loss: 2.015867\n",
      "Epoch : [3639/8999], Training Loss: 2.015986, Validation Loss: 2.015769\n",
      "Epoch : [3640/8999], Training Loss: 2.015825, Validation Loss: 2.015911\n",
      "Epoch : [3641/8999], Training Loss: 2.015849, Validation Loss: 2.016197\n",
      "Epoch : [3642/8999], Training Loss: 2.015816, Validation Loss: 2.016193\n",
      "Epoch : [3643/8999], Training Loss: 2.016049, Validation Loss: 2.015813\n",
      "Epoch : [3644/8999], Training Loss: 2.015979, Validation Loss: 2.016029\n",
      "Epoch : [3645/8999], Training Loss: 2.015781, Validation Loss: 2.016077\n",
      "Epoch : [3646/8999], Training Loss: 2.015787, Validation Loss: 2.015873\n",
      "Epoch : [3647/8999], Training Loss: 2.015808, Validation Loss: 2.015789\n",
      "Epoch : [3648/8999], Training Loss: 2.015743, Validation Loss: 2.016081\n",
      "Epoch : [3649/8999], Training Loss: 2.015714, Validation Loss: 2.016073\n",
      "Epoch : [3650/8999], Training Loss: 2.016087, Validation Loss: 2.016018\n",
      "Epoch : [3651/8999], Training Loss: 2.015940, Validation Loss: 2.015731\n",
      "Epoch : [3652/8999], Training Loss: 2.015846, Validation Loss: 2.016199\n",
      "Epoch : [3653/8999], Training Loss: 2.015794, Validation Loss: 2.016100\n",
      "Epoch : [3654/8999], Training Loss: 2.015911, Validation Loss: 2.015755\n",
      "Epoch : [3655/8999], Training Loss: 2.015858, Validation Loss: 2.015938\n",
      "Epoch : [3656/8999], Training Loss: 2.015794, Validation Loss: 2.016062\n",
      "Epoch : [3657/8999], Training Loss: 2.015864, Validation Loss: 2.015665\n",
      "Epoch : [3658/8999], Training Loss: 2.015880, Validation Loss: 2.016037\n",
      "Epoch : [3659/8999], Training Loss: 2.015833, Validation Loss: 2.015876\n",
      "Epoch : [3660/8999], Training Loss: 2.015707, Validation Loss: 2.015748\n",
      "Epoch : [3661/8999], Training Loss: 2.015674, Validation Loss: 2.015595\n",
      "Epoch : [3662/8999], Training Loss: 2.015836, Validation Loss: 2.016259\n",
      "Epoch : [3663/8999], Training Loss: 2.015905, Validation Loss: 2.015770\n",
      "Epoch : [3664/8999], Training Loss: 2.015712, Validation Loss: 2.015796\n",
      "Epoch : [3665/8999], Training Loss: 2.015845, Validation Loss: 2.015879\n",
      "Epoch : [3666/8999], Training Loss: 2.015910, Validation Loss: 2.015709\n",
      "Epoch : [3667/8999], Training Loss: 2.015738, Validation Loss: 2.016028\n",
      "Epoch : [3668/8999], Training Loss: 2.015754, Validation Loss: 2.015871\n",
      "Epoch : [3669/8999], Training Loss: 2.015728, Validation Loss: 2.015649\n",
      "Epoch : [3670/8999], Training Loss: 2.015659, Validation Loss: 2.015596\n",
      "Epoch : [3671/8999], Training Loss: 2.015729, Validation Loss: 2.015996\n",
      "Epoch : [3672/8999], Training Loss: 2.015832, Validation Loss: 2.016024\n",
      "Epoch : [3673/8999], Training Loss: 2.015673, Validation Loss: 2.015699\n",
      "Epoch : [3674/8999], Training Loss: 2.015767, Validation Loss: 2.015782\n",
      "Epoch : [3675/8999], Training Loss: 2.015825, Validation Loss: 2.016402\n",
      "Epoch : [3676/8999], Training Loss: 2.015773, Validation Loss: 2.015747\n",
      "Epoch : [3677/8999], Training Loss: 2.015695, Validation Loss: 2.015771\n",
      "Epoch : [3678/8999], Training Loss: 2.015680, Validation Loss: 2.015655\n",
      "Epoch : [3679/8999], Training Loss: 2.016001, Validation Loss: 2.016574\n",
      "Epoch : [3680/8999], Training Loss: 2.015818, Validation Loss: 2.015754\n",
      "Epoch : [3681/8999], Training Loss: 2.015760, Validation Loss: 2.016031\n",
      "Epoch : [3682/8999], Training Loss: 2.015662, Validation Loss: 2.016007\n",
      "Epoch : [3683/8999], Training Loss: 2.015843, Validation Loss: 2.015725\n",
      "Epoch : [3684/8999], Training Loss: 2.015905, Validation Loss: 2.015754\n",
      "Epoch : [3685/8999], Training Loss: 2.015818, Validation Loss: 2.016352\n",
      "Epoch : [3686/8999], Training Loss: 2.015831, Validation Loss: 2.015766\n",
      "Epoch : [3687/8999], Training Loss: 2.015709, Validation Loss: 2.016359\n",
      "Epoch : [3688/8999], Training Loss: 2.015748, Validation Loss: 2.016104\n",
      "Epoch : [3689/8999], Training Loss: 2.015697, Validation Loss: 2.015926\n",
      "Epoch : [3690/8999], Training Loss: 2.015893, Validation Loss: 2.015999\n",
      "Epoch : [3691/8999], Training Loss: 2.015606, Validation Loss: 2.015989\n",
      "Epoch : [3692/8999], Training Loss: 2.015740, Validation Loss: 2.015743\n",
      "Epoch : [3693/8999], Training Loss: 2.015751, Validation Loss: 2.015918\n",
      "Epoch : [3694/8999], Training Loss: 2.015816, Validation Loss: 2.016340\n",
      "Epoch : [3695/8999], Training Loss: 2.015718, Validation Loss: 2.015761\n",
      "Epoch : [3696/8999], Training Loss: 2.015709, Validation Loss: 2.016103\n",
      "Epoch : [3697/8999], Training Loss: 2.015753, Validation Loss: 2.016064\n",
      "Epoch : [3698/8999], Training Loss: 2.015974, Validation Loss: 2.015917\n",
      "Epoch : [3699/8999], Training Loss: 2.015839, Validation Loss: 2.016458\n",
      "Epoch : [3700/8999], Training Loss: 2.015903, Validation Loss: 2.015951\n",
      "Epoch : [3701/8999], Training Loss: 2.015658, Validation Loss: 2.015907\n",
      "Epoch : [3702/8999], Training Loss: 2.015590, Validation Loss: 2.015873\n",
      "Epoch : [3703/8999], Training Loss: 2.015684, Validation Loss: 2.015980\n",
      "Epoch : [3704/8999], Training Loss: 2.015656, Validation Loss: 2.016283\n",
      "Epoch : [3705/8999], Training Loss: 2.015755, Validation Loss: 2.015939\n",
      "Epoch : [3706/8999], Training Loss: 2.015724, Validation Loss: 2.016078\n",
      "Epoch : [3707/8999], Training Loss: 2.015828, Validation Loss: 2.016194\n",
      "Epoch : [3708/8999], Training Loss: 2.015819, Validation Loss: 2.015729\n",
      "Epoch : [3709/8999], Training Loss: 2.015748, Validation Loss: 2.016378\n",
      "Epoch : [3710/8999], Training Loss: 2.015835, Validation Loss: 2.016126\n",
      "Epoch : [3711/8999], Training Loss: 2.015863, Validation Loss: 2.015934\n",
      "Epoch : [3712/8999], Training Loss: 2.015827, Validation Loss: 2.016029\n",
      "Epoch : [3713/8999], Training Loss: 2.015697, Validation Loss: 2.016210\n",
      "Epoch : [3714/8999], Training Loss: 2.015714, Validation Loss: 2.015810\n",
      "Epoch : [3715/8999], Training Loss: 2.015778, Validation Loss: 2.015930\n",
      "Epoch : [3716/8999], Training Loss: 2.016004, Validation Loss: 2.016624\n",
      "Epoch : [3717/8999], Training Loss: 2.015921, Validation Loss: 2.015857\n",
      "Epoch : [3718/8999], Training Loss: 2.015862, Validation Loss: 2.016153\n",
      "Epoch : [3719/8999], Training Loss: 2.015772, Validation Loss: 2.015838\n",
      "Epoch : [3720/8999], Training Loss: 2.015746, Validation Loss: 2.015804\n",
      "Epoch : [3721/8999], Training Loss: 2.015930, Validation Loss: 2.015786\n",
      "Epoch : [3722/8999], Training Loss: 2.015671, Validation Loss: 2.015715\n",
      "Epoch : [3723/8999], Training Loss: 2.015869, Validation Loss: 2.015816\n",
      "Epoch : [3724/8999], Training Loss: 2.015754, Validation Loss: 2.016032\n",
      "Epoch : [3725/8999], Training Loss: 2.015926, Validation Loss: 2.015968\n",
      "Epoch : [3726/8999], Training Loss: 2.016098, Validation Loss: 2.016001\n",
      "Epoch : [3727/8999], Training Loss: 2.015709, Validation Loss: 2.015653\n",
      "Epoch : [3728/8999], Training Loss: 2.015677, Validation Loss: 2.015550\n",
      "Epoch : [3729/8999], Training Loss: 2.015542, Validation Loss: 2.015665\n",
      "Epoch : [3730/8999], Training Loss: 2.015596, Validation Loss: 2.015418\n",
      "Epoch : [3731/8999], Training Loss: 2.015494, Validation Loss: 2.015698\n",
      "Epoch : [3732/8999], Training Loss: 2.015605, Validation Loss: 2.015742\n",
      "Epoch : [3733/8999], Training Loss: 2.015926, Validation Loss: 2.015782\n",
      "Epoch : [3734/8999], Training Loss: 2.015710, Validation Loss: 2.015714\n",
      "Epoch : [3735/8999], Training Loss: 2.015737, Validation Loss: 2.015666\n",
      "Epoch : [3736/8999], Training Loss: 2.015792, Validation Loss: 2.015797\n",
      "Epoch : [3737/8999], Training Loss: 2.015746, Validation Loss: 2.015587\n",
      "Epoch : [3738/8999], Training Loss: 2.015693, Validation Loss: 2.015608\n",
      "Epoch : [3739/8999], Training Loss: 2.015598, Validation Loss: 2.015873\n",
      "Epoch : [3740/8999], Training Loss: 2.015528, Validation Loss: 2.016101\n",
      "Epoch : [3741/8999], Training Loss: 2.015755, Validation Loss: 2.015507\n",
      "Epoch : [3742/8999], Training Loss: 2.015708, Validation Loss: 2.015726\n",
      "Epoch : [3743/8999], Training Loss: 2.015576, Validation Loss: 2.016039\n",
      "Epoch : [3744/8999], Training Loss: 2.015596, Validation Loss: 2.015658\n",
      "Epoch : [3745/8999], Training Loss: 2.015730, Validation Loss: 2.015500\n",
      "Epoch : [3746/8999], Training Loss: 2.015799, Validation Loss: 2.015790\n",
      "Epoch : [3747/8999], Training Loss: 2.015590, Validation Loss: 2.015790\n",
      "Epoch : [3748/8999], Training Loss: 2.015647, Validation Loss: 2.015772\n",
      "Epoch : [3749/8999], Training Loss: 2.015653, Validation Loss: 2.015525\n",
      "Epoch : [3750/8999], Training Loss: 2.015656, Validation Loss: 2.015913\n",
      "Epoch : [3751/8999], Training Loss: 2.015549, Validation Loss: 2.015709\n",
      "Epoch : [3752/8999], Training Loss: 2.015632, Validation Loss: 2.015481\n",
      "Epoch : [3753/8999], Training Loss: 2.015548, Validation Loss: 2.015957\n",
      "Epoch : [3754/8999], Training Loss: 2.015600, Validation Loss: 2.016675\n",
      "Epoch : [3755/8999], Training Loss: 2.015852, Validation Loss: 2.015614\n",
      "Epoch : [3756/8999], Training Loss: 2.015671, Validation Loss: 2.015932\n",
      "Epoch : [3757/8999], Training Loss: 2.015699, Validation Loss: 2.016357\n",
      "Epoch : [3758/8999], Training Loss: 2.015780, Validation Loss: 2.015716\n",
      "Epoch : [3759/8999], Training Loss: 2.015640, Validation Loss: 2.016103\n",
      "Epoch : [3760/8999], Training Loss: 2.015509, Validation Loss: 2.016533\n",
      "Epoch : [3761/8999], Training Loss: 2.015847, Validation Loss: 2.015935\n",
      "Epoch : [3762/8999], Training Loss: 2.015761, Validation Loss: 2.016009\n",
      "Epoch : [3763/8999], Training Loss: 2.015722, Validation Loss: 2.016423\n",
      "Epoch : [3764/8999], Training Loss: 2.015655, Validation Loss: 2.016372\n",
      "Epoch : [3765/8999], Training Loss: 2.015537, Validation Loss: 2.016080\n",
      "Epoch : [3766/8999], Training Loss: 2.015625, Validation Loss: 2.015926\n",
      "Epoch : [3767/8999], Training Loss: 2.015556, Validation Loss: 2.016176\n",
      "Epoch : [3768/8999], Training Loss: 2.015530, Validation Loss: 2.015815\n",
      "Epoch : [3769/8999], Training Loss: 2.015680, Validation Loss: 2.015875\n",
      "Epoch : [3770/8999], Training Loss: 2.015579, Validation Loss: 2.016922\n",
      "Epoch : [3771/8999], Training Loss: 2.015923, Validation Loss: 2.016254\n",
      "Epoch : [3772/8999], Training Loss: 2.015851, Validation Loss: 2.016013\n",
      "Epoch : [3773/8999], Training Loss: 2.015819, Validation Loss: 2.016540\n",
      "Epoch : [3774/8999], Training Loss: 2.015577, Validation Loss: 2.016781\n",
      "Epoch : [3775/8999], Training Loss: 2.015628, Validation Loss: 2.015965\n",
      "Epoch : [3776/8999], Training Loss: 2.015641, Validation Loss: 2.016177\n",
      "Epoch : [3777/8999], Training Loss: 2.015568, Validation Loss: 2.016256\n",
      "Epoch : [3778/8999], Training Loss: 2.015716, Validation Loss: 2.016377\n",
      "Epoch : [3779/8999], Training Loss: 2.015658, Validation Loss: 2.016420\n",
      "Epoch : [3780/8999], Training Loss: 2.016001, Validation Loss: 2.016421\n",
      "Epoch : [3781/8999], Training Loss: 2.015685, Validation Loss: 2.016037\n",
      "Epoch : [3782/8999], Training Loss: 2.015594, Validation Loss: 2.016240\n",
      "Epoch : [3783/8999], Training Loss: 2.015848, Validation Loss: 2.016804\n",
      "Epoch : [3784/8999], Training Loss: 2.015732, Validation Loss: 2.016705\n",
      "Epoch : [3785/8999], Training Loss: 2.016104, Validation Loss: 2.016146\n",
      "Epoch : [3786/8999], Training Loss: 2.015692, Validation Loss: 2.015980\n",
      "Epoch : [3787/8999], Training Loss: 2.015611, Validation Loss: 2.016706\n",
      "Epoch : [3788/8999], Training Loss: 2.015734, Validation Loss: 2.016349\n",
      "Epoch : [3789/8999], Training Loss: 2.015730, Validation Loss: 2.015761\n",
      "Epoch : [3790/8999], Training Loss: 2.015686, Validation Loss: 2.016129\n",
      "Epoch : [3791/8999], Training Loss: 2.015742, Validation Loss: 2.016515\n",
      "Epoch : [3792/8999], Training Loss: 2.015886, Validation Loss: 2.015651\n",
      "Epoch : [3793/8999], Training Loss: 2.016086, Validation Loss: 2.016325\n",
      "Epoch : [3794/8999], Training Loss: 2.015803, Validation Loss: 2.015991\n",
      "Epoch : [3795/8999], Training Loss: 2.015660, Validation Loss: 2.016010\n",
      "Epoch : [3796/8999], Training Loss: 2.015539, Validation Loss: 2.016183\n",
      "Epoch : [3797/8999], Training Loss: 2.015622, Validation Loss: 2.016049\n",
      "Epoch : [3798/8999], Training Loss: 2.015595, Validation Loss: 2.016271\n",
      "Epoch : [3799/8999], Training Loss: 2.015769, Validation Loss: 2.015793\n",
      "Epoch : [3800/8999], Training Loss: 2.015596, Validation Loss: 2.015721\n",
      "Epoch : [3801/8999], Training Loss: 2.015598, Validation Loss: 2.016413\n",
      "Epoch : [3802/8999], Training Loss: 2.015745, Validation Loss: 2.016249\n",
      "Epoch : [3803/8999], Training Loss: 2.015892, Validation Loss: 2.015922\n",
      "Epoch : [3804/8999], Training Loss: 2.015727, Validation Loss: 2.016125\n",
      "Epoch : [3805/8999], Training Loss: 2.015670, Validation Loss: 2.015923\n",
      "Epoch : [3806/8999], Training Loss: 2.015689, Validation Loss: 2.016349\n",
      "Epoch : [3807/8999], Training Loss: 2.015769, Validation Loss: 2.016057\n",
      "Epoch : [3808/8999], Training Loss: 2.015547, Validation Loss: 2.015747\n",
      "Epoch : [3809/8999], Training Loss: 2.015603, Validation Loss: 2.016010\n",
      "Epoch : [3810/8999], Training Loss: 2.015709, Validation Loss: 2.016102\n",
      "Epoch : [3811/8999], Training Loss: 2.015779, Validation Loss: 2.015955\n",
      "Epoch : [3812/8999], Training Loss: 2.015722, Validation Loss: 2.015835\n",
      "Epoch : [3813/8999], Training Loss: 2.015615, Validation Loss: 2.015842\n",
      "Epoch : [3814/8999], Training Loss: 2.015648, Validation Loss: 2.016134\n",
      "Epoch : [3815/8999], Training Loss: 2.015835, Validation Loss: 2.015693\n",
      "Epoch : [3816/8999], Training Loss: 2.015761, Validation Loss: 2.016041\n",
      "Epoch : [3817/8999], Training Loss: 2.015808, Validation Loss: 2.015922\n",
      "Epoch : [3818/8999], Training Loss: 2.015905, Validation Loss: 2.015681\n",
      "Epoch : [3819/8999], Training Loss: 2.015745, Validation Loss: 2.015817\n",
      "Epoch : [3820/8999], Training Loss: 2.015787, Validation Loss: 2.015575\n",
      "Epoch : [3821/8999], Training Loss: 2.015631, Validation Loss: 2.015798\n",
      "Epoch : [3822/8999], Training Loss: 2.015668, Validation Loss: 2.015714\n",
      "Epoch : [3823/8999], Training Loss: 2.015551, Validation Loss: 2.015676\n",
      "Epoch : [3824/8999], Training Loss: 2.015672, Validation Loss: 2.016050\n",
      "Epoch : [3825/8999], Training Loss: 2.016001, Validation Loss: 2.015709\n",
      "Epoch : [3826/8999], Training Loss: 2.015599, Validation Loss: 2.016157\n",
      "Epoch : [3827/8999], Training Loss: 2.015953, Validation Loss: 2.015653\n",
      "Epoch : [3828/8999], Training Loss: 2.015771, Validation Loss: 2.016237\n",
      "Epoch : [3829/8999], Training Loss: 2.015805, Validation Loss: 2.015998\n",
      "Epoch : [3830/8999], Training Loss: 2.015682, Validation Loss: 2.015958\n",
      "Epoch : [3831/8999], Training Loss: 2.015692, Validation Loss: 2.015846\n",
      "Epoch : [3832/8999], Training Loss: 2.015856, Validation Loss: 2.015937\n",
      "Epoch : [3833/8999], Training Loss: 2.015781, Validation Loss: 2.016820\n",
      "Epoch : [3834/8999], Training Loss: 2.016079, Validation Loss: 2.016138\n",
      "Epoch : [3835/8999], Training Loss: 2.015817, Validation Loss: 2.017047\n",
      "Epoch : [3836/8999], Training Loss: 2.016122, Validation Loss: 2.016031\n",
      "Epoch : [3837/8999], Training Loss: 2.015744, Validation Loss: 2.016702\n",
      "Epoch : [3838/8999], Training Loss: 2.015866, Validation Loss: 2.016136\n",
      "Epoch : [3839/8999], Training Loss: 2.015704, Validation Loss: 2.016269\n",
      "Epoch : [3840/8999], Training Loss: 2.015765, Validation Loss: 2.016799\n",
      "Epoch : [3841/8999], Training Loss: 2.015728, Validation Loss: 2.016047\n",
      "Epoch : [3842/8999], Training Loss: 2.015622, Validation Loss: 2.016632\n",
      "Epoch : [3843/8999], Training Loss: 2.015796, Validation Loss: 2.017025\n",
      "Epoch : [3844/8999], Training Loss: 2.015974, Validation Loss: 2.016690\n",
      "Epoch : [3845/8999], Training Loss: 2.015769, Validation Loss: 2.016488\n",
      "Epoch : [3846/8999], Training Loss: 2.016228, Validation Loss: 2.017193\n",
      "Epoch : [3847/8999], Training Loss: 2.016286, Validation Loss: 2.017009\n",
      "Epoch : [3848/8999], Training Loss: 2.016478, Validation Loss: 2.017303\n",
      "Epoch : [3849/8999], Training Loss: 2.017125, Validation Loss: 2.016744\n",
      "Epoch : [3850/8999], Training Loss: 2.016800, Validation Loss: 2.016137\n",
      "Epoch : [3851/8999], Training Loss: 2.016485, Validation Loss: 2.016223\n",
      "Epoch : [3852/8999], Training Loss: 2.015986, Validation Loss: 2.015799\n",
      "Epoch : [3853/8999], Training Loss: 2.015971, Validation Loss: 2.016463\n",
      "Epoch : [3854/8999], Training Loss: 2.015834, Validation Loss: 2.016349\n",
      "Epoch : [3855/8999], Training Loss: 2.015723, Validation Loss: 2.015853\n",
      "Epoch : [3856/8999], Training Loss: 2.016045, Validation Loss: 2.016570\n",
      "Epoch : [3857/8999], Training Loss: 2.015804, Validation Loss: 2.015968\n",
      "Epoch : [3858/8999], Training Loss: 2.015680, Validation Loss: 2.016013\n",
      "Epoch : [3859/8999], Training Loss: 2.016141, Validation Loss: 2.016301\n",
      "Epoch : [3860/8999], Training Loss: 2.015917, Validation Loss: 2.015730\n",
      "Epoch : [3861/8999], Training Loss: 2.015588, Validation Loss: 2.015632\n",
      "Epoch : [3862/8999], Training Loss: 2.015786, Validation Loss: 2.015383\n",
      "Epoch : [3863/8999], Training Loss: 2.015630, Validation Loss: 2.015506\n",
      "Epoch : [3864/8999], Training Loss: 2.015623, Validation Loss: 2.015642\n",
      "Epoch : [3865/8999], Training Loss: 2.015525, Validation Loss: 2.015383\n",
      "Epoch : [3866/8999], Training Loss: 2.015614, Validation Loss: 2.015550\n",
      "Epoch : [3867/8999], Training Loss: 2.015522, Validation Loss: 2.015458\n",
      "Epoch : [3868/8999], Training Loss: 2.015461, Validation Loss: 2.015622\n",
      "Epoch : [3869/8999], Training Loss: 2.015805, Validation Loss: 2.015638\n",
      "Epoch : [3870/8999], Training Loss: 2.015566, Validation Loss: 2.015468\n",
      "Epoch : [3871/8999], Training Loss: 2.015504, Validation Loss: 2.015673\n",
      "Epoch : [3872/8999], Training Loss: 2.015526, Validation Loss: 2.015400\n",
      "Epoch : [3873/8999], Training Loss: 2.015533, Validation Loss: 2.015587\n",
      "Epoch : [3874/8999], Training Loss: 2.015481, Validation Loss: 2.015441\n",
      "Epoch : [3875/8999], Training Loss: 2.015322, Validation Loss: 2.015438\n",
      "Epoch : [3876/8999], Training Loss: 2.015465, Validation Loss: 2.015448\n",
      "Epoch : [3877/8999], Training Loss: 2.015452, Validation Loss: 2.015314\n",
      "Epoch : [3878/8999], Training Loss: 2.015438, Validation Loss: 2.015344\n",
      "Epoch : [3879/8999], Training Loss: 2.015391, Validation Loss: 2.015340\n",
      "Epoch : [3880/8999], Training Loss: 2.015508, Validation Loss: 2.015491\n",
      "Epoch : [3881/8999], Training Loss: 2.015594, Validation Loss: 2.015509\n",
      "Epoch : [3882/8999], Training Loss: 2.015384, Validation Loss: 2.015287\n",
      "Epoch : [3883/8999], Training Loss: 2.015389, Validation Loss: 2.015427\n",
      "Epoch : [3884/8999], Training Loss: 2.015435, Validation Loss: 2.015888\n",
      "Epoch : [3885/8999], Training Loss: 2.015468, Validation Loss: 2.015314\n",
      "Epoch : [3886/8999], Training Loss: 2.015410, Validation Loss: 2.015278\n",
      "Epoch : [3887/8999], Training Loss: 2.015308, Validation Loss: 2.015269\n",
      "Epoch : [3888/8999], Training Loss: 2.015271, Validation Loss: 2.015245\n",
      "Epoch : [3889/8999], Training Loss: 2.015281, Validation Loss: 2.015126\n",
      "Epoch : [3890/8999], Training Loss: 2.015288, Validation Loss: 2.015125\n",
      "Epoch : [3891/8999], Training Loss: 2.015229, Validation Loss: 2.015223\n",
      "Epoch : [3892/8999], Training Loss: 2.015206, Validation Loss: 2.015507\n",
      "Epoch : [3893/8999], Training Loss: 2.015561, Validation Loss: 2.015396\n",
      "Epoch : [3894/8999], Training Loss: 2.015415, Validation Loss: 2.015382\n",
      "Epoch : [3895/8999], Training Loss: 2.015376, Validation Loss: 2.015539\n",
      "Epoch : [3896/8999], Training Loss: 2.015457, Validation Loss: 2.015388\n",
      "Epoch : [3897/8999], Training Loss: 2.015333, Validation Loss: 2.015215\n",
      "Epoch : [3898/8999], Training Loss: 2.015387, Validation Loss: 2.015363\n",
      "Epoch : [3899/8999], Training Loss: 2.015343, Validation Loss: 2.015443\n",
      "Epoch : [3900/8999], Training Loss: 2.015549, Validation Loss: 2.015506\n",
      "Epoch : [3901/8999], Training Loss: 2.015504, Validation Loss: 2.015369\n",
      "Epoch : [3902/8999], Training Loss: 2.015437, Validation Loss: 2.015463\n",
      "Epoch : [3903/8999], Training Loss: 2.015323, Validation Loss: 2.015319\n",
      "Epoch : [3904/8999], Training Loss: 2.015486, Validation Loss: 2.015219\n",
      "Epoch : [3905/8999], Training Loss: 2.015180, Validation Loss: 2.015099\n",
      "Epoch : [3906/8999], Training Loss: 2.015151, Validation Loss: 2.015548\n",
      "Epoch : [3907/8999], Training Loss: 2.015333, Validation Loss: 2.015567\n",
      "Epoch : [3908/8999], Training Loss: 2.015378, Validation Loss: 2.015251\n",
      "Epoch : [3909/8999], Training Loss: 2.015442, Validation Loss: 2.015576\n",
      "Epoch : [3910/8999], Training Loss: 2.015537, Validation Loss: 2.015603\n",
      "Epoch : [3911/8999], Training Loss: 2.015431, Validation Loss: 2.015333\n",
      "Epoch : [3912/8999], Training Loss: 2.015417, Validation Loss: 2.015397\n",
      "Epoch : [3913/8999], Training Loss: 2.015365, Validation Loss: 2.015383\n",
      "Epoch : [3914/8999], Training Loss: 2.015238, Validation Loss: 2.015366\n",
      "Epoch : [3915/8999], Training Loss: 2.015349, Validation Loss: 2.015433\n",
      "Epoch : [3916/8999], Training Loss: 2.015352, Validation Loss: 2.015450\n",
      "Epoch : [3917/8999], Training Loss: 2.015304, Validation Loss: 2.015424\n",
      "Epoch : [3918/8999], Training Loss: 2.015537, Validation Loss: 2.015713\n",
      "Epoch : [3919/8999], Training Loss: 2.015400, Validation Loss: 2.015338\n",
      "Epoch : [3920/8999], Training Loss: 2.015325, Validation Loss: 2.015425\n",
      "Epoch : [3921/8999], Training Loss: 2.015407, Validation Loss: 2.015231\n",
      "Epoch : [3922/8999], Training Loss: 2.015394, Validation Loss: 2.015863\n",
      "Epoch : [3923/8999], Training Loss: 2.015536, Validation Loss: 2.015452\n",
      "Epoch : [3924/8999], Training Loss: 2.015399, Validation Loss: 2.015225\n",
      "Epoch : [3925/8999], Training Loss: 2.015337, Validation Loss: 2.015303\n",
      "Epoch : [3926/8999], Training Loss: 2.015384, Validation Loss: 2.015918\n",
      "Epoch : [3927/8999], Training Loss: 2.015594, Validation Loss: 2.015398\n",
      "Epoch : [3928/8999], Training Loss: 2.015312, Validation Loss: 2.015261\n",
      "Epoch : [3929/8999], Training Loss: 2.015315, Validation Loss: 2.015552\n",
      "Epoch : [3930/8999], Training Loss: 2.015575, Validation Loss: 2.015533\n",
      "Epoch : [3931/8999], Training Loss: 2.015279, Validation Loss: 2.015175\n",
      "Epoch : [3932/8999], Training Loss: 2.015345, Validation Loss: 2.015399\n",
      "Epoch : [3933/8999], Training Loss: 2.015473, Validation Loss: 2.015564\n",
      "Epoch : [3934/8999], Training Loss: 2.015451, Validation Loss: 2.015500\n",
      "Epoch : [3935/8999], Training Loss: 2.015388, Validation Loss: 2.015421\n",
      "Epoch : [3936/8999], Training Loss: 2.015428, Validation Loss: 2.015327\n",
      "Epoch : [3937/8999], Training Loss: 2.015283, Validation Loss: 2.015457\n",
      "Epoch : [3938/8999], Training Loss: 2.015414, Validation Loss: 2.015178\n",
      "Epoch : [3939/8999], Training Loss: 2.015412, Validation Loss: 2.015440\n",
      "Epoch : [3940/8999], Training Loss: 2.015506, Validation Loss: 2.015513\n",
      "Epoch : [3941/8999], Training Loss: 2.015236, Validation Loss: 2.015296\n",
      "Epoch : [3942/8999], Training Loss: 2.015323, Validation Loss: 2.015162\n",
      "Epoch : [3943/8999], Training Loss: 2.015292, Validation Loss: 2.015408\n",
      "Epoch : [3944/8999], Training Loss: 2.015251, Validation Loss: 2.015255\n",
      "Epoch : [3945/8999], Training Loss: 2.015252, Validation Loss: 2.015834\n",
      "Epoch : [3946/8999], Training Loss: 2.015405, Validation Loss: 2.015312\n",
      "Epoch : [3947/8999], Training Loss: 2.015317, Validation Loss: 2.015469\n",
      "Epoch : [3948/8999], Training Loss: 2.015265, Validation Loss: 2.015444\n",
      "Epoch : [3949/8999], Training Loss: 2.015348, Validation Loss: 2.015337\n",
      "Epoch : [3950/8999], Training Loss: 2.015285, Validation Loss: 2.015356\n",
      "Epoch : [3951/8999], Training Loss: 2.015196, Validation Loss: 2.015326\n",
      "Epoch : [3952/8999], Training Loss: 2.015391, Validation Loss: 2.015469\n",
      "Epoch : [3953/8999], Training Loss: 2.015448, Validation Loss: 2.015454\n",
      "Epoch : [3954/8999], Training Loss: 2.015334, Validation Loss: 2.015688\n",
      "Epoch : [3955/8999], Training Loss: 2.015425, Validation Loss: 2.015537\n",
      "Epoch : [3956/8999], Training Loss: 2.015367, Validation Loss: 2.015264\n",
      "Epoch : [3957/8999], Training Loss: 2.015295, Validation Loss: 2.015392\n",
      "Epoch : [3958/8999], Training Loss: 2.015286, Validation Loss: 2.015367\n",
      "Epoch : [3959/8999], Training Loss: 2.015306, Validation Loss: 2.015522\n",
      "Epoch : [3960/8999], Training Loss: 2.015297, Validation Loss: 2.015569\n",
      "Epoch : [3961/8999], Training Loss: 2.015487, Validation Loss: 2.015228\n",
      "Epoch : [3962/8999], Training Loss: 2.015449, Validation Loss: 2.015617\n",
      "Epoch : [3963/8999], Training Loss: 2.015463, Validation Loss: 2.015833\n",
      "Epoch : [3964/8999], Training Loss: 2.015393, Validation Loss: 2.015219\n",
      "Epoch : [3965/8999], Training Loss: 2.015393, Validation Loss: 2.015476\n",
      "Epoch : [3966/8999], Training Loss: 2.015298, Validation Loss: 2.015233\n",
      "Epoch : [3967/8999], Training Loss: 2.015350, Validation Loss: 2.015163\n",
      "Epoch : [3968/8999], Training Loss: 2.015331, Validation Loss: 2.015574\n",
      "Epoch : [3969/8999], Training Loss: 2.015346, Validation Loss: 2.015355\n",
      "Epoch : [3970/8999], Training Loss: 2.015293, Validation Loss: 2.015468\n",
      "Epoch : [3971/8999], Training Loss: 2.015433, Validation Loss: 2.015277\n",
      "Epoch : [3972/8999], Training Loss: 2.015333, Validation Loss: 2.015361\n",
      "Epoch : [3973/8999], Training Loss: 2.015363, Validation Loss: 2.015391\n",
      "Epoch : [3974/8999], Training Loss: 2.015364, Validation Loss: 2.015326\n",
      "Epoch : [3975/8999], Training Loss: 2.015529, Validation Loss: 2.015356\n",
      "Epoch : [3976/8999], Training Loss: 2.015375, Validation Loss: 2.015446\n",
      "Epoch : [3977/8999], Training Loss: 2.015356, Validation Loss: 2.015219\n",
      "Epoch : [3978/8999], Training Loss: 2.015275, Validation Loss: 2.015316\n",
      "Epoch : [3979/8999], Training Loss: 2.015360, Validation Loss: 2.015497\n",
      "Epoch : [3980/8999], Training Loss: 2.015524, Validation Loss: 2.015659\n",
      "Epoch : [3981/8999], Training Loss: 2.015326, Validation Loss: 2.015223\n",
      "Epoch : [3982/8999], Training Loss: 2.015294, Validation Loss: 2.015376\n",
      "Epoch : [3983/8999], Training Loss: 2.015401, Validation Loss: 2.015326\n",
      "Epoch : [3984/8999], Training Loss: 2.015328, Validation Loss: 2.015681\n",
      "Epoch : [3985/8999], Training Loss: 2.015332, Validation Loss: 2.015311\n",
      "Epoch : [3986/8999], Training Loss: 2.015405, Validation Loss: 2.015291\n",
      "Epoch : [3987/8999], Training Loss: 2.015285, Validation Loss: 2.015456\n",
      "Epoch : [3988/8999], Training Loss: 2.015204, Validation Loss: 2.015325\n",
      "Epoch : [3989/8999], Training Loss: 2.015338, Validation Loss: 2.015272\n",
      "Epoch : [3990/8999], Training Loss: 2.015349, Validation Loss: 2.015308\n",
      "Epoch : [3991/8999], Training Loss: 2.015370, Validation Loss: 2.015426\n",
      "Epoch : [3992/8999], Training Loss: 2.015269, Validation Loss: 2.015268\n",
      "Epoch : [3993/8999], Training Loss: 2.015425, Validation Loss: 2.015648\n",
      "Epoch : [3994/8999], Training Loss: 2.015607, Validation Loss: 2.015579\n",
      "Epoch : [3995/8999], Training Loss: 2.015457, Validation Loss: 2.015289\n",
      "Epoch : [3996/8999], Training Loss: 2.015464, Validation Loss: 2.015632\n",
      "Epoch : [3997/8999], Training Loss: 2.015363, Validation Loss: 2.015379\n",
      "Epoch : [3998/8999], Training Loss: 2.015302, Validation Loss: 2.015121\n",
      "Epoch : [3999/8999], Training Loss: 2.015335, Validation Loss: 2.015438\n",
      "Epoch : [4000/8999], Training Loss: 2.015296, Validation Loss: 2.015193\n",
      "Epoch : [4001/8999], Training Loss: 2.015167, Validation Loss: 2.015124\n",
      "Epoch : [4002/8999], Training Loss: 2.015399, Validation Loss: 2.015613\n",
      "Epoch : [4003/8999], Training Loss: 2.015402, Validation Loss: 2.015575\n",
      "Epoch : [4004/8999], Training Loss: 2.015387, Validation Loss: 2.015348\n",
      "Epoch : [4005/8999], Training Loss: 2.015118, Validation Loss: 2.015365\n",
      "Epoch : [4006/8999], Training Loss: 2.015210, Validation Loss: 2.015189\n",
      "Epoch : [4007/8999], Training Loss: 2.015306, Validation Loss: 2.015314\n",
      "Epoch : [4008/8999], Training Loss: 2.015418, Validation Loss: 2.015525\n",
      "Epoch : [4009/8999], Training Loss: 2.015464, Validation Loss: 2.015582\n",
      "Epoch : [4010/8999], Training Loss: 2.015309, Validation Loss: 2.015161\n",
      "Epoch : [4011/8999], Training Loss: 2.015281, Validation Loss: 2.015194\n",
      "Epoch : [4012/8999], Training Loss: 2.015200, Validation Loss: 2.015378\n",
      "Epoch : [4013/8999], Training Loss: 2.015271, Validation Loss: 2.015517\n",
      "Epoch : [4014/8999], Training Loss: 2.015367, Validation Loss: 2.015290\n",
      "Epoch : [4015/8999], Training Loss: 2.015279, Validation Loss: 2.015898\n",
      "Epoch : [4016/8999], Training Loss: 2.015231, Validation Loss: 2.015318\n",
      "Epoch : [4017/8999], Training Loss: 2.015294, Validation Loss: 2.015242\n",
      "Epoch : [4018/8999], Training Loss: 2.015347, Validation Loss: 2.015910\n",
      "Epoch : [4019/8999], Training Loss: 2.015317, Validation Loss: 2.015305\n",
      "Epoch : [4020/8999], Training Loss: 2.015342, Validation Loss: 2.015461\n",
      "Epoch : [4021/8999], Training Loss: 2.015393, Validation Loss: 2.015209\n",
      "Epoch : [4022/8999], Training Loss: 2.015209, Validation Loss: 2.015141\n",
      "Epoch : [4023/8999], Training Loss: 2.015334, Validation Loss: 2.015906\n",
      "Epoch : [4024/8999], Training Loss: 2.015253, Validation Loss: 2.015254\n",
      "Epoch : [4025/8999], Training Loss: 2.015196, Validation Loss: 2.015304\n",
      "Epoch : [4026/8999], Training Loss: 2.015230, Validation Loss: 2.015027\n",
      "Epoch : [4027/8999], Training Loss: 2.015178, Validation Loss: 2.015553\n",
      "Epoch : [4028/8999], Training Loss: 2.015245, Validation Loss: 2.015195\n",
      "Epoch : [4029/8999], Training Loss: 2.015162, Validation Loss: 2.015201\n",
      "Epoch : [4030/8999], Training Loss: 2.015459, Validation Loss: 2.015664\n",
      "Epoch : [4031/8999], Training Loss: 2.015353, Validation Loss: 2.015458\n",
      "Epoch : [4032/8999], Training Loss: 2.015239, Validation Loss: 2.015298\n",
      "Epoch : [4033/8999], Training Loss: 2.015329, Validation Loss: 2.015269\n",
      "Epoch : [4034/8999], Training Loss: 2.015328, Validation Loss: 2.015400\n",
      "Epoch : [4035/8999], Training Loss: 2.015566, Validation Loss: 2.015610\n",
      "Epoch : [4036/8999], Training Loss: 2.015261, Validation Loss: 2.015311\n",
      "Epoch : [4037/8999], Training Loss: 2.015060, Validation Loss: 2.015076\n",
      "Epoch : [4038/8999], Training Loss: 2.015142, Validation Loss: 2.015397\n",
      "Epoch : [4039/8999], Training Loss: 2.015227, Validation Loss: 2.015035\n",
      "Epoch : [4040/8999], Training Loss: 2.015235, Validation Loss: 2.015859\n",
      "Epoch : [4041/8999], Training Loss: 2.015283, Validation Loss: 2.015133\n",
      "Epoch : [4042/8999], Training Loss: 2.015066, Validation Loss: 2.015111\n",
      "Epoch : [4043/8999], Training Loss: 2.015109, Validation Loss: 2.015245\n",
      "Epoch : [4044/8999], Training Loss: 2.015041, Validation Loss: 2.015476\n",
      "Epoch : [4045/8999], Training Loss: 2.015164, Validation Loss: 2.015276\n",
      "Epoch : [4046/8999], Training Loss: 2.015337, Validation Loss: 2.015093\n",
      "Epoch : [4047/8999], Training Loss: 2.015202, Validation Loss: 2.015324\n",
      "Epoch : [4048/8999], Training Loss: 2.015234, Validation Loss: 2.015256\n",
      "Epoch : [4049/8999], Training Loss: 2.015112, Validation Loss: 2.015331\n",
      "Epoch : [4050/8999], Training Loss: 2.015207, Validation Loss: 2.015193\n",
      "Epoch : [4051/8999], Training Loss: 2.015290, Validation Loss: 2.015303\n",
      "Epoch : [4052/8999], Training Loss: 2.015141, Validation Loss: 2.015425\n",
      "Epoch : [4053/8999], Training Loss: 2.015150, Validation Loss: 2.015190\n",
      "Epoch : [4054/8999], Training Loss: 2.015211, Validation Loss: 2.015329\n",
      "Epoch : [4055/8999], Training Loss: 2.015185, Validation Loss: 2.015128\n",
      "Epoch : [4056/8999], Training Loss: 2.015244, Validation Loss: 2.015166\n",
      "Epoch : [4057/8999], Training Loss: 2.015271, Validation Loss: 2.015444\n",
      "Epoch : [4058/8999], Training Loss: 2.015332, Validation Loss: 2.015320\n",
      "Epoch : [4059/8999], Training Loss: 2.015294, Validation Loss: 2.015201\n",
      "Epoch : [4060/8999], Training Loss: 2.015157, Validation Loss: 2.015244\n",
      "Epoch : [4061/8999], Training Loss: 2.015163, Validation Loss: 2.015236\n",
      "Epoch : [4062/8999], Training Loss: 2.015164, Validation Loss: 2.015215\n",
      "Epoch : [4063/8999], Training Loss: 2.015158, Validation Loss: 2.015230\n",
      "Epoch : [4064/8999], Training Loss: 2.015213, Validation Loss: 2.015213\n",
      "Epoch : [4065/8999], Training Loss: 2.015138, Validation Loss: 2.015342\n",
      "Epoch : [4066/8999], Training Loss: 2.015140, Validation Loss: 2.015223\n",
      "Epoch : [4067/8999], Training Loss: 2.015131, Validation Loss: 2.014998\n",
      "Epoch : [4068/8999], Training Loss: 2.015145, Validation Loss: 2.015298\n",
      "Epoch : [4069/8999], Training Loss: 2.015177, Validation Loss: 2.015174\n",
      "Epoch : [4070/8999], Training Loss: 2.015061, Validation Loss: 2.015181\n",
      "Epoch : [4071/8999], Training Loss: 2.015085, Validation Loss: 2.015744\n",
      "Epoch : [4072/8999], Training Loss: 2.015485, Validation Loss: 2.015146\n",
      "Epoch : [4073/8999], Training Loss: 2.015493, Validation Loss: 2.015530\n",
      "Epoch : [4074/8999], Training Loss: 2.015251, Validation Loss: 2.015500\n",
      "Epoch : [4075/8999], Training Loss: 2.015484, Validation Loss: 2.015202\n",
      "Epoch : [4076/8999], Training Loss: 2.015210, Validation Loss: 2.015343\n",
      "Epoch : [4077/8999], Training Loss: 2.015334, Validation Loss: 2.015677\n",
      "Epoch : [4078/8999], Training Loss: 2.015127, Validation Loss: 2.015250\n",
      "Epoch : [4079/8999], Training Loss: 2.015092, Validation Loss: 2.014986\n",
      "Epoch : [4080/8999], Training Loss: 2.015237, Validation Loss: 2.015418\n",
      "Epoch : [4081/8999], Training Loss: 2.015229, Validation Loss: 2.015335\n",
      "Epoch : [4082/8999], Training Loss: 2.015209, Validation Loss: 2.015340\n",
      "Epoch : [4083/8999], Training Loss: 2.015339, Validation Loss: 2.015263\n",
      "Epoch : [4084/8999], Training Loss: 2.015243, Validation Loss: 2.014979\n",
      "Epoch : [4085/8999], Training Loss: 2.015135, Validation Loss: 2.015191\n",
      "Epoch : [4086/8999], Training Loss: 2.015136, Validation Loss: 2.015763\n",
      "Epoch : [4087/8999], Training Loss: 2.015211, Validation Loss: 2.015239\n",
      "Epoch : [4088/8999], Training Loss: 2.015264, Validation Loss: 2.015390\n",
      "Epoch : [4089/8999], Training Loss: 2.015093, Validation Loss: 2.015037\n",
      "Epoch : [4090/8999], Training Loss: 2.015083, Validation Loss: 2.015225\n",
      "Epoch : [4091/8999], Training Loss: 2.015043, Validation Loss: 2.015916\n",
      "Epoch : [4092/8999], Training Loss: 2.015147, Validation Loss: 2.015769\n",
      "Epoch : [4093/8999], Training Loss: 2.015194, Validation Loss: 2.015112\n",
      "Epoch : [4094/8999], Training Loss: 2.015262, Validation Loss: 2.015400\n",
      "Epoch : [4095/8999], Training Loss: 2.015407, Validation Loss: 2.015774\n",
      "Epoch : [4096/8999], Training Loss: 2.015288, Validation Loss: 2.015178\n",
      "Epoch : [4097/8999], Training Loss: 2.015336, Validation Loss: 2.015340\n",
      "Epoch : [4098/8999], Training Loss: 2.015183, Validation Loss: 2.015107\n",
      "Epoch : [4099/8999], Training Loss: 2.015308, Validation Loss: 2.015489\n",
      "Epoch : [4100/8999], Training Loss: 2.015255, Validation Loss: 2.015330\n",
      "Epoch : [4101/8999], Training Loss: 2.015242, Validation Loss: 2.015169\n",
      "Epoch : [4102/8999], Training Loss: 2.015282, Validation Loss: 2.015719\n",
      "Epoch : [4103/8999], Training Loss: 2.015474, Validation Loss: 2.016452\n",
      "Epoch : [4104/8999], Training Loss: 2.015591, Validation Loss: 2.015498\n",
      "Epoch : [4105/8999], Training Loss: 2.015311, Validation Loss: 2.015764\n",
      "Epoch : [4106/8999], Training Loss: 2.015280, Validation Loss: 2.015457\n",
      "Epoch : [4107/8999], Training Loss: 2.015136, Validation Loss: 2.015494\n",
      "Epoch : [4108/8999], Training Loss: 2.015091, Validation Loss: 2.015708\n",
      "Epoch : [4109/8999], Training Loss: 2.015054, Validation Loss: 2.015296\n",
      "Epoch : [4110/8999], Training Loss: 2.015164, Validation Loss: 2.015272\n",
      "Epoch : [4111/8999], Training Loss: 2.015200, Validation Loss: 2.015338\n",
      "Epoch : [4112/8999], Training Loss: 2.015311, Validation Loss: 2.015318\n",
      "Epoch : [4113/8999], Training Loss: 2.015150, Validation Loss: 2.015086\n",
      "Epoch : [4114/8999], Training Loss: 2.015064, Validation Loss: 2.015205\n",
      "Epoch : [4115/8999], Training Loss: 2.015115, Validation Loss: 2.015660\n",
      "Epoch : [4116/8999], Training Loss: 2.015173, Validation Loss: 2.015715\n",
      "Epoch : [4117/8999], Training Loss: 2.015315, Validation Loss: 2.015445\n",
      "Epoch : [4118/8999], Training Loss: 2.015310, Validation Loss: 2.015166\n",
      "Epoch : [4119/8999], Training Loss: 2.015456, Validation Loss: 2.016116\n",
      "Epoch : [4120/8999], Training Loss: 2.015565, Validation Loss: 2.015874\n",
      "Epoch : [4121/8999], Training Loss: 2.015227, Validation Loss: 2.015492\n",
      "Epoch : [4122/8999], Training Loss: 2.015184, Validation Loss: 2.015631\n",
      "Epoch : [4123/8999], Training Loss: 2.015297, Validation Loss: 2.015540\n",
      "Epoch : [4124/8999], Training Loss: 2.015603, Validation Loss: 2.016758\n",
      "Epoch : [4125/8999], Training Loss: 2.015608, Validation Loss: 2.015827\n",
      "Epoch : [4126/8999], Training Loss: 2.015348, Validation Loss: 2.015361\n",
      "Epoch : [4127/8999], Training Loss: 2.015511, Validation Loss: 2.016225\n",
      "Epoch : [4128/8999], Training Loss: 2.015453, Validation Loss: 2.015744\n",
      "Epoch : [4129/8999], Training Loss: 2.015217, Validation Loss: 2.015638\n",
      "Epoch : [4130/8999], Training Loss: 2.015268, Validation Loss: 2.015136\n",
      "Epoch : [4131/8999], Training Loss: 2.015358, Validation Loss: 2.015609\n",
      "Epoch : [4132/8999], Training Loss: 2.015373, Validation Loss: 2.015928\n",
      "Epoch : [4133/8999], Training Loss: 2.015302, Validation Loss: 2.015551\n",
      "Epoch : [4134/8999], Training Loss: 2.015300, Validation Loss: 2.015893\n",
      "Epoch : [4135/8999], Training Loss: 2.015145, Validation Loss: 2.015508\n",
      "Epoch : [4136/8999], Training Loss: 2.015152, Validation Loss: 2.015361\n",
      "Epoch : [4137/8999], Training Loss: 2.015240, Validation Loss: 2.015681\n",
      "Epoch : [4138/8999], Training Loss: 2.015296, Validation Loss: 2.016310\n",
      "Epoch : [4139/8999], Training Loss: 2.015503, Validation Loss: 2.015646\n",
      "Epoch : [4140/8999], Training Loss: 2.015465, Validation Loss: 2.015506\n",
      "Epoch : [4141/8999], Training Loss: 2.015392, Validation Loss: 2.015505\n",
      "Epoch : [4142/8999], Training Loss: 2.015260, Validation Loss: 2.015685\n",
      "Epoch : [4143/8999], Training Loss: 2.015284, Validation Loss: 2.015503\n",
      "Epoch : [4144/8999], Training Loss: 2.015167, Validation Loss: 2.015144\n",
      "Epoch : [4145/8999], Training Loss: 2.015108, Validation Loss: 2.015381\n",
      "Epoch : [4146/8999], Training Loss: 2.015299, Validation Loss: 2.015618\n",
      "Epoch : [4147/8999], Training Loss: 2.015317, Validation Loss: 2.015916\n",
      "Epoch : [4148/8999], Training Loss: 2.015233, Validation Loss: 2.015319\n",
      "Epoch : [4149/8999], Training Loss: 2.015222, Validation Loss: 2.015139\n",
      "Epoch : [4150/8999], Training Loss: 2.015141, Validation Loss: 2.015794\n",
      "Epoch : [4151/8999], Training Loss: 2.015243, Validation Loss: 2.015790\n",
      "Epoch : [4152/8999], Training Loss: 2.015191, Validation Loss: 2.015296\n",
      "Epoch : [4153/8999], Training Loss: 2.015130, Validation Loss: 2.015171\n",
      "Epoch : [4154/8999], Training Loss: 2.015130, Validation Loss: 2.015111\n",
      "Epoch : [4155/8999], Training Loss: 2.015069, Validation Loss: 2.015534\n",
      "Epoch : [4156/8999], Training Loss: 2.015174, Validation Loss: 2.015688\n",
      "Epoch : [4157/8999], Training Loss: 2.015128, Validation Loss: 2.015402\n",
      "Epoch : [4158/8999], Training Loss: 2.015203, Validation Loss: 2.015056\n",
      "Epoch : [4159/8999], Training Loss: 2.015050, Validation Loss: 2.015437\n",
      "Epoch : [4160/8999], Training Loss: 2.015218, Validation Loss: 2.015465\n",
      "Epoch : [4161/8999], Training Loss: 2.015106, Validation Loss: 2.015309\n",
      "Epoch : [4162/8999], Training Loss: 2.015174, Validation Loss: 2.015279\n",
      "Epoch : [4163/8999], Training Loss: 2.015289, Validation Loss: 2.015365\n",
      "Epoch : [4164/8999], Training Loss: 2.015210, Validation Loss: 2.015371\n",
      "Epoch : [4165/8999], Training Loss: 2.015188, Validation Loss: 2.015648\n",
      "Epoch : [4166/8999], Training Loss: 2.015250, Validation Loss: 2.015451\n",
      "Epoch : [4167/8999], Training Loss: 2.015154, Validation Loss: 2.015232\n",
      "Epoch : [4168/8999], Training Loss: 2.015240, Validation Loss: 2.015424\n",
      "Epoch : [4169/8999], Training Loss: 2.015246, Validation Loss: 2.015668\n",
      "Epoch : [4170/8999], Training Loss: 2.015300, Validation Loss: 2.015406\n",
      "Epoch : [4171/8999], Training Loss: 2.015220, Validation Loss: 2.015258\n",
      "Epoch : [4172/8999], Training Loss: 2.015209, Validation Loss: 2.015249\n",
      "Epoch : [4173/8999], Training Loss: 2.015213, Validation Loss: 2.015316\n",
      "Epoch : [4174/8999], Training Loss: 2.015145, Validation Loss: 2.015232\n",
      "Epoch : [4175/8999], Training Loss: 2.015081, Validation Loss: 2.015383\n",
      "Epoch : [4176/8999], Training Loss: 2.015288, Validation Loss: 2.015118\n",
      "Epoch : [4177/8999], Training Loss: 2.015074, Validation Loss: 2.015616\n",
      "Epoch : [4178/8999], Training Loss: 2.015386, Validation Loss: 2.015406\n",
      "Epoch : [4179/8999], Training Loss: 2.015341, Validation Loss: 2.015515\n",
      "Epoch : [4180/8999], Training Loss: 2.015323, Validation Loss: 2.015267\n",
      "Epoch : [4181/8999], Training Loss: 2.015393, Validation Loss: 2.016541\n",
      "Epoch : [4182/8999], Training Loss: 2.015934, Validation Loss: 2.015440\n",
      "Epoch : [4183/8999], Training Loss: 2.015375, Validation Loss: 2.015412\n",
      "Epoch : [4184/8999], Training Loss: 2.015161, Validation Loss: 2.015212\n",
      "Epoch : [4185/8999], Training Loss: 2.015281, Validation Loss: 2.015303\n",
      "Epoch : [4186/8999], Training Loss: 2.015268, Validation Loss: 2.015614\n",
      "Epoch : [4187/8999], Training Loss: 2.015160, Validation Loss: 2.015217\n",
      "Epoch : [4188/8999], Training Loss: 2.015114, Validation Loss: 2.015269\n",
      "Epoch : [4189/8999], Training Loss: 2.015255, Validation Loss: 2.015147\n",
      "Epoch : [4190/8999], Training Loss: 2.015090, Validation Loss: 2.015508\n",
      "Epoch : [4191/8999], Training Loss: 2.015094, Validation Loss: 2.015296\n",
      "Epoch : [4192/8999], Training Loss: 2.015090, Validation Loss: 2.015282\n",
      "Epoch : [4193/8999], Training Loss: 2.015428, Validation Loss: 2.015713\n",
      "Epoch : [4194/8999], Training Loss: 2.015365, Validation Loss: 2.016348\n",
      "Epoch : [4195/8999], Training Loss: 2.015501, Validation Loss: 2.015731\n",
      "Epoch : [4196/8999], Training Loss: 2.015262, Validation Loss: 2.015518\n",
      "Epoch : [4197/8999], Training Loss: 2.015310, Validation Loss: 2.016153\n",
      "Epoch : [4198/8999], Training Loss: 2.015171, Validation Loss: 2.015483\n",
      "Epoch : [4199/8999], Training Loss: 2.015330, Validation Loss: 2.015319\n",
      "Epoch : [4200/8999], Training Loss: 2.015123, Validation Loss: 2.015581\n",
      "Epoch : [4201/8999], Training Loss: 2.015441, Validation Loss: 2.016323\n",
      "Epoch : [4202/8999], Training Loss: 2.015365, Validation Loss: 2.015800\n",
      "Epoch : [4203/8999], Training Loss: 2.015340, Validation Loss: 2.015806\n",
      "Epoch : [4204/8999], Training Loss: 2.015513, Validation Loss: 2.016262\n",
      "Epoch : [4205/8999], Training Loss: 2.015274, Validation Loss: 2.016152\n",
      "Epoch : [4206/8999], Training Loss: 2.015376, Validation Loss: 2.015830\n",
      "Epoch : [4207/8999], Training Loss: 2.015401, Validation Loss: 2.016286\n",
      "Epoch : [4208/8999], Training Loss: 2.015421, Validation Loss: 2.016918\n",
      "Epoch : [4209/8999], Training Loss: 2.015413, Validation Loss: 2.015763\n",
      "Epoch : [4210/8999], Training Loss: 2.015380, Validation Loss: 2.015822\n",
      "Epoch : [4211/8999], Training Loss: 2.015326, Validation Loss: 2.016435\n",
      "Epoch : [4212/8999], Training Loss: 2.015384, Validation Loss: 2.016436\n",
      "Epoch : [4213/8999], Training Loss: 2.015435, Validation Loss: 2.016820\n",
      "Epoch : [4214/8999], Training Loss: 2.015673, Validation Loss: 2.016769\n",
      "Epoch : [4215/8999], Training Loss: 2.016173, Validation Loss: 2.017464\n",
      "Epoch : [4216/8999], Training Loss: 2.015928, Validation Loss: 2.016599\n",
      "Epoch : [4217/8999], Training Loss: 2.016778, Validation Loss: 2.016966\n",
      "Epoch : [4218/8999], Training Loss: 2.016381, Validation Loss: 2.017071\n",
      "Epoch : [4219/8999], Training Loss: 2.015863, Validation Loss: 2.016807\n",
      "Epoch : [4220/8999], Training Loss: 2.016102, Validation Loss: 2.016936\n",
      "Epoch : [4221/8999], Training Loss: 2.016302, Validation Loss: 2.016911\n",
      "Epoch : [4222/8999], Training Loss: 2.016342, Validation Loss: 2.016920\n",
      "Epoch : [4223/8999], Training Loss: 2.016481, Validation Loss: 2.016246\n",
      "Epoch : [4224/8999], Training Loss: 2.016728, Validation Loss: 2.015834\n",
      "Epoch : [4225/8999], Training Loss: 2.015890, Validation Loss: 2.016371\n",
      "Epoch : [4226/8999], Training Loss: 2.015943, Validation Loss: 2.015707\n",
      "Epoch : [4227/8999], Training Loss: 2.015658, Validation Loss: 2.015842\n",
      "Epoch : [4228/8999], Training Loss: 2.015597, Validation Loss: 2.016000\n",
      "Epoch : [4229/8999], Training Loss: 2.015779, Validation Loss: 2.016218\n",
      "Epoch : [4230/8999], Training Loss: 2.015856, Validation Loss: 2.015605\n",
      "Epoch : [4231/8999], Training Loss: 2.015482, Validation Loss: 2.015488\n",
      "Epoch : [4232/8999], Training Loss: 2.015728, Validation Loss: 2.015522\n",
      "Epoch : [4233/8999], Training Loss: 2.015302, Validation Loss: 2.015451\n",
      "Epoch : [4234/8999], Training Loss: 2.015377, Validation Loss: 2.015662\n",
      "Epoch : [4235/8999], Training Loss: 2.015480, Validation Loss: 2.015681\n",
      "Epoch : [4236/8999], Training Loss: 2.015435, Validation Loss: 2.015596\n",
      "Epoch : [4237/8999], Training Loss: 2.015463, Validation Loss: 2.015360\n",
      "Epoch : [4238/8999], Training Loss: 2.015259, Validation Loss: 2.015211\n",
      "Epoch : [4239/8999], Training Loss: 2.015131, Validation Loss: 2.015445\n",
      "Epoch : [4240/8999], Training Loss: 2.015409, Validation Loss: 2.016002\n",
      "Epoch : [4241/8999], Training Loss: 2.015907, Validation Loss: 2.015424\n",
      "Epoch : [4242/8999], Training Loss: 2.015468, Validation Loss: 2.015557\n",
      "Epoch : [4243/8999], Training Loss: 2.015542, Validation Loss: 2.015433\n",
      "Epoch : [4244/8999], Training Loss: 2.015408, Validation Loss: 2.015369\n",
      "Epoch : [4245/8999], Training Loss: 2.015245, Validation Loss: 2.015554\n",
      "Epoch : [4246/8999], Training Loss: 2.015465, Validation Loss: 2.016292\n",
      "Epoch : [4247/8999], Training Loss: 2.015544, Validation Loss: 2.015579\n",
      "Epoch : [4248/8999], Training Loss: 2.015293, Validation Loss: 2.015468\n",
      "Epoch : [4249/8999], Training Loss: 2.015321, Validation Loss: 2.015752\n",
      "Epoch : [4250/8999], Training Loss: 2.015448, Validation Loss: 2.016018\n",
      "Epoch : [4251/8999], Training Loss: 2.015503, Validation Loss: 2.015465\n",
      "Epoch : [4252/8999], Training Loss: 2.015363, Validation Loss: 2.015436\n",
      "Epoch : [4253/8999], Training Loss: 2.015225, Validation Loss: 2.015737\n",
      "Epoch : [4254/8999], Training Loss: 2.015221, Validation Loss: 2.015964\n",
      "Epoch : [4255/8999], Training Loss: 2.015177, Validation Loss: 2.015103\n",
      "Epoch : [4256/8999], Training Loss: 2.015265, Validation Loss: 2.015611\n",
      "Epoch : [4257/8999], Training Loss: 2.015244, Validation Loss: 2.016209\n",
      "Epoch : [4258/8999], Training Loss: 2.015359, Validation Loss: 2.015677\n",
      "Epoch : [4259/8999], Training Loss: 2.015175, Validation Loss: 2.015367\n",
      "Epoch : [4260/8999], Training Loss: 2.015185, Validation Loss: 2.015199\n",
      "Epoch : [4261/8999], Training Loss: 2.015225, Validation Loss: 2.016050\n",
      "Epoch : [4262/8999], Training Loss: 2.015548, Validation Loss: 2.015513\n",
      "Epoch : [4263/8999], Training Loss: 2.015313, Validation Loss: 2.016113\n",
      "Epoch : [4264/8999], Training Loss: 2.015365, Validation Loss: 2.015361\n",
      "Epoch : [4265/8999], Training Loss: 2.015253, Validation Loss: 2.015277\n",
      "Epoch : [4266/8999], Training Loss: 2.015270, Validation Loss: 2.015737\n",
      "Epoch : [4267/8999], Training Loss: 2.015208, Validation Loss: 2.015272\n",
      "Epoch : [4268/8999], Training Loss: 2.015249, Validation Loss: 2.015531\n",
      "Epoch : [4269/8999], Training Loss: 2.015117, Validation Loss: 2.015403\n",
      "Epoch : [4270/8999], Training Loss: 2.015232, Validation Loss: 2.015525\n",
      "Epoch : [4271/8999], Training Loss: 2.015416, Validation Loss: 2.016260\n",
      "Epoch : [4272/8999], Training Loss: 2.015260, Validation Loss: 2.016228\n",
      "Epoch : [4273/8999], Training Loss: 2.015288, Validation Loss: 2.015271\n",
      "Epoch : [4274/8999], Training Loss: 2.015343, Validation Loss: 2.015582\n",
      "Epoch : [4275/8999], Training Loss: 2.015154, Validation Loss: 2.016079\n",
      "Epoch : [4276/8999], Training Loss: 2.015204, Validation Loss: 2.015914\n",
      "Epoch : [4277/8999], Training Loss: 2.015333, Validation Loss: 2.016005\n",
      "Epoch : [4278/8999], Training Loss: 2.015261, Validation Loss: 2.015161\n",
      "Epoch : [4279/8999], Training Loss: 2.015155, Validation Loss: 2.015533\n",
      "Epoch : [4280/8999], Training Loss: 2.015366, Validation Loss: 2.016227\n",
      "Epoch : [4281/8999], Training Loss: 2.015231, Validation Loss: 2.015274\n",
      "Epoch : [4282/8999], Training Loss: 2.015005, Validation Loss: 2.015243\n",
      "Epoch : [4283/8999], Training Loss: 2.015122, Validation Loss: 2.015555\n",
      "Epoch : [4284/8999], Training Loss: 2.015220, Validation Loss: 2.015750\n",
      "Epoch : [4285/8999], Training Loss: 2.015205, Validation Loss: 2.015755\n",
      "Epoch : [4286/8999], Training Loss: 2.015209, Validation Loss: 2.015593\n",
      "Epoch : [4287/8999], Training Loss: 2.015145, Validation Loss: 2.015157\n",
      "Epoch : [4288/8999], Training Loss: 2.015039, Validation Loss: 2.015784\n",
      "Epoch : [4289/8999], Training Loss: 2.015161, Validation Loss: 2.016085\n",
      "Epoch : [4290/8999], Training Loss: 2.015273, Validation Loss: 2.015382\n",
      "Epoch : [4291/8999], Training Loss: 2.015237, Validation Loss: 2.015390\n",
      "Epoch : [4292/8999], Training Loss: 2.015052, Validation Loss: 2.015346\n",
      "Epoch : [4293/8999], Training Loss: 2.015141, Validation Loss: 2.015712\n",
      "Epoch : [4294/8999], Training Loss: 2.015252, Validation Loss: 2.015799\n",
      "Epoch : [4295/8999], Training Loss: 2.015164, Validation Loss: 2.015095\n",
      "Epoch : [4296/8999], Training Loss: 2.015165, Validation Loss: 2.015266\n",
      "Epoch : [4297/8999], Training Loss: 2.015231, Validation Loss: 2.015663\n",
      "Epoch : [4298/8999], Training Loss: 2.015207, Validation Loss: 2.015880\n",
      "Epoch : [4299/8999], Training Loss: 2.015129, Validation Loss: 2.015534\n",
      "Epoch : [4300/8999], Training Loss: 2.015081, Validation Loss: 2.015241\n",
      "Epoch : [4301/8999], Training Loss: 2.015202, Validation Loss: 2.015444\n",
      "Epoch : [4302/8999], Training Loss: 2.015054, Validation Loss: 2.015292\n",
      "Epoch : [4303/8999], Training Loss: 2.015064, Validation Loss: 2.015851\n",
      "Epoch : [4304/8999], Training Loss: 2.015051, Validation Loss: 2.015576\n",
      "Epoch : [4305/8999], Training Loss: 2.015189, Validation Loss: 2.015299\n",
      "Epoch : [4306/8999], Training Loss: 2.015205, Validation Loss: 2.015462\n",
      "Epoch : [4307/8999], Training Loss: 2.015109, Validation Loss: 2.015197\n",
      "Epoch : [4308/8999], Training Loss: 2.015067, Validation Loss: 2.015608\n",
      "Epoch : [4309/8999], Training Loss: 2.015198, Validation Loss: 2.015484\n",
      "Epoch : [4310/8999], Training Loss: 2.015163, Validation Loss: 2.015332\n",
      "Epoch : [4311/8999], Training Loss: 2.015289, Validation Loss: 2.015578\n",
      "Epoch : [4312/8999], Training Loss: 2.015178, Validation Loss: 2.015821\n",
      "Epoch : [4313/8999], Training Loss: 2.015109, Validation Loss: 2.015513\n",
      "Epoch : [4314/8999], Training Loss: 2.015108, Validation Loss: 2.015231\n",
      "Epoch : [4315/8999], Training Loss: 2.015052, Validation Loss: 2.015227\n",
      "Epoch : [4316/8999], Training Loss: 2.015142, Validation Loss: 2.015533\n",
      "Epoch : [4317/8999], Training Loss: 2.014975, Validation Loss: 2.015318\n",
      "Epoch : [4318/8999], Training Loss: 2.015106, Validation Loss: 2.015360\n",
      "Epoch : [4319/8999], Training Loss: 2.015220, Validation Loss: 2.015469\n",
      "Epoch : [4320/8999], Training Loss: 2.015047, Validation Loss: 2.015228\n",
      "Epoch : [4321/8999], Training Loss: 2.015045, Validation Loss: 2.015286\n",
      "Epoch : [4322/8999], Training Loss: 2.015229, Validation Loss: 2.015634\n",
      "Epoch : [4323/8999], Training Loss: 2.015131, Validation Loss: 2.015281\n",
      "Epoch : [4324/8999], Training Loss: 2.015053, Validation Loss: 2.015495\n",
      "Epoch : [4325/8999], Training Loss: 2.015326, Validation Loss: 2.015338\n",
      "Epoch : [4326/8999], Training Loss: 2.015034, Validation Loss: 2.015272\n",
      "Epoch : [4327/8999], Training Loss: 2.014989, Validation Loss: 2.015226\n",
      "Epoch : [4328/8999], Training Loss: 2.015091, Validation Loss: 2.015440\n",
      "Epoch : [4329/8999], Training Loss: 2.015199, Validation Loss: 2.015480\n",
      "Epoch : [4330/8999], Training Loss: 2.015207, Validation Loss: 2.015222\n",
      "Epoch : [4331/8999], Training Loss: 2.015145, Validation Loss: 2.015445\n",
      "Epoch : [4332/8999], Training Loss: 2.015096, Validation Loss: 2.015402\n",
      "Epoch : [4333/8999], Training Loss: 2.015069, Validation Loss: 2.015237\n",
      "Epoch : [4334/8999], Training Loss: 2.014994, Validation Loss: 2.015120\n",
      "Epoch : [4335/8999], Training Loss: 2.015054, Validation Loss: 2.015295\n",
      "Epoch : [4336/8999], Training Loss: 2.014983, Validation Loss: 2.015121\n",
      "Epoch : [4337/8999], Training Loss: 2.014993, Validation Loss: 2.015188\n",
      "Epoch : [4338/8999], Training Loss: 2.015073, Validation Loss: 2.016007\n",
      "Epoch : [4339/8999], Training Loss: 2.015264, Validation Loss: 2.015345\n",
      "Epoch : [4340/8999], Training Loss: 2.015267, Validation Loss: 2.015571\n",
      "Epoch : [4341/8999], Training Loss: 2.015263, Validation Loss: 2.015459\n",
      "Epoch : [4342/8999], Training Loss: 2.015284, Validation Loss: 2.015993\n",
      "Epoch : [4343/8999], Training Loss: 2.015406, Validation Loss: 2.015225\n",
      "Epoch : [4344/8999], Training Loss: 2.015230, Validation Loss: 2.015421\n",
      "Epoch : [4345/8999], Training Loss: 2.015107, Validation Loss: 2.015917\n",
      "Epoch : [4346/8999], Training Loss: 2.015231, Validation Loss: 2.015324\n",
      "Epoch : [4347/8999], Training Loss: 2.015146, Validation Loss: 2.015587\n",
      "Epoch : [4348/8999], Training Loss: 2.015218, Validation Loss: 2.015494\n",
      "Epoch : [4349/8999], Training Loss: 2.014968, Validation Loss: 2.015583\n",
      "Epoch : [4350/8999], Training Loss: 2.015118, Validation Loss: 2.015616\n",
      "Epoch : [4351/8999], Training Loss: 2.015304, Validation Loss: 2.015911\n",
      "Epoch : [4352/8999], Training Loss: 2.015179, Validation Loss: 2.016097\n",
      "Epoch : [4353/8999], Training Loss: 2.015279, Validation Loss: 2.015867\n",
      "Epoch : [4354/8999], Training Loss: 2.015243, Validation Loss: 2.016054\n",
      "Epoch : [4355/8999], Training Loss: 2.015329, Validation Loss: 2.016259\n",
      "Epoch : [4356/8999], Training Loss: 2.015511, Validation Loss: 2.016227\n",
      "Epoch : [4357/8999], Training Loss: 2.015510, Validation Loss: 2.015856\n",
      "Epoch : [4358/8999], Training Loss: 2.015662, Validation Loss: 2.017057\n",
      "Epoch : [4359/8999], Training Loss: 2.015794, Validation Loss: 2.017143\n",
      "Epoch : [4360/8999], Training Loss: 2.015560, Validation Loss: 2.016306\n",
      "Epoch : [4361/8999], Training Loss: 2.015328, Validation Loss: 2.016084\n",
      "Epoch : [4362/8999], Training Loss: 2.015431, Validation Loss: 2.016915\n",
      "Epoch : [4363/8999], Training Loss: 2.015710, Validation Loss: 2.017512\n",
      "Epoch : [4364/8999], Training Loss: 2.015764, Validation Loss: 2.016139\n",
      "Epoch : [4365/8999], Training Loss: 2.015838, Validation Loss: 2.017608\n",
      "Epoch : [4366/8999], Training Loss: 2.016015, Validation Loss: 2.017378\n",
      "Epoch : [4367/8999], Training Loss: 2.015994, Validation Loss: 2.016687\n",
      "Epoch : [4368/8999], Training Loss: 2.016166, Validation Loss: 2.017177\n",
      "Epoch : [4369/8999], Training Loss: 2.016646, Validation Loss: 2.017141\n",
      "Epoch : [4370/8999], Training Loss: 2.016757, Validation Loss: 2.016718\n",
      "Epoch : [4371/8999], Training Loss: 2.016681, Validation Loss: 2.016844\n",
      "Epoch : [4372/8999], Training Loss: 2.017122, Validation Loss: 2.016030\n",
      "Epoch : [4373/8999], Training Loss: 2.016162, Validation Loss: 2.016031\n",
      "Epoch : [4374/8999], Training Loss: 2.015634, Validation Loss: 2.016035\n",
      "Epoch : [4375/8999], Training Loss: 2.016035, Validation Loss: 2.015769\n",
      "Epoch : [4376/8999], Training Loss: 2.015616, Validation Loss: 2.015741\n",
      "Epoch : [4377/8999], Training Loss: 2.015530, Validation Loss: 2.015422\n",
      "Epoch : [4378/8999], Training Loss: 2.015631, Validation Loss: 2.015651\n",
      "Epoch : [4379/8999], Training Loss: 2.015261, Validation Loss: 2.015198\n",
      "Epoch : [4380/8999], Training Loss: 2.015313, Validation Loss: 2.015579\n",
      "Epoch : [4381/8999], Training Loss: 2.015347, Validation Loss: 2.015478\n",
      "Epoch : [4382/8999], Training Loss: 2.015246, Validation Loss: 2.015216\n",
      "Epoch : [4383/8999], Training Loss: 2.015424, Validation Loss: 2.015877\n",
      "Epoch : [4384/8999], Training Loss: 2.015175, Validation Loss: 2.015296\n",
      "Epoch : [4385/8999], Training Loss: 2.015016, Validation Loss: 2.015101\n",
      "Epoch : [4386/8999], Training Loss: 2.015050, Validation Loss: 2.015126\n",
      "Epoch : [4387/8999], Training Loss: 2.015151, Validation Loss: 2.015470\n",
      "Epoch : [4388/8999], Training Loss: 2.015078, Validation Loss: 2.015339\n",
      "Epoch : [4389/8999], Training Loss: 2.015022, Validation Loss: 2.015189\n",
      "Epoch : [4390/8999], Training Loss: 2.015213, Validation Loss: 2.015285\n",
      "Epoch : [4391/8999], Training Loss: 2.015124, Validation Loss: 2.015259\n",
      "Epoch : [4392/8999], Training Loss: 2.015272, Validation Loss: 2.015229\n",
      "Epoch : [4393/8999], Training Loss: 2.015018, Validation Loss: 2.015260\n",
      "Epoch : [4394/8999], Training Loss: 2.014987, Validation Loss: 2.015049\n",
      "Epoch : [4395/8999], Training Loss: 2.015139, Validation Loss: 2.015068\n",
      "Epoch : [4396/8999], Training Loss: 2.015073, Validation Loss: 2.015287\n",
      "Epoch : [4397/8999], Training Loss: 2.015032, Validation Loss: 2.015203\n",
      "Epoch : [4398/8999], Training Loss: 2.015136, Validation Loss: 2.015239\n",
      "Epoch : [4399/8999], Training Loss: 2.015513, Validation Loss: 2.015760\n",
      "Epoch : [4400/8999], Training Loss: 2.015277, Validation Loss: 2.015047\n",
      "Epoch : [4401/8999], Training Loss: 2.015115, Validation Loss: 2.015015\n",
      "Epoch : [4402/8999], Training Loss: 2.015008, Validation Loss: 2.015100\n",
      "Epoch : [4403/8999], Training Loss: 2.014954, Validation Loss: 2.014927\n",
      "Epoch : [4404/8999], Training Loss: 2.014932, Validation Loss: 2.015105\n",
      "Epoch : [4405/8999], Training Loss: 2.015057, Validation Loss: 2.015332\n",
      "Epoch : [4406/8999], Training Loss: 2.015259, Validation Loss: 2.015407\n",
      "Epoch : [4407/8999], Training Loss: 2.015202, Validation Loss: 2.015172\n",
      "Epoch : [4408/8999], Training Loss: 2.015077, Validation Loss: 2.015156\n",
      "Epoch : [4409/8999], Training Loss: 2.014896, Validation Loss: 2.015040\n",
      "Epoch : [4410/8999], Training Loss: 2.015137, Validation Loss: 2.015100\n",
      "Epoch : [4411/8999], Training Loss: 2.015142, Validation Loss: 2.014980\n",
      "Epoch : [4412/8999], Training Loss: 2.015138, Validation Loss: 2.014944\n",
      "Epoch : [4413/8999], Training Loss: 2.014960, Validation Loss: 2.014883\n",
      "Epoch : [4414/8999], Training Loss: 2.014877, Validation Loss: 2.015055\n",
      "Epoch : [4415/8999], Training Loss: 2.015168, Validation Loss: 2.015462\n",
      "Epoch : [4416/8999], Training Loss: 2.015104, Validation Loss: 2.015169\n",
      "Epoch : [4417/8999], Training Loss: 2.015339, Validation Loss: 2.015901\n",
      "Epoch : [4418/8999], Training Loss: 2.015275, Validation Loss: 2.015237\n",
      "Epoch : [4419/8999], Training Loss: 2.015137, Validation Loss: 2.015057\n",
      "Epoch : [4420/8999], Training Loss: 2.015143, Validation Loss: 2.015451\n",
      "Epoch : [4421/8999], Training Loss: 2.015008, Validation Loss: 2.015340\n",
      "Epoch : [4422/8999], Training Loss: 2.015054, Validation Loss: 2.015519\n",
      "Epoch : [4423/8999], Training Loss: 2.015066, Validation Loss: 2.015093\n",
      "Epoch : [4424/8999], Training Loss: 2.014976, Validation Loss: 2.015365\n",
      "Epoch : [4425/8999], Training Loss: 2.014870, Validation Loss: 2.014983\n",
      "Epoch : [4426/8999], Training Loss: 2.014954, Validation Loss: 2.015137\n",
      "Epoch : [4427/8999], Training Loss: 2.015220, Validation Loss: 2.015168\n",
      "Epoch : [4428/8999], Training Loss: 2.015007, Validation Loss: 2.014954\n",
      "Epoch : [4429/8999], Training Loss: 2.014910, Validation Loss: 2.015024\n",
      "Epoch : [4430/8999], Training Loss: 2.015115, Validation Loss: 2.015500\n",
      "Epoch : [4431/8999], Training Loss: 2.014990, Validation Loss: 2.015511\n",
      "Epoch : [4432/8999], Training Loss: 2.014972, Validation Loss: 2.015057\n",
      "Epoch : [4433/8999], Training Loss: 2.015005, Validation Loss: 2.015046\n",
      "Epoch : [4434/8999], Training Loss: 2.015252, Validation Loss: 2.015878\n",
      "Epoch : [4435/8999], Training Loss: 2.015123, Validation Loss: 2.015633\n",
      "Epoch : [4436/8999], Training Loss: 2.015105, Validation Loss: 2.014890\n",
      "Epoch : [4437/8999], Training Loss: 2.015141, Validation Loss: 2.015343\n",
      "Epoch : [4438/8999], Training Loss: 2.015048, Validation Loss: 2.016060\n",
      "Epoch : [4439/8999], Training Loss: 2.015112, Validation Loss: 2.015281\n",
      "Epoch : [4440/8999], Training Loss: 2.014940, Validation Loss: 2.014916\n",
      "Epoch : [4441/8999], Training Loss: 2.015087, Validation Loss: 2.015040\n",
      "Epoch : [4442/8999], Training Loss: 2.015111, Validation Loss: 2.015521\n",
      "Epoch : [4443/8999], Training Loss: 2.015061, Validation Loss: 2.015099\n",
      "Epoch : [4444/8999], Training Loss: 2.015106, Validation Loss: 2.015380\n",
      "Epoch : [4445/8999], Training Loss: 2.014880, Validation Loss: 2.014812\n",
      "Epoch : [4446/8999], Training Loss: 2.014921, Validation Loss: 2.014852\n",
      "Epoch : [4447/8999], Training Loss: 2.014934, Validation Loss: 2.015326\n",
      "Epoch : [4448/8999], Training Loss: 2.014969, Validation Loss: 2.015926\n",
      "Epoch : [4449/8999], Training Loss: 2.015042, Validation Loss: 2.014896\n",
      "Epoch : [4450/8999], Training Loss: 2.015051, Validation Loss: 2.015178\n",
      "Epoch : [4451/8999], Training Loss: 2.015215, Validation Loss: 2.015473\n",
      "Epoch : [4452/8999], Training Loss: 2.014919, Validation Loss: 2.015437\n",
      "Epoch : [4453/8999], Training Loss: 2.014965, Validation Loss: 2.015635\n",
      "Epoch : [4454/8999], Training Loss: 2.014963, Validation Loss: 2.014957\n",
      "Epoch : [4455/8999], Training Loss: 2.015005, Validation Loss: 2.015096\n",
      "Epoch : [4456/8999], Training Loss: 2.015146, Validation Loss: 2.015598\n",
      "Epoch : [4457/8999], Training Loss: 2.015175, Validation Loss: 2.015541\n",
      "Epoch : [4458/8999], Training Loss: 2.014902, Validation Loss: 2.015067\n",
      "Epoch : [4459/8999], Training Loss: 2.014891, Validation Loss: 2.014979\n",
      "Epoch : [4460/8999], Training Loss: 2.015023, Validation Loss: 2.015276\n",
      "Epoch : [4461/8999], Training Loss: 2.014969, Validation Loss: 2.015517\n",
      "Epoch : [4462/8999], Training Loss: 2.015136, Validation Loss: 2.015303\n",
      "Epoch : [4463/8999], Training Loss: 2.015034, Validation Loss: 2.015211\n",
      "Epoch : [4464/8999], Training Loss: 2.015131, Validation Loss: 2.015131\n",
      "Epoch : [4465/8999], Training Loss: 2.014977, Validation Loss: 2.015177\n",
      "Epoch : [4466/8999], Training Loss: 2.014939, Validation Loss: 2.015183\n",
      "Epoch : [4467/8999], Training Loss: 2.015100, Validation Loss: 2.015316\n",
      "Epoch : [4468/8999], Training Loss: 2.015098, Validation Loss: 2.015240\n",
      "Epoch : [4469/8999], Training Loss: 2.014964, Validation Loss: 2.015215\n",
      "Epoch : [4470/8999], Training Loss: 2.014856, Validation Loss: 2.015198\n",
      "Epoch : [4471/8999], Training Loss: 2.014905, Validation Loss: 2.015434\n",
      "Epoch : [4472/8999], Training Loss: 2.014851, Validation Loss: 2.015445\n",
      "Epoch : [4473/8999], Training Loss: 2.014878, Validation Loss: 2.015039\n",
      "Epoch : [4474/8999], Training Loss: 2.014963, Validation Loss: 2.014960\n",
      "Epoch : [4475/8999], Training Loss: 2.014960, Validation Loss: 2.015323\n",
      "Epoch : [4476/8999], Training Loss: 2.014912, Validation Loss: 2.015268\n",
      "Epoch : [4477/8999], Training Loss: 2.014890, Validation Loss: 2.014849\n",
      "Epoch : [4478/8999], Training Loss: 2.014822, Validation Loss: 2.015012\n",
      "Epoch : [4479/8999], Training Loss: 2.014870, Validation Loss: 2.015653\n",
      "Epoch : [4480/8999], Training Loss: 2.014969, Validation Loss: 2.015204\n",
      "Epoch : [4481/8999], Training Loss: 2.015031, Validation Loss: 2.014887\n",
      "Epoch : [4482/8999], Training Loss: 2.015086, Validation Loss: 2.014888\n",
      "Epoch : [4483/8999], Training Loss: 2.014879, Validation Loss: 2.015134\n",
      "Epoch : [4484/8999], Training Loss: 2.014831, Validation Loss: 2.015169\n",
      "Epoch : [4485/8999], Training Loss: 2.014803, Validation Loss: 2.015236\n",
      "Epoch : [4486/8999], Training Loss: 2.014955, Validation Loss: 2.015008\n",
      "Epoch : [4487/8999], Training Loss: 2.014994, Validation Loss: 2.015263\n",
      "Epoch : [4488/8999], Training Loss: 2.015177, Validation Loss: 2.015249\n",
      "Epoch : [4489/8999], Training Loss: 2.015124, Validation Loss: 2.015439\n",
      "Epoch : [4490/8999], Training Loss: 2.015000, Validation Loss: 2.015111\n",
      "Epoch : [4491/8999], Training Loss: 2.014949, Validation Loss: 2.015017\n",
      "Epoch : [4492/8999], Training Loss: 2.014953, Validation Loss: 2.014887\n",
      "Epoch : [4493/8999], Training Loss: 2.014779, Validation Loss: 2.015071\n",
      "Epoch : [4494/8999], Training Loss: 2.014806, Validation Loss: 2.015235\n",
      "Epoch : [4495/8999], Training Loss: 2.014952, Validation Loss: 2.015070\n",
      "Epoch : [4496/8999], Training Loss: 2.015054, Validation Loss: 2.014984\n",
      "Epoch : [4497/8999], Training Loss: 2.014988, Validation Loss: 2.015071\n",
      "Epoch : [4498/8999], Training Loss: 2.014989, Validation Loss: 2.015295\n",
      "Epoch : [4499/8999], Training Loss: 2.014961, Validation Loss: 2.014996\n",
      "Epoch : [4500/8999], Training Loss: 2.014898, Validation Loss: 2.015080\n",
      "Epoch : [4501/8999], Training Loss: 2.014856, Validation Loss: 2.014911\n",
      "Epoch : [4502/8999], Training Loss: 2.014950, Validation Loss: 2.015416\n",
      "Epoch : [4503/8999], Training Loss: 2.015074, Validation Loss: 2.015229\n",
      "Epoch : [4504/8999], Training Loss: 2.014922, Validation Loss: 2.014887\n",
      "Epoch : [4505/8999], Training Loss: 2.014820, Validation Loss: 2.015048\n",
      "Epoch : [4506/8999], Training Loss: 2.014950, Validation Loss: 2.015212\n",
      "Epoch : [4507/8999], Training Loss: 2.014975, Validation Loss: 2.014996\n",
      "Epoch : [4508/8999], Training Loss: 2.015010, Validation Loss: 2.015264\n",
      "Epoch : [4509/8999], Training Loss: 2.014978, Validation Loss: 2.015150\n",
      "Epoch : [4510/8999], Training Loss: 2.014919, Validation Loss: 2.015069\n",
      "Epoch : [4511/8999], Training Loss: 2.014947, Validation Loss: 2.015326\n",
      "Epoch : [4512/8999], Training Loss: 2.014938, Validation Loss: 2.015074\n",
      "Epoch : [4513/8999], Training Loss: 2.015024, Validation Loss: 2.015214\n",
      "Epoch : [4514/8999], Training Loss: 2.014992, Validation Loss: 2.015342\n",
      "Epoch : [4515/8999], Training Loss: 2.015103, Validation Loss: 2.015094\n",
      "Epoch : [4516/8999], Training Loss: 2.015184, Validation Loss: 2.015589\n",
      "Epoch : [4517/8999], Training Loss: 2.015047, Validation Loss: 2.015182\n",
      "Epoch : [4518/8999], Training Loss: 2.014935, Validation Loss: 2.015211\n",
      "Epoch : [4519/8999], Training Loss: 2.015060, Validation Loss: 2.015594\n",
      "Epoch : [4520/8999], Training Loss: 2.015064, Validation Loss: 2.015236\n",
      "Epoch : [4521/8999], Training Loss: 2.015082, Validation Loss: 2.015149\n",
      "Epoch : [4522/8999], Training Loss: 2.015101, Validation Loss: 2.015454\n",
      "Epoch : [4523/8999], Training Loss: 2.015160, Validation Loss: 2.016212\n",
      "Epoch : [4524/8999], Training Loss: 2.015254, Validation Loss: 2.015512\n",
      "Epoch : [4525/8999], Training Loss: 2.015271, Validation Loss: 2.015971\n",
      "Epoch : [4526/8999], Training Loss: 2.015126, Validation Loss: 2.015599\n",
      "Epoch : [4527/8999], Training Loss: 2.015158, Validation Loss: 2.016402\n",
      "Epoch : [4528/8999], Training Loss: 2.015204, Validation Loss: 2.015155\n",
      "Epoch : [4529/8999], Training Loss: 2.015062, Validation Loss: 2.015342\n",
      "Epoch : [4530/8999], Training Loss: 2.015088, Validation Loss: 2.016670\n",
      "Epoch : [4531/8999], Training Loss: 2.015331, Validation Loss: 2.015696\n",
      "Epoch : [4532/8999], Training Loss: 2.015018, Validation Loss: 2.015496\n",
      "Epoch : [4533/8999], Training Loss: 2.015106, Validation Loss: 2.015897\n",
      "Epoch : [4534/8999], Training Loss: 2.015290, Validation Loss: 2.016906\n",
      "Epoch : [4535/8999], Training Loss: 2.015331, Validation Loss: 2.015560\n",
      "Epoch : [4536/8999], Training Loss: 2.015140, Validation Loss: 2.016130\n",
      "Epoch : [4537/8999], Training Loss: 2.015361, Validation Loss: 2.016821\n",
      "Epoch : [4538/8999], Training Loss: 2.015175, Validation Loss: 2.016105\n",
      "Epoch : [4539/8999], Training Loss: 2.015306, Validation Loss: 2.016147\n",
      "Epoch : [4540/8999], Training Loss: 2.015798, Validation Loss: 2.017250\n",
      "Epoch : [4541/8999], Training Loss: 2.015887, Validation Loss: 2.017054\n",
      "Epoch : [4542/8999], Training Loss: 2.015636, Validation Loss: 2.016713\n",
      "Epoch : [4543/8999], Training Loss: 2.015572, Validation Loss: 2.017076\n",
      "Epoch : [4544/8999], Training Loss: 2.015832, Validation Loss: 2.016923\n",
      "Epoch : [4545/8999], Training Loss: 2.015725, Validation Loss: 2.017026\n",
      "Epoch : [4546/8999], Training Loss: 2.016274, Validation Loss: 2.017156\n",
      "Epoch : [4547/8999], Training Loss: 2.016183, Validation Loss: 2.016945\n",
      "Epoch : [4548/8999], Training Loss: 2.016607, Validation Loss: 2.016874\n",
      "Epoch : [4549/8999], Training Loss: 2.016962, Validation Loss: 2.016410\n",
      "Epoch : [4550/8999], Training Loss: 2.016497, Validation Loss: 2.016139\n",
      "Epoch : [4551/8999], Training Loss: 2.015960, Validation Loss: 2.016067\n",
      "Epoch : [4552/8999], Training Loss: 2.015764, Validation Loss: 2.016642\n",
      "Epoch : [4553/8999], Training Loss: 2.016054, Validation Loss: 2.016240\n",
      "Epoch : [4554/8999], Training Loss: 2.016358, Validation Loss: 2.015864\n",
      "Epoch : [4555/8999], Training Loss: 2.016514, Validation Loss: 2.015967\n",
      "Epoch : [4556/8999], Training Loss: 2.015821, Validation Loss: 2.015999\n",
      "Epoch : [4557/8999], Training Loss: 2.015706, Validation Loss: 2.015564\n",
      "Epoch : [4558/8999], Training Loss: 2.015596, Validation Loss: 2.016409\n",
      "Epoch : [4559/8999], Training Loss: 2.015895, Validation Loss: 2.015692\n",
      "Epoch : [4560/8999], Training Loss: 2.015502, Validation Loss: 2.015361\n",
      "Epoch : [4561/8999], Training Loss: 2.015213, Validation Loss: 2.015410\n",
      "Epoch : [4562/8999], Training Loss: 2.015192, Validation Loss: 2.015143\n",
      "Epoch : [4563/8999], Training Loss: 2.015297, Validation Loss: 2.015448\n",
      "Epoch : [4564/8999], Training Loss: 2.015162, Validation Loss: 2.015377\n",
      "Epoch : [4565/8999], Training Loss: 2.015123, Validation Loss: 2.015227\n",
      "Epoch : [4566/8999], Training Loss: 2.015251, Validation Loss: 2.015731\n",
      "Epoch : [4567/8999], Training Loss: 2.015235, Validation Loss: 2.014919\n",
      "Epoch : [4568/8999], Training Loss: 2.015080, Validation Loss: 2.014868\n",
      "Epoch : [4569/8999], Training Loss: 2.014922, Validation Loss: 2.015599\n",
      "Epoch : [4570/8999], Training Loss: 2.015096, Validation Loss: 2.014878\n",
      "Epoch : [4571/8999], Training Loss: 2.015141, Validation Loss: 2.015051\n",
      "Epoch : [4572/8999], Training Loss: 2.014923, Validation Loss: 2.015358\n",
      "Epoch : [4573/8999], Training Loss: 2.015093, Validation Loss: 2.015110\n",
      "Epoch : [4574/8999], Training Loss: 2.015036, Validation Loss: 2.014970\n",
      "Epoch : [4575/8999], Training Loss: 2.015021, Validation Loss: 2.014996\n",
      "Epoch : [4576/8999], Training Loss: 2.014889, Validation Loss: 2.014805\n",
      "Epoch : [4577/8999], Training Loss: 2.014774, Validation Loss: 2.014908\n",
      "Epoch : [4578/8999], Training Loss: 2.014965, Validation Loss: 2.015498\n",
      "Epoch : [4579/8999], Training Loss: 2.014965, Validation Loss: 2.014997\n",
      "Epoch : [4580/8999], Training Loss: 2.014926, Validation Loss: 2.015194\n",
      "Epoch : [4581/8999], Training Loss: 2.014906, Validation Loss: 2.015048\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m valid_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     17\u001b[0m model_for_checkpoint\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28miter\u001b[39m, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     19\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m data\n\u001b[0;32m     20\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move inputs and labels to the device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:439\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 439\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:387\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1040\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[1;34m(self, loader)\u001b[0m\n\u001b[0;32m   1033\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\multiprocessing\\process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[0;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m _cleanup()\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\multiprocessing\\context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\multiprocessing\\context.py:336\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[1;34m(process_obj)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_win32\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[1;32m--> 336\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\multiprocessing\\popen_spawn_win32.py:93\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, process_obj)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     92\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, to_child)\n\u001b[1;32m---> 93\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_child\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\multiprocessing\\reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, file, protocol)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Check if GPU is available and set the device accordingly\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "\n",
    "# get the base model (VGG16NoLite) and move it to the chosen device\n",
    "# model_for_checkpoint = ViT((3, 32, 32), n_patches=8, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "model_for_checkpoint = ViT((3, 32, 32), n_patches=16, n_blocks=7, hidden_d=512, n_heads=8, out_d=10).to(device)\n",
    "\n",
    "# creating branchpoints\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_for_checkpoint.parameters(), lr=0.001, momentum=0.9)  # momentum=0.9\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model_for_checkpoint.train()\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # Move inputs and labels to the device\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_for_checkpoint(inputs)\n",
    "\n",
    "        # Here assuming your loss function and any other operation are compatible with CUDA tensors\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        if iter % 20 == 0:\n",
    "            # print(\"Running validation for {} Epoch, {} Iteration...\".format(epoch, iter))\n",
    "            res = accuracy_multiclass_gpu(model_for_checkpoint, test_loader)  # Ensure this function also handles data on GPU\n",
    "\n",
    "            # print(\"ACCURACY: {}\".format(res))\n",
    "            if res > 0.7:\n",
    "                # Move model to CPU before saving\n",
    "                model_for_checkpoint.to('cpu')\n",
    "                torch.save(model_for_checkpoint.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/vit/branch_{}.pt\".format(res))\n",
    "                # Optionally, move model back to the original device (GPU) if further computation is needed\n",
    "                print(\"model saved at accuracy: \", res)\n",
    "                model_for_checkpoint.to(device)\n",
    "            if res > 0.9:\n",
    "                isLoop = False\n",
    "                break\n",
    "\n",
    "    # Validation phase\n",
    "    model_for_checkpoint.eval()\n",
    "    with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "        for data, target in test_loader:\n",
    "            # Move data and target to the correct device\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            output = model_for_checkpoint(data)\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "            valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(test_loader.dataset)\n",
    "\n",
    "    print(\"Epoch : [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss))\n",
    "    if not isLoop:\n",
    "        break\n",
    "print(\"Max epoch reached\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting branchpoints of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DECOMPOSED_LAYERS = ['classifier.0.weight', 'classifier.4.weight', 'classifier.8.weight']\n",
    "DECOMPOSED_LAYERS = [\n",
    "        'blocks.0.mhsa.q_mappings.0.weight',  'blocks.0.mhsa.q_mappings.1.weight',\n",
    "        'blocks.0.mhsa.k_mappings.0.weight',  'blocks.0.mhsa.k_mappings.1.weight',\n",
    "        'blocks.0.mhsa.v_mappings.0.weight',  'blocks.0.mhsa.v_mappings.1.weight',\n",
    "        'blocks.0.mlp.0.weight', 'blocks.0.mlp.2.weight',\n",
    "        'blocks.1.mhsa.q_mappings.0.weight',  'blocks.1.mhsa.q_mappings.1.weight',\n",
    "        'blocks.1.mhsa.k_mappings.0.weight',  'blocks.1.mhsa.k_mappings.1.weight',\n",
    "        'blocks.1.mhsa.v_mappings.0.weight',  'blocks.1.mhsa.v_mappings.1.weight',\n",
    "        'blocks.1.mlp.0.weight', 'blocks.1.mlp.2.weight'\n",
    "    ]\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "BRANCH_ACC = \"0.71\"\n",
    "\n",
    "# CUDA setup\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model setup\n",
    "original = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "model_original = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "\n",
    "# Load state from a checkpoint into the GPU directly\n",
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/vit/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['class_token', 'linear_mapper.weight', 'linear_mapper.bias', 'blocks.0.norm1.weight', 'blocks.0.norm1.bias', 'blocks.0.mhsa.q_mappings.0.weight', 'blocks.0.mhsa.q_mappings.0.bias', 'blocks.0.mhsa.q_mappings.1.weight', 'blocks.0.mhsa.q_mappings.1.bias', 'blocks.0.mhsa.k_mappings.0.weight', 'blocks.0.mhsa.k_mappings.0.bias', 'blocks.0.mhsa.k_mappings.1.weight', 'blocks.0.mhsa.k_mappings.1.bias', 'blocks.0.mhsa.v_mappings.0.weight', 'blocks.0.mhsa.v_mappings.0.bias', 'blocks.0.mhsa.v_mappings.1.weight', 'blocks.0.mhsa.v_mappings.1.bias', 'blocks.0.norm2.weight', 'blocks.0.norm2.bias', 'blocks.0.mlp.0.weight', 'blocks.0.mlp.0.bias', 'blocks.0.mlp.2.weight', 'blocks.0.mlp.2.bias', 'blocks.1.norm1.weight', 'blocks.1.norm1.bias', 'blocks.1.mhsa.q_mappings.0.weight', 'blocks.1.mhsa.q_mappings.0.bias', 'blocks.1.mhsa.q_mappings.1.weight', 'blocks.1.mhsa.q_mappings.1.bias', 'blocks.1.mhsa.k_mappings.0.weight', 'blocks.1.mhsa.k_mappings.0.bias', 'blocks.1.mhsa.k_mappings.1.weight', 'blocks.1.mhsa.k_mappings.1.bias', 'blocks.1.mhsa.v_mappings.0.weight', 'blocks.1.mhsa.v_mappings.0.bias', 'blocks.1.mhsa.v_mappings.1.weight', 'blocks.1.mhsa.v_mappings.1.bias', 'blocks.1.norm2.weight', 'blocks.1.norm2.bias', 'blocks.1.mlp.0.weight', 'blocks.1.mlp.0.bias', 'blocks.1.mlp.2.weight', 'blocks.1.mlp.2.bias', 'mlp.0.weight', 'mlp.0.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model_original.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = getBase(model_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.5658, -0.2513,  0.1698, -0.6916],\n",
       "         [-0.3514,  0.0901, -0.2642, -0.0597],\n",
       "         [-1.2717,  0.5795,  0.1772, -0.4261],\n",
       "         [ 0.1714,  0.7975, -1.1433,  0.5885]], device='cuda:0'),\n",
       " tensor([[ 0.5311,  0.8509, -0.0412, -0.0010],\n",
       "         [ 0.0061,  0.3410,  0.1322,  0.0693],\n",
       "         [-0.6286,  0.1443,  0.3623, -0.2997],\n",
       "         [ 0.2858, -0.7629, -0.4007, -0.2514]], device='cuda:0'),\n",
       " tensor([[ 0.9189, -0.2308,  0.1440,  0.1268],\n",
       "         [-0.0123, -0.3851, -0.4045,  0.0149],\n",
       "         [ 0.7634, -0.6067, -1.5817, -0.8321],\n",
       "         [-1.3926, -0.2428, -0.3663,  1.2968]], device='cuda:0'),\n",
       " tensor([[-2.7760, -0.8013,  0.6979, -0.2862],\n",
       "         [-1.0334,  0.0854,  0.1006, -0.0088],\n",
       "         [-0.6855,  0.5427, -0.1136,  0.1041],\n",
       "         [ 1.9153, -1.8821,  0.1980, -0.5972]], device='cuda:0'),\n",
       " tensor([[-1.6767,  0.4531,  1.0616,  0.3768],\n",
       "         [-2.2843, -1.2014,  0.0141,  0.8544],\n",
       "         [ 1.8979,  0.9023, -2.6706, -0.4557],\n",
       "         [ 0.6793,  0.4546,  1.0992,  1.0802]], device='cuda:0'),\n",
       " tensor([[ 0.6343, -1.5592, -3.3356, -3.1129],\n",
       "         [ 1.6637,  0.8736, -3.6951,  0.7488],\n",
       "         [-0.2832, -0.5427, -0.8550, -1.6404],\n",
       "         [-1.3786, -1.1486, -0.3318, -0.1149]], device='cuda:0'),\n",
       " tensor([[-1.3035e+00,  4.5964e-01,  7.9302e-01, -5.6023e-01, -5.8155e-01,\n",
       "           4.1378e-01,  6.1134e-01, -6.7781e-01],\n",
       "         [ 1.7468e-01, -6.3302e-01,  7.3131e-02, -7.1829e-01,  1.4079e+00,\n",
       "          -9.2796e-01, -1.9380e+00,  1.9748e+00],\n",
       "         [-6.0810e-01,  7.1146e-02,  8.7763e-01, -1.3832e+00,  6.5223e-01,\n",
       "           5.7260e-01, -6.5386e-01,  5.4563e-01],\n",
       "         [-1.9439e+00,  1.1283e+00, -1.0578e-01,  1.1162e+00, -6.2927e-01,\n",
       "          -4.8323e-01,  2.9427e-01, -5.7558e-01],\n",
       "         [-2.8514e-01, -8.7234e-01, -1.1090e-01,  1.0305e+00,  1.3891e-01,\n",
       "          -5.4929e-01, -3.8626e-01,  5.9806e-01],\n",
       "         [-2.3573e-01, -1.4195e+00,  1.0763e+00, -4.9413e-01, -3.7547e-01,\n",
       "           2.7520e-03,  3.0077e-01,  1.3219e+00],\n",
       "         [-1.5387e-01, -6.5452e-02,  3.0313e-01, -3.0829e-01, -1.0225e-01,\n",
       "           5.9591e-02,  2.6391e-01,  1.2120e-01],\n",
       "         [-4.7001e-01, -5.7100e-01, -2.8306e-01,  9.8309e-01,  4.5418e-01,\n",
       "          -4.2044e-01, -1.0660e+00, -4.1183e-01],\n",
       "         [-2.3511e-01, -3.5418e-01,  9.3547e-01, -4.7768e-01, -1.4517e-01,\n",
       "           1.2469e-01, -3.0931e-01, -3.3358e-01],\n",
       "         [ 8.2241e-01, -3.1476e-01, -1.7098e-01, -6.4652e-01, -4.6550e-01,\n",
       "           8.2050e-01, -7.7846e-01,  1.0440e-01],\n",
       "         [-1.2196e+00, -5.7727e-01,  7.6183e-01,  2.1598e-01,  4.0948e-01,\n",
       "           8.7566e-01, -7.8650e-01, -2.0565e-01],\n",
       "         [ 3.4065e-01,  1.8367e-01,  5.0808e-02, -7.5624e-01,  1.8378e-01,\n",
       "           2.7845e-01, -2.0507e+00,  2.5269e-01],\n",
       "         [-1.4724e+00,  1.4815e-01, -8.1719e-01,  6.3046e-01,  4.3829e-01,\n",
       "           7.7312e-02, -1.0321e+00,  3.4819e-01],\n",
       "         [-7.9088e-01, -6.6882e-02,  6.3316e-01, -6.3535e-01,  2.6854e-01,\n",
       "          -5.9349e-01, -1.6858e-01, -2.4584e-01],\n",
       "         [-2.2239e+00,  8.5087e-01, -4.5790e-02,  2.2359e-01,  2.2978e-01,\n",
       "          -9.3937e-01, -9.6628e-01,  1.5636e+00],\n",
       "         [-5.9758e-01,  5.6915e-01,  1.2384e+00, -4.8218e-01,  4.0218e-01,\n",
       "          -8.7358e-01,  3.2564e-01, -4.6591e-01],\n",
       "         [-1.2838e+00, -8.4313e-02, -1.1742e+00,  1.0401e+00, -7.9629e-01,\n",
       "           1.1422e+00,  8.4715e-01, -2.2600e-02],\n",
       "         [-9.9227e-01, -3.8175e-01, -6.9020e-01, -2.4194e-01,  4.6543e-01,\n",
       "           5.8336e-01,  5.3921e-01,  8.4139e-01],\n",
       "         [-1.9365e+00, -3.4128e-01,  1.0981e+00,  7.7329e-01, -4.7940e-02,\n",
       "          -1.0250e+00,  6.3497e-01,  2.9206e-01],\n",
       "         [ 6.7259e-02, -1.3660e-01, -4.0268e-01, -1.2002e+00, -4.0878e-01,\n",
       "          -2.1028e-01,  1.2452e+00,  8.3422e-01],\n",
       "         [-4.7260e-01, -7.7311e-01,  1.2721e+00,  3.6154e-01, -1.0845e-01,\n",
       "           5.8041e-01,  2.3094e-01, -6.4245e-01],\n",
       "         [-1.0880e-01, -7.7130e-01,  4.6288e-01,  1.2038e+00,  1.0555e+00,\n",
       "          -2.7891e-01, -4.4405e-01, -2.3398e-01],\n",
       "         [ 3.7114e-01, -1.4462e+00,  9.3443e-02,  1.5528e-01,  7.4386e-01,\n",
       "          -7.4801e-01,  5.8253e-01,  8.0939e-01],\n",
       "         [-5.2176e-01, -1.9211e-01, -8.9015e-01, -9.2054e-01,  1.6103e+00,\n",
       "          -1.3523e+00, -1.3750e-02,  1.7822e+00],\n",
       "         [ 1.0499e+00, -9.3095e-01,  3.5901e-01,  1.0262e-01,  1.1779e-01,\n",
       "           1.8342e-01, -5.4895e-01,  1.0331e-01],\n",
       "         [-1.2217e+00,  3.6900e-01,  3.6980e-01, -1.5377e+00, -1.5592e-01,\n",
       "           5.1529e-01, -2.5934e-01,  2.2683e-01],\n",
       "         [-6.3013e-01, -3.9208e-01, -7.6057e-01, -7.9256e-01, -7.3079e-01,\n",
       "           1.2913e+00, -1.1902e-01,  2.1146e+00],\n",
       "         [-7.7275e-01,  2.8344e-01, -6.4354e-02,  7.6556e-01,  5.0507e-01,\n",
       "          -5.3276e-01, -5.7825e-01, -3.1818e-01],\n",
       "         [ 5.5126e-01, -1.1485e-02,  8.0020e-01, -3.3684e-01, -9.5733e-01,\n",
       "          -2.7333e-01, -3.4811e-01,  1.0287e+00],\n",
       "         [ 2.1014e-01, -4.2512e-01, -4.5434e-02,  9.7381e-01, -5.8766e-01,\n",
       "          -2.8928e-01, -4.2674e-01,  7.7868e-01],\n",
       "         [-3.0747e-02, -1.0004e+00, -3.4679e-01, -3.9142e-01,  5.0584e-01,\n",
       "           1.4515e+00, -1.0850e+00,  7.1857e-01],\n",
       "         [ 4.5435e-01, -3.8646e-02,  5.3456e-01, -7.2266e-01,  1.5425e-03,\n",
       "          -4.7156e-02, -4.7956e-01, -2.5723e-01]], device='cuda:0'),\n",
       " tensor([[-0.1549,  1.2876, -0.2556,  0.1199, -0.7850, -0.1423, -0.1792, -0.2678,\n",
       "          -0.0093,  0.1853, -0.0775,  0.9234,  1.0124, -0.1990,  1.7282,  0.1909,\n",
       "          -1.1574, -0.3261, -1.1863, -0.9451,  0.7483,  0.0221,  0.2553,  0.8752,\n",
       "           0.7421, -0.3417,  0.2781,  0.2268, -0.8004, -0.1261,  0.0054,  0.1018],\n",
       "         [ 0.1834,  0.0906, -0.6411,  0.6061,  0.6402, -0.9717, -0.1792,  1.0975,\n",
       "           0.8725, -0.4348, -0.1275, -0.2320,  0.3389, -0.8998,  0.5291, -0.8597,\n",
       "           0.6797, -0.5140, -1.1847, -0.1937,  0.4214,  0.8093,  1.0782,  1.6684,\n",
       "           0.0321, -0.5912, -0.4766, -0.1185,  0.5878, -0.0996, -0.8184,  0.2990],\n",
       "         [ 0.3913, -1.2887,  0.6028, -1.3416,  0.1519,  0.0838,  0.1419,  0.7548,\n",
       "           0.2303, -1.4751,  0.6227, -1.2738,  0.0546,  0.9114,  0.7904,  1.1157,\n",
       "          -0.5466,  0.9836, -0.5950,  1.1728, -0.3646,  0.9092, -1.2918, -0.7197,\n",
       "           0.2191,  1.5288,  0.0391,  0.5292,  0.0094,  0.2119, -0.6014, -0.6010],\n",
       "         [ 0.8338, -0.3647,  0.4143, -0.0384,  0.1673,  0.4256,  0.0813,  0.1185,\n",
       "          -0.3786, -0.2478,  0.0739, -0.0968,  0.0460,  0.1439, -0.2173,  0.6242,\n",
       "           0.5063, -0.4224,  0.7801, -0.2404,  0.4475, -0.6064,  0.0686,  0.6201,\n",
       "          -0.3832,  0.2127, -1.1280, -0.0702,  0.2818,  0.3724,  0.3449, -0.4328],\n",
       "         [-0.9523,  0.0675, -0.0339, -0.5740, -0.0422,  0.4033,  0.1088,  0.2561,\n",
       "          -0.3971,  0.0220,  0.1173,  0.2582, -0.0761, -0.2265, -1.1756, -0.9437,\n",
       "          -0.9685,  0.7193, -0.0594,  0.4948, -0.8692,  0.7853,  1.0006, -0.1340,\n",
       "           0.0173, -0.2068,  1.0359,  0.4216,  0.2964,  0.4546, -0.4447, -0.0229],\n",
       "         [-1.0492,  0.3491,  0.4693, -0.7582, -0.6590, -1.1421, -0.1280, -1.1448,\n",
       "           0.0397,  0.2985,  0.1182,  0.0145, -1.0525, -0.0293, -0.2814,  0.1070,\n",
       "           0.0796, -0.1987,  0.7334, -0.0222, -0.5393, -0.6801, -0.2228,  0.0492,\n",
       "          -0.6740, -0.5705, -0.9874, -1.1433, -0.8907, -0.3987,  0.9057,  0.0670],\n",
       "         [ 0.0207, -0.2529, -1.1390,  0.7878,  0.5529,  0.5422,  0.3435, -0.1511,\n",
       "          -0.1728, -0.2200, -1.1755,  0.2247, -0.0629,  0.0757, -0.6244, -0.1022,\n",
       "          -0.1453, -0.4159,  0.4062,  0.3458, -0.8492, -0.7044, -0.6970, -0.1770,\n",
       "          -0.1587, -0.7299,  1.0104, -0.0918, -0.2425, -0.6043, -0.7326, -0.0750],\n",
       "         [ 0.9641,  0.3034, -0.2698,  1.1037, -0.0820,  0.7277,  0.0703, -0.6370,\n",
       "          -0.1673, -0.0586,  0.1061, -0.3564,  0.7237,  0.4868, -0.8385,  0.3077,\n",
       "           0.4182, -0.8686,  0.9085, -0.6940,  0.6925, -0.4702,  0.1782,  0.4339,\n",
       "          -0.3013, -0.4415,  1.2320,  0.4875,  0.5682, -0.5141,  0.4319, -0.5056]],\n",
       "        device='cuda:0'),\n",
       " tensor([[ 0.3884, -0.1595, -0.2452, -0.2383],\n",
       "         [ 0.4881, -0.0343, -0.1527, -1.2513],\n",
       "         [-0.7821, -0.3311, -0.2211,  0.5774],\n",
       "         [-0.0733, -0.2905, -0.2136,  0.3493]], device='cuda:0'),\n",
       " tensor([[-0.2306, -0.1433, -0.0546, -0.4563],\n",
       "         [-0.5008,  0.2998, -0.5072, -0.0245],\n",
       "         [ 0.2294, -0.1298,  0.3949, -0.0719],\n",
       "         [-0.3829,  0.1174, -0.1529, -0.2710]], device='cuda:0'),\n",
       " tensor([[ 1.6714, -0.5572, -0.8831, -0.1924],\n",
       "         [-0.5302, -0.1013, -0.0170, -0.8799],\n",
       "         [ 0.3659,  0.1509, -0.3060,  0.6532],\n",
       "         [-1.2477,  0.6462,  0.6253,  0.2282]], device='cuda:0'),\n",
       " tensor([[-0.3114,  0.4341, -0.0990,  0.3708],\n",
       "         [-1.3154, -0.1055, -0.2016, -0.4860],\n",
       "         [-1.6167,  1.5081, -0.0997, -0.3783],\n",
       "         [ 1.1038, -0.8287,  0.0339,  0.5945]], device='cuda:0'),\n",
       " tensor([[ 0.3028,  2.8252, -3.7660,  0.5738],\n",
       "         [ 2.0844,  2.6154,  2.8681, -2.3480],\n",
       "         [ 1.4285, -0.5731,  0.1964,  0.0701],\n",
       "         [ 1.6044, -1.1173,  1.0481,  1.4407]], device='cuda:0'),\n",
       " tensor([[-1.2338, -2.0102,  2.9727,  1.2426],\n",
       "         [ 0.0083,  0.4192, -3.5557, -1.1119],\n",
       "         [-1.3820, -0.6218, -1.1465, -0.5034],\n",
       "         [ 1.4063,  0.7588,  0.0569, -2.9477]], device='cuda:0'),\n",
       " tensor([[-0.2849,  0.2999,  0.4496,  0.0048,  0.0221,  0.1207, -0.2810, -0.2065],\n",
       "         [ 0.5614,  0.0169,  0.4876, -0.0246, -0.0989,  1.0932, -0.6437, -1.4153],\n",
       "         [-0.3598,  0.2587,  0.1838,  0.3887, -0.3140,  0.5840,  0.6499,  0.1046],\n",
       "         [ 0.7284, -1.1677,  0.2725,  0.5790, -0.6789, -0.0387, -0.8244,  0.6049],\n",
       "         [-0.0495, -0.5960, -0.1172, -0.0508,  0.2725, -0.3149, -0.2592, -0.0886],\n",
       "         [-0.2727,  0.4136, -0.0441, -0.3578,  0.2274, -0.3225, -0.1569,  0.9056],\n",
       "         [ 0.1628, -0.7489,  0.3134,  0.3422,  0.4917,  0.1308, -0.2182, -0.0416],\n",
       "         [ 0.1756, -0.5694,  0.2095,  0.0497,  0.0962, -0.1077,  0.6452, -0.2003],\n",
       "         [-0.5893, -0.6737,  0.8537,  0.2770, -0.1138, -0.1607,  0.1925, -0.3253],\n",
       "         [ 0.4388, -0.1983,  0.6623,  0.3600, -0.6235, -1.1395,  0.1007,  0.3421],\n",
       "         [ 0.4846, -0.4382,  0.8508,  0.4781, -0.6978, -0.9993,  0.4191, -0.7986],\n",
       "         [-0.0261, -0.4504,  0.7311,  0.2560, -0.4906, -1.0721,  0.1038,  0.1613],\n",
       "         [-0.3471,  0.7153, -0.1865,  0.4415, -0.2211, -0.6422,  0.3082,  0.4187],\n",
       "         [-0.2223,  0.0170,  0.2192,  0.1170,  0.2671, -0.0506,  0.0122,  0.2032],\n",
       "         [ 0.5599, -0.6319, -0.0746,  0.3799,  0.1797, -0.3022,  0.2612, -0.0446],\n",
       "         [-0.0480,  0.5039,  0.4066, -0.2604, -0.1128, -0.3066, -0.2349, -0.1556],\n",
       "         [ 0.1073, -0.1356, -0.1069,  0.2051, -0.1446, -0.7447,  1.1262, -0.0024],\n",
       "         [ 0.3463,  0.2037,  0.4482, -0.1248, -0.2318, -0.6836, -0.2081,  0.7370],\n",
       "         [-0.4796, -0.3259, -0.1784,  0.8042,  0.2260,  0.5212, -0.4526,  0.0334],\n",
       "         [-0.1657,  0.3234,  0.3169, -0.2754, -0.1509,  0.0284, -0.0198,  0.2729],\n",
       "         [ 0.4032,  0.4549,  0.2131,  0.7060, -0.8352, -0.1200, -0.5588, -0.5478],\n",
       "         [ 0.0985,  0.2915,  0.3913, -0.2630,  0.4199, -0.4791, -0.1522, -0.2857],\n",
       "         [ 0.0958, -0.1393, -0.1063, -0.4052,  0.3194,  0.1385,  0.2409,  0.2302],\n",
       "         [-0.6956,  0.3199, -0.5051,  0.0890,  0.5738, -0.0738, -0.6488,  1.1911],\n",
       "         [ 0.2307, -0.1764,  0.2024, -0.2427, -0.4780,  0.1610,  0.0092,  0.8620],\n",
       "         [-0.1563, -0.4375,  0.6221,  0.4006,  0.7153, -0.5127,  0.0133, -0.0151],\n",
       "         [ 0.8702, -0.0281,  0.3771, -0.5410,  0.5147, -0.8690,  0.0526,  0.0043],\n",
       "         [ 0.4734, -0.5043,  0.1011,  0.6651,  0.2636, -0.3754,  0.5929, -0.3676],\n",
       "         [-1.1429,  0.0459, -0.0940,  0.9138,  0.4382, -0.1160, -0.0703, -0.1747],\n",
       "         [ 0.7977, -0.9040,  0.2740,  0.3044,  0.2011, -0.4707, -0.4249,  0.0866],\n",
       "         [-0.4915,  1.0026,  0.3869, -0.6557, -0.4901,  0.3913, -1.0774,  1.0682],\n",
       "         [-0.5071,  1.0034,  0.9002,  0.1776, -0.7245, -0.7056,  0.2005,  0.2847]],\n",
       "        device='cuda:0'),\n",
       " tensor([[-6.9717e-02, -7.3437e-01,  2.8858e-01,  1.0360e+00,  3.1043e-02,\n",
       "          -4.1016e-01,  1.2298e-01,  3.4299e-03, -5.2247e-01,  4.4517e-01,\n",
       "           3.4175e-01,  3.0731e-01, -5.7684e-01, -2.9390e-01,  4.9667e-01,\n",
       "           1.6866e-01,  8.3077e-02,  1.6735e-01, -3.6374e-01, -2.6004e-02,\n",
       "           4.0096e-01,  8.5225e-02, -2.4110e-01, -6.3022e-01,  4.6208e-01,\n",
       "          -2.3495e-01, -2.1355e-01,  3.0605e-01, -6.6778e-01,  8.7714e-01,\n",
       "           5.6904e-01,  3.6744e-01],\n",
       "         [ 2.6378e-01,  2.0255e-02, -3.0459e-01, -4.3932e-01, -5.0760e-02,\n",
       "          -7.3005e-02, -3.3593e-02, -3.4518e-01, -1.3711e-01, -3.4707e-01,\n",
       "          -1.2281e-01, -5.0618e-01, -9.3893e-02,  1.5396e-01, -3.0476e-01,\n",
       "           3.6214e-01, -8.4739e-01, -2.0155e-01, -5.0710e-01, -9.0357e-02,\n",
       "           5.2310e-01, -7.3304e-02, -3.5912e-02, -7.0239e-01, -5.4757e-01,\n",
       "          -4.6809e-01, -2.9237e-01, -3.7378e-01,  2.6118e-03, -2.7869e-01,\n",
       "          -1.1198e+00,  6.0900e-01],\n",
       "         [-2.1113e-01, -5.0589e-01,  3.3893e-01,  7.3036e-01,  7.6683e-02,\n",
       "           4.9326e-01, -5.3258e-02, -2.4629e-01,  2.5672e-02, -7.6116e-02,\n",
       "           3.3844e-01, -2.9422e-01,  2.3939e-01, -1.3990e-01,  2.9597e-01,\n",
       "           4.5240e-01, -2.5622e-01,  2.4177e-01,  2.2304e-01,  1.7101e-01,\n",
       "          -1.1864e-02,  4.2901e-01, -1.9327e-01,  8.0019e-02,  9.7019e-02,\n",
       "           9.5745e-02,  2.0451e-01,  5.6622e-02,  1.9175e-01,  5.2270e-01,\n",
       "           7.3118e-01,  5.7856e-01],\n",
       "         [ 8.3155e-02, -1.7421e-03,  4.4616e-01, -4.2944e-01,  5.6993e-02,\n",
       "           3.7794e-01, -1.5029e-02, -2.1872e-01, -1.0316e-02, -4.8227e-01,\n",
       "          -5.3568e-02, -3.1358e-01,  2.9528e-01,  2.7355e-02,  1.8379e-02,\n",
       "          -2.6083e-01, -9.6239e-02, -2.3659e-01,  6.9233e-01, -1.0481e-01,\n",
       "          -8.0271e-02,  8.5019e-02, -2.5710e-01,  5.3575e-01, -2.3620e-01,\n",
       "           3.1669e-01,  2.1883e-04,  2.1493e-01,  2.1759e-01, -2.2147e-04,\n",
       "           4.5041e-02, -7.0713e-01],\n",
       "         [ 2.4366e-02, -6.2796e-01,  2.4296e-01, -5.2803e-01,  1.5968e-01,\n",
       "          -5.6668e-01, -2.2799e-01, -4.7214e-02, -2.2099e-01, -6.6241e-01,\n",
       "          -1.1142e+00, -7.2080e-01, -4.6623e-01,  1.0444e-01,  1.0147e-02,\n",
       "          -2.0702e-01, -5.4766e-01, -4.9495e-01,  3.8284e-01, -1.4001e-03,\n",
       "          -1.2713e-01,  1.7289e-01, -5.6243e-04, -2.5075e-02,  2.9863e-01,\n",
       "           2.6319e-01,  4.5532e-01, -4.0052e-01,  1.4064e-01, -1.6804e-01,\n",
       "          -8.8222e-01, -1.5227e-01],\n",
       "         [-1.0176e-01, -8.3783e-02, -2.3458e-01, -6.3804e-02,  4.7917e-01,\n",
       "          -6.0351e-02,  7.4951e-01,  2.4364e-01,  6.2448e-01, -5.1305e-01,\n",
       "          -4.6675e-01, -2.6846e-01, -2.0591e-01, -1.5054e-01,  4.2153e-01,\n",
       "          -1.1592e-01, -2.1191e-02, -4.9606e-01, -6.2671e-01,  3.1050e-01,\n",
       "          -4.0270e-01, -2.9950e-01, -3.1017e-01, -9.3532e-01,  1.4428e-01,\n",
       "           4.4304e-01, -7.8540e-01,  4.3491e-01, -1.2073e+00,  3.3696e-01,\n",
       "           2.6736e-01,  1.7593e-02],\n",
       "         [ 2.3838e-01, -1.3328e+00,  2.5883e-01,  6.7604e-01,  2.6383e-01,\n",
       "           1.7874e-01,  1.2886e-01,  4.1436e-01,  4.0752e-01,  8.5383e-02,\n",
       "           9.8212e-02,  4.1639e-01, -3.9846e-01, -1.0106e-01,  9.1058e-02,\n",
       "           3.6439e-01, -4.9845e-02,  1.4470e-01, -9.4873e-02,  2.4809e-01,\n",
       "          -7.0400e-01,  4.3883e-01, -1.0473e-01, -2.9666e-01,  2.6387e-01,\n",
       "          -2.4787e-02,  3.5777e-01,  1.3708e-01, -1.2332e-01,  2.4255e-01,\n",
       "           4.4928e-01,  5.2416e-01],\n",
       "         [-1.9543e-01,  1.8225e-01,  4.3446e-01, -2.6606e-02,  3.1078e-02,\n",
       "           2.2145e-01, -5.6790e-02,  1.4269e-01,  2.5177e-01,  1.1113e-01,\n",
       "           3.0863e-01,  5.2836e-02,  5.1740e-01, -8.7580e-02,  2.4888e-01,\n",
       "          -2.8633e-01, -1.1550e-01,  2.3726e-02,  9.4497e-02, -1.0497e-02,\n",
       "           2.6449e-01, -2.6450e-01,  1.8040e-02,  3.3979e-01, -4.5554e-01,\n",
       "           4.2033e-01, -5.5481e-01,  4.0471e-01, -9.1691e-02,  2.1283e-01,\n",
       "           9.3307e-01, -8.0322e-01]], device='cuda:0')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.6917,  0.0032,  0.8365, -0.4448], device='cuda:0'),\n",
       " tensor([-0.1019,  0.2196, -0.2425, -1.5323], device='cuda:0'),\n",
       " tensor([ 0.1882,  0.4882,  0.1099, -0.2820], device='cuda:0'),\n",
       " tensor([-0.1711,  0.3454,  0.4972, -0.3358], device='cuda:0'),\n",
       " tensor([-0.7676,  1.7581, -2.3391, -1.0603], device='cuda:0'),\n",
       " tensor([ 0.4818, -0.4597,  1.4388,  0.2067], device='cuda:0'),\n",
       " tensor([-0.1802, -0.6315, -0.0504, -1.7324, -1.6006, -0.5083, -0.1448, -1.4494,\n",
       "         -0.5348, -1.2961, -0.1745, -0.5085, -0.7553, -0.3622, -0.6367, -0.7979,\n",
       "         -0.8459, -1.0748, -0.5841, -1.2785, -0.6875, -0.7121, -0.9463, -0.6996,\n",
       "         -0.8696, -0.4908, -1.1363, -1.0437, -1.3037, -0.6465, -0.3553, -0.2472],\n",
       "        device='cuda:0'),\n",
       " tensor([ 1.3050,  0.9573, -1.9485, -0.3157,  0.4417, -0.7795, -1.2490,  0.7661],\n",
       "        device='cuda:0'),\n",
       " tensor([-1.8495,  0.1299, -0.3234,  0.5582], device='cuda:0'),\n",
       " tensor([ 0.2067,  0.8082,  0.4997, -0.3433], device='cuda:0'),\n",
       " tensor([-0.3258,  0.1346,  0.0536, -0.0708], device='cuda:0'),\n",
       " tensor([ 0.2779,  0.4629, -0.2878,  0.1682], device='cuda:0'),\n",
       " tensor([-0.1294, -1.0317,  0.6554,  0.4348], device='cuda:0'),\n",
       " tensor([-0.8030, -0.6222, -0.8991,  0.2319], device='cuda:0'),\n",
       " tensor([-0.3787, -0.5465, -0.3795,  0.0746, -0.1646, -0.1810, -0.1416, -0.1415,\n",
       "         -0.3222, -0.0407,  0.2749,  0.2540,  0.2621,  0.1626, -0.1343,  0.1881,\n",
       "         -0.0868, -0.0768,  0.1916, -0.3164,  0.1996,  0.0826, -0.2562,  0.0023,\n",
       "          0.2402, -0.1208, -0.4206, -0.0801,  0.0599, -0.3463, -0.2253, -0.0234],\n",
       "        device='cuda:0'),\n",
       " tensor([ 0.4113, -1.0062,  0.2239,  0.2864, -0.4606, -0.4435, -0.7807,  0.8094],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViT_LowRank(w, b , RANK, (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decomposed layers\n",
    "load_sd_decomp(torch.load(BRANCH_LOC, map_location=device), model, DECOMPOSED_LAYERS)\n",
    "\n",
    "# Optimizers for both models\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer_full = torch.optim.SGD(model_original.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using checkpoint, creation of model with only LC-checkpoint and another with LC-checkpoint + LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8970996141433716\n",
      "LC Training Loss (Full): 1.8970996141433716\n",
      "Training Accuracy | Decomposed: 0.5625, Full : 0.5625\n",
      "Epoch: 0, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.710892677307129\n",
      "LC Training Loss (Full): 1.7108980417251587\n",
      "Epoch: 0, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.8837192058563232\n",
      "LC Training Loss (Full): 1.8828917741775513\n",
      "Epoch: 0, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.6205984354019165\n",
      "LC Training Loss (Full): 1.6181761026382446\n",
      "Epoch: 0, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.775834321975708\n",
      "LC Training Loss (Full): 1.7751952409744263\n",
      "Epoch: 0, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.7042368650436401\n",
      "LC Training Loss (Full): 1.7056975364685059\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.715\n",
      "model accuracy: 0.714\n",
      "model accuracy: 0.714\n",
      "model accuracy: 0.715\n",
      "Full accuracy: 0.715, LC accuracy: 0.715, Decomposed-Full accuracy: 0.714, Decomposed-Restored accuracy: 0.714\n",
      "Epoch: 0, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.8689907789230347\n",
      "LC Training Loss (Full): 1.8688310384750366\n",
      "Epoch: 0, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.684546947479248\n",
      "LC Training Loss (Full): 1.6850944757461548\n",
      "Epoch: 0, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.7866209745407104\n",
      "LC Training Loss (Full): 1.7876830101013184\n",
      "Epoch: 0, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_0\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_0\n",
      "LoRA+LC Training Loss (Decomposed): 1.7068647146224976\n",
      "LC Training Loss (Full): 1.7070538997650146\n",
      "Epoch: 0, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7362565994262695\n",
      "LC Training Loss (Full): 1.73409903049469\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.717\n",
      "model accuracy: 0.714\n",
      "model accuracy: 0.714\n",
      "model accuracy: 0.715\n",
      "Full accuracy: 0.717, LC accuracy: 0.715, Decomposed-Full accuracy: 0.714, Decomposed-Restored accuracy: 0.714\n",
      "Epoch: 0, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.7374440431594849\n",
      "LC Training Loss (Full): 1.734911322593689\n",
      "Epoch: 0, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.6487226486206055\n",
      "LC Training Loss (Full): 1.649316430091858\n",
      "Epoch: 0, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.7841694355010986\n",
      "LC Training Loss (Full): 1.7858482599258423\n",
      "Epoch: 0, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.652828574180603\n",
      "LC Training Loss (Full): 1.6509124040603638\n",
      "Epoch: 0, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.5859804153442383\n",
      "LC Training Loss (Full): 1.5871669054031372\n",
      "Training Accuracy | Decomposed: 0.875, Full : 0.875\n",
      "model accuracy: 0.721\n",
      "model accuracy: 0.715\n",
      "model accuracy: 0.715\n",
      "model accuracy: 0.719\n",
      "Full accuracy: 0.721, LC accuracy: 0.719, Decomposed-Full accuracy: 0.715, Decomposed-Restored accuracy: 0.715\n",
      "Epoch: 0, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.7271147966384888\n",
      "LC Training Loss (Full): 1.7151561975479126\n",
      "Epoch: 0, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.915898323059082\n",
      "LC Training Loss (Full): 1.9185453653335571\n",
      "Epoch: 0, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.7717612981796265\n",
      "LC Training Loss (Full): 1.7719473838806152\n",
      "Epoch: 0, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_1\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_1\n",
      "LoRA+LC Training Loss (Decomposed): 1.8770078420639038\n",
      "LC Training Loss (Full): 1.8643465042114258\n",
      "Epoch: 0, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7113020420074463\n",
      "LC Training Loss (Full): 1.7117059230804443\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.726\n",
      "model accuracy: 0.719\n",
      "model accuracy: 0.719\n",
      "model accuracy: 0.726\n",
      "Full accuracy: 0.726, LC accuracy: 0.726, Decomposed-Full accuracy: 0.719, Decomposed-Restored accuracy: 0.719\n",
      "Epoch: 0, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.8040432929992676\n",
      "LC Training Loss (Full): 1.8038586378097534\n",
      "Epoch: 0, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.6972908973693848\n",
      "LC Training Loss (Full): 1.6973599195480347\n",
      "Epoch: 0, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.7110645771026611\n",
      "LC Training Loss (Full): 1.7108936309814453\n",
      "Epoch: 0, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.7169485092163086\n",
      "LC Training Loss (Full): 1.7203261852264404\n",
      "Epoch: 0, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.7148025035858154\n",
      "LC Training Loss (Full): 1.6901428699493408\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.726\n",
      "model accuracy: 0.724\n",
      "model accuracy: 0.721\n",
      "model accuracy: 0.727\n",
      "Full accuracy: 0.726, LC accuracy: 0.727, Decomposed-Full accuracy: 0.724, Decomposed-Restored accuracy: 0.721\n",
      "Epoch: 0, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.8151847124099731\n",
      "LC Training Loss (Full): 1.8117940425872803\n",
      "Epoch: 0, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.6294951438903809\n",
      "LC Training Loss (Full): 1.6261729001998901\n",
      "Epoch: 0, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.7674708366394043\n",
      "LC Training Loss (Full): 1.7523151636123657\n",
      "Epoch: 0, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_2\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_2\n",
      "LoRA+LC Training Loss (Decomposed): 1.7419499158859253\n",
      "LC Training Loss (Full): 1.7406082153320312\n",
      "Epoch: 0, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_3\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_3/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.681894063949585\n",
      "LC Training Loss (Full): 1.676714301109314\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.727\n",
      "model accuracy: 0.726\n",
      "model accuracy: 0.73\n",
      "Full accuracy: 0.732, LC accuracy: 0.73, Decomposed-Full accuracy: 0.727, Decomposed-Restored accuracy: 0.726\n",
      "Epoch: 0, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_3\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_3\n",
      "LoRA+LC Training Loss (Decomposed): 1.9544951915740967\n",
      "LC Training Loss (Full): 1.9562153816223145\n",
      "Epoch: 1, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7402470111846924\n",
      "LC Training Loss (Full): 1.7410608530044556\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "Epoch: 1, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.7429730892181396\n",
      "LC Training Loss (Full): 1.7425975799560547\n",
      "Epoch: 1, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.7625770568847656\n",
      "LC Training Loss (Full): 1.7524123191833496\n",
      "Epoch: 1, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.7730742692947388\n",
      "LC Training Loss (Full): 1.7738500833511353\n",
      "Epoch: 1, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.6867358684539795\n",
      "LC Training Loss (Full): 1.6793346405029297\n",
      "Epoch: 1, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.835213303565979\n",
      "LC Training Loss (Full): 1.8019274473190308\n",
      "Training Accuracy | Decomposed: 0.625, Full : 0.65625\n",
      "model accuracy: 0.733\n",
      "model accuracy: 0.727\n",
      "model accuracy: 0.728\n",
      "model accuracy: 0.733\n",
      "Full accuracy: 0.733, LC accuracy: 0.733, Decomposed-Full accuracy: 0.727, Decomposed-Restored accuracy: 0.728\n",
      "Epoch: 1, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.837007761001587\n",
      "LC Training Loss (Full): 1.8366434574127197\n",
      "Epoch: 1, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.759442687034607\n",
      "LC Training Loss (Full): 1.745651125907898\n",
      "Epoch: 1, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.8604331016540527\n",
      "LC Training Loss (Full): 1.8502118587493896\n",
      "Epoch: 1, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_4\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_4\n",
      "LoRA+LC Training Loss (Decomposed): 1.6797761917114258\n",
      "LC Training Loss (Full): 1.677736520767212\n",
      "Epoch: 1, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7293258905410767\n",
      "LC Training Loss (Full): 1.7377190589904785\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.71875\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.728\n",
      "model accuracy: 0.73\n",
      "model accuracy: 0.735\n",
      "Full accuracy: 0.736, LC accuracy: 0.735, Decomposed-Full accuracy: 0.728, Decomposed-Restored accuracy: 0.73\n",
      "Epoch: 1, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.8954843282699585\n",
      "LC Training Loss (Full): 1.8770869970321655\n",
      "Epoch: 1, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.8348501920700073\n",
      "LC Training Loss (Full): 1.825373888015747\n",
      "Epoch: 1, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.6866871118545532\n",
      "LC Training Loss (Full): 1.6789758205413818\n",
      "Epoch: 1, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.5910676717758179\n",
      "LC Training Loss (Full): 1.5869967937469482\n",
      "Epoch: 1, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.7100954055786133\n",
      "LC Training Loss (Full): 1.7115671634674072\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.731\n",
      "model accuracy: 0.731\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.736, LC accuracy: 0.736, Decomposed-Full accuracy: 0.731, Decomposed-Restored accuracy: 0.731\n",
      "Epoch: 1, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.711686134338379\n",
      "LC Training Loss (Full): 1.7107700109481812\n",
      "Epoch: 1, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.65628981590271\n",
      "LC Training Loss (Full): 1.6497578620910645\n",
      "Epoch: 1, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.6672242879867554\n",
      "LC Training Loss (Full): 1.6534374952316284\n",
      "Epoch: 1, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_5\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_5\n",
      "LoRA+LC Training Loss (Decomposed): 1.6827023029327393\n",
      "LC Training Loss (Full): 1.6799594163894653\n",
      "Epoch: 1, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.739173173904419\n",
      "LC Training Loss (Full): 1.7426745891571045\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.731\n",
      "model accuracy: 0.731\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.736, LC accuracy: 0.736, Decomposed-Full accuracy: 0.731, Decomposed-Restored accuracy: 0.731\n",
      "Epoch: 1, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.6112805604934692\n",
      "LC Training Loss (Full): 1.5946892499923706\n",
      "Epoch: 1, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.8609144687652588\n",
      "LC Training Loss (Full): 1.8594783544540405\n",
      "Epoch: 1, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.6801456212997437\n",
      "LC Training Loss (Full): 1.6735973358154297\n",
      "Epoch: 1, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.6508102416992188\n",
      "LC Training Loss (Full): 1.6503137350082397\n",
      "Epoch: 1, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.710335612297058\n",
      "LC Training Loss (Full): 1.7095857858657837\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.731\n",
      "model accuracy: 0.731\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.736, LC accuracy: 0.736, Decomposed-Full accuracy: 0.731, Decomposed-Restored accuracy: 0.731\n",
      "Epoch: 1, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.8147505521774292\n",
      "LC Training Loss (Full): 1.8077419996261597\n",
      "Epoch: 1, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.7555948495864868\n",
      "LC Training Loss (Full): 1.753637433052063\n",
      "Epoch: 1, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.8083187341690063\n",
      "LC Training Loss (Full): 1.8148846626281738\n",
      "Epoch: 1, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_6\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_6\n",
      "LoRA+LC Training Loss (Decomposed): 1.6836193799972534\n",
      "LC Training Loss (Full): 1.6817680597305298\n",
      "Epoch: 1, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_7\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_7/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6631395816802979\n",
      "LC Training Loss (Full): 1.668936014175415\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.735\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.735, LC accuracy: 0.736, Decomposed-Full accuracy: 0.732, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 1, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_7\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_7\n",
      "LoRA+LC Training Loss (Decomposed): 1.586275339126587\n",
      "LC Training Loss (Full): 1.5862185955047607\n",
      "Epoch: 2, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.67849600315094\n",
      "LC Training Loss (Full): 1.6785551309585571\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "Epoch: 2, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.6488564014434814\n",
      "LC Training Loss (Full): 1.649111270904541\n",
      "Epoch: 2, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.7119390964508057\n",
      "LC Training Loss (Full): 1.705893874168396\n",
      "Epoch: 2, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.716596245765686\n",
      "LC Training Loss (Full): 1.7083009481430054\n",
      "Epoch: 2, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.721011757850647\n",
      "LC Training Loss (Full): 1.7278404235839844\n",
      "Epoch: 2, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.6539766788482666\n",
      "LC Training Loss (Full): 1.6523716449737549\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.736, LC accuracy: 0.736, Decomposed-Full accuracy: 0.732, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 2, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.6822150945663452\n",
      "LC Training Loss (Full): 1.6809839010238647\n",
      "Epoch: 2, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.7381680011749268\n",
      "LC Training Loss (Full): 1.7087739706039429\n",
      "Epoch: 2, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.7715281248092651\n",
      "LC Training Loss (Full): 1.7713614702224731\n",
      "Epoch: 2, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_8\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_8\n",
      "LoRA+LC Training Loss (Decomposed): 1.7790732383728027\n",
      "LC Training Loss (Full): 1.7746374607086182\n",
      "Epoch: 2, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7734699249267578\n",
      "LC Training Loss (Full): 1.7735639810562134\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.736, LC accuracy: 0.736, Decomposed-Full accuracy: 0.732, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 2, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.8151425123214722\n",
      "LC Training Loss (Full): 1.8077538013458252\n",
      "Epoch: 2, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.72981595993042\n",
      "LC Training Loss (Full): 1.7301597595214844\n",
      "Epoch: 2, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.7417495250701904\n",
      "LC Training Loss (Full): 1.7123749256134033\n",
      "Epoch: 2, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.6759049892425537\n",
      "LC Training Loss (Full): 1.6762259006500244\n",
      "Epoch: 2, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.8124767541885376\n",
      "LC Training Loss (Full): 1.8055881261825562\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.736\n",
      "Full accuracy: 0.736, LC accuracy: 0.736, Decomposed-Full accuracy: 0.732, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 2, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.6526122093200684\n",
      "LC Training Loss (Full): 1.6548699140548706\n",
      "Epoch: 2, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.6812303066253662\n",
      "LC Training Loss (Full): 1.6808844804763794\n",
      "Epoch: 2, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.805908441543579\n",
      "LC Training Loss (Full): 1.7928643226623535\n",
      "Epoch: 2, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_9\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_9\n",
      "LoRA+LC Training Loss (Decomposed): 1.6493020057678223\n",
      "LC Training Loss (Full): 1.6489757299423218\n",
      "Epoch: 2, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7414884567260742\n",
      "LC Training Loss (Full): 1.7410670518875122\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.739\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.738\n",
      "Full accuracy: 0.739, LC accuracy: 0.738, Decomposed-Full accuracy: 0.732, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 2, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.7111859321594238\n",
      "LC Training Loss (Full): 1.711011528968811\n",
      "Epoch: 2, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.767128825187683\n",
      "LC Training Loss (Full): 1.747154712677002\n",
      "Epoch: 2, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.622881531715393\n",
      "LC Training Loss (Full): 1.608161449432373\n",
      "Epoch: 2, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.7467982769012451\n",
      "LC Training Loss (Full): 1.7432986497879028\n",
      "Epoch: 2, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.709908127784729\n",
      "LC Training Loss (Full): 1.699850082397461\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.739\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.739\n",
      "Full accuracy: 0.739, LC accuracy: 0.739, Decomposed-Full accuracy: 0.732, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 2, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.9635196924209595\n",
      "LC Training Loss (Full): 1.9668461084365845\n",
      "Epoch: 2, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.7789342403411865\n",
      "LC Training Loss (Full): 1.7730281352996826\n",
      "Epoch: 2, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.6802746057510376\n",
      "LC Training Loss (Full): 1.6799298524856567\n",
      "Epoch: 2, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_10\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_10\n",
      "LoRA+LC Training Loss (Decomposed): 1.716099739074707\n",
      "LC Training Loss (Full): 1.689300775527954\n",
      "Epoch: 2, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_11\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_11/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8465970754623413\n",
      "LC Training Loss (Full): 1.8478330373764038\n",
      "Training Accuracy | Decomposed: 0.625, Full : 0.59375\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.733\n",
      "model accuracy: 0.732\n",
      "model accuracy: 0.74\n",
      "Full accuracy: 0.741, LC accuracy: 0.74, Decomposed-Full accuracy: 0.733, Decomposed-Restored accuracy: 0.732\n",
      "Epoch: 2, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_11\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_11\n",
      "LoRA+LC Training Loss (Decomposed): 1.4612375497817993\n",
      "LC Training Loss (Full): 1.461217999458313\n",
      "Epoch: 3, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7438832521438599\n",
      "LC Training Loss (Full): 1.742071509361267\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "Epoch: 3, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.677736759185791\n",
      "LC Training Loss (Full): 1.675693392753601\n",
      "Epoch: 3, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.7345168590545654\n",
      "LC Training Loss (Full): 1.7334407567977905\n",
      "Epoch: 3, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.7725237607955933\n",
      "LC Training Loss (Full): 1.7721567153930664\n",
      "Epoch: 3, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.7964434623718262\n",
      "LC Training Loss (Full): 1.7528048753738403\n",
      "Epoch: 3, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.6480580568313599\n",
      "LC Training Loss (Full): 1.6485880613327026\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.733\n",
      "model accuracy: 0.733\n",
      "model accuracy: 0.741\n",
      "Full accuracy: 0.741, LC accuracy: 0.741, Decomposed-Full accuracy: 0.733, Decomposed-Restored accuracy: 0.733\n",
      "Epoch: 3, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.7439473867416382\n",
      "LC Training Loss (Full): 1.7420140504837036\n",
      "Epoch: 3, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.7709803581237793\n",
      "LC Training Loss (Full): 1.7703089714050293\n",
      "Epoch: 3, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.6844254732131958\n",
      "LC Training Loss (Full): 1.6813141107559204\n",
      "Epoch: 3, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_12\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_12\n",
      "LoRA+LC Training Loss (Decomposed): 1.7736276388168335\n",
      "LC Training Loss (Full): 1.7729287147521973\n",
      "Epoch: 3, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7736696004867554\n",
      "LC Training Loss (Full): 1.772620677947998\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.74\n",
      "model accuracy: 0.733\n",
      "model accuracy: 0.733\n",
      "model accuracy: 0.74\n",
      "Full accuracy: 0.74, LC accuracy: 0.74, Decomposed-Full accuracy: 0.733, Decomposed-Restored accuracy: 0.733\n",
      "Epoch: 3, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.7951592206954956\n",
      "LC Training Loss (Full): 1.7751446962356567\n",
      "Epoch: 3, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.7702226638793945\n",
      "LC Training Loss (Full): 1.754337191581726\n",
      "Epoch: 3, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.7813379764556885\n",
      "LC Training Loss (Full): 1.7566957473754883\n",
      "Epoch: 3, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.7406057119369507\n",
      "LC Training Loss (Full): 1.741310477256775\n",
      "Epoch: 3, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.6220017671585083\n",
      "LC Training Loss (Full): 1.6223150491714478\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.743\n",
      "Full accuracy: 0.743, LC accuracy: 0.743, Decomposed-Full accuracy: 0.736, Decomposed-Restored accuracy: 0.736\n",
      "Epoch: 3, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.6074925661087036\n",
      "LC Training Loss (Full): 1.608718752861023\n",
      "Epoch: 3, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.655106544494629\n",
      "LC Training Loss (Full): 1.652902364730835\n",
      "Epoch: 3, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.804693341255188\n",
      "LC Training Loss (Full): 1.8047068119049072\n",
      "Epoch: 3, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_13\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_13\n",
      "LoRA+LC Training Loss (Decomposed): 1.7139759063720703\n",
      "LC Training Loss (Full): 1.7113398313522339\n",
      "Epoch: 3, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6454607248306274\n",
      "LC Training Loss (Full): 1.6423976421356201\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.742\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.742\n",
      "Full accuracy: 0.742, LC accuracy: 0.742, Decomposed-Full accuracy: 0.736, Decomposed-Restored accuracy: 0.736\n",
      "Epoch: 3, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.7004095315933228\n",
      "LC Training Loss (Full): 1.6828949451446533\n",
      "Epoch: 3, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.771215558052063\n",
      "LC Training Loss (Full): 1.7395265102386475\n",
      "Epoch: 3, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.7436772584915161\n",
      "LC Training Loss (Full): 1.7467873096466064\n",
      "Epoch: 3, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.6488004922866821\n",
      "LC Training Loss (Full): 1.6487705707550049\n",
      "Epoch: 3, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.6340441703796387\n",
      "LC Training Loss (Full): 1.626206398010254\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.84375\n",
      "model accuracy: 0.742\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.743\n",
      "Full accuracy: 0.742, LC accuracy: 0.743, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.736\n",
      "Epoch: 3, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.6183494329452515\n",
      "LC Training Loss (Full): 1.6177756786346436\n",
      "Epoch: 3, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.8183172941207886\n",
      "LC Training Loss (Full): 1.786571741104126\n",
      "Epoch: 3, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.7815430164337158\n",
      "LC Training Loss (Full): 1.7631876468658447\n",
      "Epoch: 3, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_14\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_14\n",
      "LoRA+LC Training Loss (Decomposed): 1.8684256076812744\n",
      "LC Training Loss (Full): 1.8660478591918945\n",
      "Epoch: 3, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_15\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_15/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.752967357635498\n",
      "LC Training Loss (Full): 1.7657493352890015\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.6875\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.744\n",
      "Full accuracy: 0.744, LC accuracy: 0.744, Decomposed-Full accuracy: 0.736, Decomposed-Restored accuracy: 0.736\n",
      "Epoch: 3, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_15\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_15\n",
      "LoRA+LC Training Loss (Decomposed): 1.7277215719223022\n",
      "LC Training Loss (Full): 1.7180160284042358\n",
      "Epoch: 4, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.721301555633545\n",
      "LC Training Loss (Full): 1.7247611284255981\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "Epoch: 4, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.6476247310638428\n",
      "LC Training Loss (Full): 1.6368032693862915\n",
      "Epoch: 4, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.7115836143493652\n",
      "LC Training Loss (Full): 1.7109737396240234\n",
      "Epoch: 4, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.8669068813323975\n",
      "LC Training Loss (Full): 1.8671129941940308\n",
      "Epoch: 4, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.7238715887069702\n",
      "LC Training Loss (Full): 1.7180914878845215\n",
      "Epoch: 4, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.6255574226379395\n",
      "LC Training Loss (Full): 1.6050301790237427\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.875\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.745\n",
      "Full accuracy: 0.745, LC accuracy: 0.745, Decomposed-Full accuracy: 0.736, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 4, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.7097930908203125\n",
      "LC Training Loss (Full): 1.7097740173339844\n",
      "Epoch: 4, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.7146189212799072\n",
      "LC Training Loss (Full): 1.6828043460845947\n",
      "Epoch: 4, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.7030123472213745\n",
      "LC Training Loss (Full): 1.696010947227478\n",
      "Epoch: 4, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_16\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_16\n",
      "LoRA+LC Training Loss (Decomposed): 1.7131884098052979\n",
      "LC Training Loss (Full): 1.711713194847107\n",
      "Epoch: 4, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.678940773010254\n",
      "LC Training Loss (Full): 1.6789804697036743\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.745\n",
      "Full accuracy: 0.745, LC accuracy: 0.745, Decomposed-Full accuracy: 0.736, Decomposed-Restored accuracy: 0.736\n",
      "Epoch: 4, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.784588098526001\n",
      "LC Training Loss (Full): 1.7865675687789917\n",
      "Epoch: 4, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.8325151205062866\n",
      "LC Training Loss (Full): 1.8027563095092773\n",
      "Epoch: 4, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.7160038948059082\n",
      "LC Training Loss (Full): 1.7127461433410645\n",
      "Epoch: 4, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.8043396472930908\n",
      "LC Training Loss (Full): 1.7987946271896362\n",
      "Epoch: 4, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.7110381126403809\n",
      "LC Training Loss (Full): 1.7142376899719238\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.746\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.736\n",
      "model accuracy: 0.746\n",
      "Full accuracy: 0.746, LC accuracy: 0.746, Decomposed-Full accuracy: 0.736, Decomposed-Restored accuracy: 0.736\n",
      "Epoch: 4, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.803280234336853\n",
      "LC Training Loss (Full): 1.8038156032562256\n",
      "Epoch: 4, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.710745930671692\n",
      "LC Training Loss (Full): 1.7102420330047607\n",
      "Epoch: 4, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.7469868659973145\n",
      "LC Training Loss (Full): 1.7484352588653564\n",
      "Epoch: 4, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_17\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_17\n",
      "LoRA+LC Training Loss (Decomposed): 1.6294623613357544\n",
      "LC Training Loss (Full): 1.591438889503479\n",
      "Epoch: 4, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7185163497924805\n",
      "LC Training Loss (Full): 1.716886043548584\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.747\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.747\n",
      "Full accuracy: 0.747, LC accuracy: 0.747, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 4, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.7818142175674438\n",
      "LC Training Loss (Full): 1.7729313373565674\n",
      "Epoch: 4, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.5887219905853271\n",
      "LC Training Loss (Full): 1.5872385501861572\n",
      "Epoch: 4, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.7112843990325928\n",
      "LC Training Loss (Full): 1.7105379104614258\n",
      "Epoch: 4, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.6574409008026123\n",
      "LC Training Loss (Full): 1.6562316417694092\n",
      "Epoch: 4, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.6865538358688354\n",
      "LC Training Loss (Full): 1.6812390089035034\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.745\n",
      "Full accuracy: 0.745, LC accuracy: 0.745, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 4, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.8359142541885376\n",
      "LC Training Loss (Full): 1.832384705543518\n",
      "Epoch: 4, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.64646577835083\n",
      "LC Training Loss (Full): 1.618377447128296\n",
      "Epoch: 4, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.8359651565551758\n",
      "LC Training Loss (Full): 1.8352988958358765\n",
      "Epoch: 4, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_18\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_18\n",
      "LoRA+LC Training Loss (Decomposed): 1.7684890031814575\n",
      "LC Training Loss (Full): 1.7505362033843994\n",
      "Epoch: 4, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_19\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_19/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8006446361541748\n",
      "LC Training Loss (Full): 1.778985857963562\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.6875\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.744\n",
      "Full accuracy: 0.744, LC accuracy: 0.744, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 4, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_19\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_19\n",
      "LoRA+LC Training Loss (Decomposed): 1.461429476737976\n",
      "LC Training Loss (Full): 1.4619650840759277\n",
      "Epoch: 5, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6793842315673828\n",
      "LC Training Loss (Full): 1.6792868375778198\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "Epoch: 5, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.7400790452957153\n",
      "LC Training Loss (Full): 1.741221308708191\n",
      "Epoch: 5, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.5623915195465088\n",
      "LC Training Loss (Full): 1.5595041513442993\n",
      "Epoch: 5, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.6479355096817017\n",
      "LC Training Loss (Full): 1.621193766593933\n",
      "Epoch: 5, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.680202841758728\n",
      "LC Training Loss (Full): 1.6796318292617798\n",
      "Epoch: 5, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.7097046375274658\n",
      "LC Training Loss (Full): 1.703553318977356\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.744\n",
      "Full accuracy: 0.743, LC accuracy: 0.744, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 5, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.6475363969802856\n",
      "LC Training Loss (Full): 1.6449724435806274\n",
      "Epoch: 5, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.8047181367874146\n",
      "LC Training Loss (Full): 1.8038023710250854\n",
      "Epoch: 5, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.6194549798965454\n",
      "LC Training Loss (Full): 1.6172703504562378\n",
      "Epoch: 5, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_20\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_20\n",
      "LoRA+LC Training Loss (Decomposed): 1.7101457118988037\n",
      "LC Training Loss (Full): 1.6784061193466187\n",
      "Epoch: 5, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7839703559875488\n",
      "LC Training Loss (Full): 1.7709084749221802\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.743\n",
      "Full accuracy: 0.743, LC accuracy: 0.743, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 5, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.685100793838501\n",
      "LC Training Loss (Full): 1.6788378953933716\n",
      "Epoch: 5, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.7060489654541016\n",
      "LC Training Loss (Full): 1.6824226379394531\n",
      "Epoch: 5, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.7735991477966309\n",
      "LC Training Loss (Full): 1.7741508483886719\n",
      "Epoch: 5, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.9035608768463135\n",
      "LC Training Loss (Full): 1.9124693870544434\n",
      "Epoch: 5, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.67302405834198\n",
      "LC Training Loss (Full): 1.6714282035827637\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.743\n",
      "Full accuracy: 0.743, LC accuracy: 0.743, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 5, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.7538795471191406\n",
      "LC Training Loss (Full): 1.7416194677352905\n",
      "Epoch: 5, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.75619637966156\n",
      "LC Training Loss (Full): 1.7416936159133911\n",
      "Epoch: 5, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.7125593423843384\n",
      "LC Training Loss (Full): 1.7024558782577515\n",
      "Epoch: 5, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_21\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_21\n",
      "LoRA+LC Training Loss (Decomposed): 1.8336682319641113\n",
      "LC Training Loss (Full): 1.8016685247421265\n",
      "Epoch: 5, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6787337064743042\n",
      "LC Training Loss (Full): 1.6783021688461304\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.747\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.747\n",
      "Full accuracy: 0.747, LC accuracy: 0.747, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 5, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.7393397092819214\n",
      "LC Training Loss (Full): 1.73897123336792\n",
      "Epoch: 5, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.6483533382415771\n",
      "LC Training Loss (Full): 1.648386836051941\n",
      "Epoch: 5, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.7435362339019775\n",
      "LC Training Loss (Full): 1.746039867401123\n",
      "Epoch: 5, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.8060898780822754\n",
      "LC Training Loss (Full): 1.8071166276931763\n",
      "Epoch: 5, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.6797654628753662\n",
      "LC Training Loss (Full): 1.679834246635437\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.745\n",
      "Full accuracy: 0.745, LC accuracy: 0.745, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 5, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.7934846878051758\n",
      "LC Training Loss (Full): 1.7953954935073853\n",
      "Epoch: 5, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.8277735710144043\n",
      "LC Training Loss (Full): 1.7745442390441895\n",
      "Epoch: 5, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.5646231174468994\n",
      "LC Training Loss (Full): 1.5655544996261597\n",
      "Epoch: 5, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_22\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_22\n",
      "LoRA+LC Training Loss (Decomposed): 1.727281928062439\n",
      "LC Training Loss (Full): 1.7052924633026123\n",
      "Epoch: 5, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_23\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_23/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8667963743209839\n",
      "LC Training Loss (Full): 1.8510127067565918\n",
      "Training Accuracy | Decomposed: 0.59375, Full : 0.59375\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.748\n",
      "Full accuracy: 0.748, LC accuracy: 0.748, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 5, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_23\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_23\n",
      "LoRA+LC Training Loss (Decomposed): 1.8308929204940796\n",
      "LC Training Loss (Full): 1.8304647207260132\n",
      "Epoch: 6, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.710150122642517\n",
      "LC Training Loss (Full): 1.7106962203979492\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "Epoch: 6, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.6179002523422241\n",
      "LC Training Loss (Full): 1.6173161268234253\n",
      "Epoch: 6, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.644655704498291\n",
      "LC Training Loss (Full): 1.6143064498901367\n",
      "Epoch: 6, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.8834131956100464\n",
      "LC Training Loss (Full): 1.8296396732330322\n",
      "Epoch: 6, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.7104439735412598\n",
      "LC Training Loss (Full): 1.7086470127105713\n",
      "Epoch: 6, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.587926983833313\n",
      "LC Training Loss (Full): 1.586347222328186\n",
      "Training Accuracy | Decomposed: 0.875, Full : 0.875\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.748\n",
      "Full accuracy: 0.748, LC accuracy: 0.748, Decomposed-Full accuracy: 0.737, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 6, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.9300614595413208\n",
      "LC Training Loss (Full): 1.9300419092178345\n",
      "Epoch: 6, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.6812528371810913\n",
      "LC Training Loss (Full): 1.6515204906463623\n",
      "Epoch: 6, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.5558743476867676\n",
      "LC Training Loss (Full): 1.5526329278945923\n",
      "Epoch: 6, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_24\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_24\n",
      "LoRA+LC Training Loss (Decomposed): 1.7096779346466064\n",
      "LC Training Loss (Full): 1.6817364692687988\n",
      "Epoch: 6, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6827787160873413\n",
      "LC Training Loss (Full): 1.679932951927185\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.749\n",
      "model accuracy: 0.739\n",
      "model accuracy: 0.737\n",
      "model accuracy: 0.749\n",
      "Full accuracy: 0.749, LC accuracy: 0.749, Decomposed-Full accuracy: 0.739, Decomposed-Restored accuracy: 0.737\n",
      "Epoch: 6, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.6490037441253662\n",
      "LC Training Loss (Full): 1.626824140548706\n",
      "Epoch: 6, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.7477021217346191\n",
      "LC Training Loss (Full): 1.7515629529953003\n",
      "Epoch: 6, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.6587878465652466\n",
      "LC Training Loss (Full): 1.6680196523666382\n",
      "Epoch: 6, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.881775140762329\n",
      "LC Training Loss (Full): 1.871176838874817\n",
      "Epoch: 6, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.7131954431533813\n",
      "LC Training Loss (Full): 1.7068381309509277\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.749\n",
      "model accuracy: 0.738\n",
      "model accuracy: 0.738\n",
      "model accuracy: 0.749\n",
      "Full accuracy: 0.749, LC accuracy: 0.749, Decomposed-Full accuracy: 0.738, Decomposed-Restored accuracy: 0.738\n",
      "Epoch: 6, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.7420673370361328\n",
      "LC Training Loss (Full): 1.7420507669448853\n",
      "Epoch: 6, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.7669836282730103\n",
      "LC Training Loss (Full): 1.7443950176239014\n",
      "Epoch: 6, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.7186355590820312\n",
      "LC Training Loss (Full): 1.7141473293304443\n",
      "Epoch: 6, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_25\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_25\n",
      "LoRA+LC Training Loss (Decomposed): 1.7215968370437622\n",
      "LC Training Loss (Full): 1.740017056465149\n",
      "Epoch: 6, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6595876216888428\n",
      "LC Training Loss (Full): 1.6540412902832031\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.738\n",
      "model accuracy: 0.738\n",
      "model accuracy: 0.75\n",
      "Full accuracy: 0.75, LC accuracy: 0.75, Decomposed-Full accuracy: 0.738, Decomposed-Restored accuracy: 0.738\n",
      "Epoch: 6, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.7424538135528564\n",
      "LC Training Loss (Full): 1.7422714233398438\n",
      "Epoch: 6, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.7620787620544434\n",
      "LC Training Loss (Full): 1.7136595249176025\n",
      "Epoch: 6, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.7736213207244873\n",
      "LC Training Loss (Full): 1.773029088973999\n",
      "Epoch: 6, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.7743096351623535\n",
      "LC Training Loss (Full): 1.7754836082458496\n",
      "Epoch: 6, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.727614164352417\n",
      "LC Training Loss (Full): 1.7291492223739624\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.74\n",
      "model accuracy: 0.74\n",
      "model accuracy: 0.75\n",
      "Full accuracy: 0.75, LC accuracy: 0.75, Decomposed-Full accuracy: 0.74, Decomposed-Restored accuracy: 0.74\n",
      "Epoch: 6, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.657922625541687\n",
      "LC Training Loss (Full): 1.6486663818359375\n",
      "Epoch: 6, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.738178014755249\n",
      "LC Training Loss (Full): 1.7105027437210083\n",
      "Epoch: 6, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.7405750751495361\n",
      "LC Training Loss (Full): 1.730614185333252\n",
      "Epoch: 6, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_26\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_26\n",
      "LoRA+LC Training Loss (Decomposed): 1.7478224039077759\n",
      "LC Training Loss (Full): 1.747185230255127\n",
      "Epoch: 6, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_27\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_27/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8039427995681763\n",
      "LC Training Loss (Full): 1.8027236461639404\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.74\n",
      "model accuracy: 0.74\n",
      "model accuracy: 0.752\n",
      "Full accuracy: 0.752, LC accuracy: 0.752, Decomposed-Full accuracy: 0.74, Decomposed-Restored accuracy: 0.74\n",
      "Epoch: 6, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_27\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_27\n",
      "LoRA+LC Training Loss (Decomposed): 1.7111356258392334\n",
      "LC Training Loss (Full): 1.7110443115234375\n",
      "Epoch: 7, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6546783447265625\n",
      "LC Training Loss (Full): 1.6478782892227173\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "Epoch: 7, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.7403020858764648\n",
      "LC Training Loss (Full): 1.74062180519104\n",
      "Epoch: 7, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.6941633224487305\n",
      "LC Training Loss (Full): 1.6658415794372559\n",
      "Epoch: 7, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.7914484739303589\n",
      "LC Training Loss (Full): 1.773474931716919\n",
      "Epoch: 7, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.8347827196121216\n",
      "LC Training Loss (Full): 1.8354982137680054\n",
      "Epoch: 7, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.7800118923187256\n",
      "LC Training Loss (Full): 1.7774748802185059\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.751\n",
      "Full accuracy: 0.751, LC accuracy: 0.751, Decomposed-Full accuracy: 0.741, Decomposed-Restored accuracy: 0.741\n",
      "Epoch: 7, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.8703422546386719\n",
      "LC Training Loss (Full): 1.8460336923599243\n",
      "Epoch: 7, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.6179465055465698\n",
      "LC Training Loss (Full): 1.6171302795410156\n",
      "Epoch: 7, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.616887092590332\n",
      "LC Training Loss (Full): 1.6170001029968262\n",
      "Epoch: 7, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_28\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_28\n",
      "LoRA+LC Training Loss (Decomposed): 1.6591321229934692\n",
      "LC Training Loss (Full): 1.648821473121643\n",
      "Epoch: 7, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6586487293243408\n",
      "LC Training Loss (Full): 1.6620995998382568\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.753\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.753\n",
      "Full accuracy: 0.753, LC accuracy: 0.753, Decomposed-Full accuracy: 0.741, Decomposed-Restored accuracy: 0.741\n",
      "Epoch: 7, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.6800775527954102\n",
      "LC Training Loss (Full): 1.676363468170166\n",
      "Epoch: 7, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.809436559677124\n",
      "LC Training Loss (Full): 1.811942219734192\n",
      "Epoch: 7, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.647268533706665\n",
      "LC Training Loss (Full): 1.6472351551055908\n",
      "Epoch: 7, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.7118043899536133\n",
      "LC Training Loss (Full): 1.7057530879974365\n",
      "Epoch: 7, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.7490464448928833\n",
      "LC Training Loss (Full): 1.716352939605713\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.75\n",
      "model accuracy: 0.753\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.741\n",
      "model accuracy: 0.754\n",
      "Full accuracy: 0.753, LC accuracy: 0.754, Decomposed-Full accuracy: 0.741, Decomposed-Restored accuracy: 0.741\n",
      "Epoch: 7, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.792327642440796\n",
      "LC Training Loss (Full): 1.7616045475006104\n",
      "Epoch: 7, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.7726640701293945\n",
      "LC Training Loss (Full): 1.7715579271316528\n",
      "Epoch: 7, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.6700291633605957\n",
      "LC Training Loss (Full): 1.64519202709198\n",
      "Epoch: 7, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_29\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_29\n",
      "LoRA+LC Training Loss (Decomposed): 1.805315613746643\n",
      "LC Training Loss (Full): 1.7915700674057007\n",
      "Epoch: 7, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8036264181137085\n",
      "LC Training Loss (Full): 1.77402663230896\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.6875\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.754\n",
      "Full accuracy: 0.754, LC accuracy: 0.754, Decomposed-Full accuracy: 0.743, Decomposed-Restored accuracy: 0.743\n",
      "Epoch: 7, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.6173326969146729\n",
      "LC Training Loss (Full): 1.6158804893493652\n",
      "Epoch: 7, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.6492269039154053\n",
      "LC Training Loss (Full): 1.6493948698043823\n",
      "Epoch: 7, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.7111574411392212\n",
      "LC Training Loss (Full): 1.6804156303405762\n",
      "Epoch: 7, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.6784911155700684\n",
      "LC Training Loss (Full): 1.6781814098358154\n",
      "Epoch: 7, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.8028148412704468\n",
      "LC Training Loss (Full): 1.8029793500900269\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "model accuracy: 0.753\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.753\n",
      "Full accuracy: 0.753, LC accuracy: 0.753, Decomposed-Full accuracy: 0.743, Decomposed-Restored accuracy: 0.743\n",
      "Epoch: 7, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.6787397861480713\n",
      "LC Training Loss (Full): 1.6516857147216797\n",
      "Epoch: 7, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.7071495056152344\n",
      "LC Training Loss (Full): 1.6823196411132812\n",
      "Epoch: 7, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.6860350370407104\n",
      "LC Training Loss (Full): 1.6857777833938599\n",
      "Epoch: 7, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_30\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_30\n",
      "LoRA+LC Training Loss (Decomposed): 1.8113970756530762\n",
      "LC Training Loss (Full): 1.7852822542190552\n",
      "Epoch: 7, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_31\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_31/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.742868185043335\n",
      "LC Training Loss (Full): 1.744584321975708\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.743\n",
      "model accuracy: 0.755\n",
      "Full accuracy: 0.755, LC accuracy: 0.755, Decomposed-Full accuracy: 0.743, Decomposed-Restored accuracy: 0.743\n",
      "Epoch: 7, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_31\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_31\n",
      "LoRA+LC Training Loss (Decomposed): 1.4622834920883179\n",
      "LC Training Loss (Full): 1.4612081050872803\n",
      "Epoch: 8, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.5815824270248413\n",
      "LC Training Loss (Full): 1.5564188957214355\n",
      "Training Accuracy | Decomposed: 0.875, Full : 0.90625\n",
      "Epoch: 8, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.7418794631958008\n",
      "LC Training Loss (Full): 1.7247936725616455\n",
      "Epoch: 8, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.6646407842636108\n",
      "LC Training Loss (Full): 1.6294209957122803\n",
      "Epoch: 8, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.803477168083191\n",
      "LC Training Loss (Full): 1.8030365705490112\n",
      "Epoch: 8, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.7719861268997192\n",
      "LC Training Loss (Full): 1.759437084197998\n",
      "Epoch: 8, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.8350847959518433\n",
      "LC Training Loss (Full): 1.8336427211761475\n",
      "Training Accuracy | Decomposed: 0.625, Full : 0.625\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.754\n",
      "Full accuracy: 0.754, LC accuracy: 0.754, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 8, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.8351315259933472\n",
      "LC Training Loss (Full): 1.8358266353607178\n",
      "Epoch: 8, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.7417879104614258\n",
      "LC Training Loss (Full): 1.7410451173782349\n",
      "Epoch: 8, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.6168594360351562\n",
      "LC Training Loss (Full): 1.6172388792037964\n",
      "Epoch: 8, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_32\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_32\n",
      "LoRA+LC Training Loss (Decomposed): 1.648533582687378\n",
      "LC Training Loss (Full): 1.6489299535751343\n",
      "Epoch: 8, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.771414041519165\n",
      "LC Training Loss (Full): 1.7471681833267212\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.71875\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.754\n",
      "Full accuracy: 0.754, LC accuracy: 0.754, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 8, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.803991436958313\n",
      "LC Training Loss (Full): 1.8043129444122314\n",
      "Epoch: 8, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.6059529781341553\n",
      "LC Training Loss (Full): 1.5884459018707275\n",
      "Epoch: 8, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.780827283859253\n",
      "LC Training Loss (Full): 1.7458430528640747\n",
      "Epoch: 8, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.6500728130340576\n",
      "LC Training Loss (Full): 1.6557313203811646\n",
      "Epoch: 8, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.72463858127594\n",
      "LC Training Loss (Full): 1.7019120454788208\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.756\n",
      "Full accuracy: 0.755, LC accuracy: 0.756, Decomposed-Full accuracy: 0.745, Decomposed-Restored accuracy: 0.745\n",
      "Epoch: 8, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.7821414470672607\n",
      "LC Training Loss (Full): 1.7742526531219482\n",
      "Epoch: 8, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.7417182922363281\n",
      "LC Training Loss (Full): 1.7420902252197266\n",
      "Epoch: 8, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.771648645401001\n",
      "LC Training Loss (Full): 1.773237705230713\n",
      "Epoch: 8, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_33\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_33\n",
      "LoRA+LC Training Loss (Decomposed): 1.6631808280944824\n",
      "LC Training Loss (Full): 1.6659685373306274\n",
      "Epoch: 8, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6302573680877686\n",
      "LC Training Loss (Full): 1.620077133178711\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.755\n",
      "Full accuracy: 0.755, LC accuracy: 0.755, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.745\n",
      "Epoch: 8, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.743248701095581\n",
      "LC Training Loss (Full): 1.7094535827636719\n",
      "Epoch: 8, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.7130513191223145\n",
      "LC Training Loss (Full): 1.6858630180358887\n",
      "Epoch: 8, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.616329312324524\n",
      "LC Training Loss (Full): 1.616803765296936\n",
      "Epoch: 8, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.6498335599899292\n",
      "LC Training Loss (Full): 1.6500426530838013\n",
      "Epoch: 8, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.6371486186981201\n",
      "LC Training Loss (Full): 1.6221940517425537\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.756\n",
      "Full accuracy: 0.756, LC accuracy: 0.756, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 8, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.8075319528579712\n",
      "LC Training Loss (Full): 1.8047863245010376\n",
      "Epoch: 8, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.7109997272491455\n",
      "LC Training Loss (Full): 1.7101216316223145\n",
      "Epoch: 8, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.7453789710998535\n",
      "LC Training Loss (Full): 1.7366942167282104\n",
      "Epoch: 8, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_34\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_34\n",
      "LoRA+LC Training Loss (Decomposed): 1.7157747745513916\n",
      "LC Training Loss (Full): 1.71956205368042\n",
      "Epoch: 8, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_35\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_35/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6964222192764282\n",
      "LC Training Loss (Full): 1.6287198066711426\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.84375\n",
      "model accuracy: 0.757\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.757, LC accuracy: 0.758, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 8, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_35\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_35\n",
      "LoRA+LC Training Loss (Decomposed): 2.210930347442627\n",
      "LC Training Loss (Full): 2.211146116256714\n",
      "Epoch: 9, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.9278831481933594\n",
      "LC Training Loss (Full): 1.9285792112350464\n",
      "Training Accuracy | Decomposed: 0.53125, Full : 0.53125\n",
      "Epoch: 9, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.7418195009231567\n",
      "LC Training Loss (Full): 1.7100954055786133\n",
      "Epoch: 9, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.7188341617584229\n",
      "LC Training Loss (Full): 1.7159876823425293\n",
      "Epoch: 9, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.7838425636291504\n",
      "LC Training Loss (Full): 1.7693718671798706\n",
      "Epoch: 9, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.6812139749526978\n",
      "LC Training Loss (Full): 1.6798837184906006\n",
      "Epoch: 9, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.6815282106399536\n",
      "LC Training Loss (Full): 1.6818313598632812\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.757\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.757\n",
      "Full accuracy: 0.757, LC accuracy: 0.757, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 9, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.6219921112060547\n",
      "LC Training Loss (Full): 1.6197736263275146\n",
      "Epoch: 9, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.7430551052093506\n",
      "LC Training Loss (Full): 1.7410095930099487\n",
      "Epoch: 9, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.7425037622451782\n",
      "LC Training Loss (Full): 1.7426602840423584\n",
      "Epoch: 9, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_36\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_36\n",
      "LoRA+LC Training Loss (Decomposed): 1.7525627613067627\n",
      "LC Training Loss (Full): 1.763627529144287\n",
      "Epoch: 9, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6843115091323853\n",
      "LC Training Loss (Full): 1.6808006763458252\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 9, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.6488728523254395\n",
      "LC Training Loss (Full): 1.648763656616211\n",
      "Epoch: 9, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.6080783605575562\n",
      "LC Training Loss (Full): 1.5734479427337646\n",
      "Epoch: 9, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.8125349283218384\n",
      "LC Training Loss (Full): 1.776822566986084\n",
      "Epoch: 9, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.7423279285430908\n",
      "LC Training Loss (Full): 1.7420974969863892\n",
      "Epoch: 9, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.7063708305358887\n",
      "LC Training Loss (Full): 1.6797499656677246\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.744, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 9, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.752821922302246\n",
      "LC Training Loss (Full): 1.743195652961731\n",
      "Epoch: 9, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.769961953163147\n",
      "LC Training Loss (Full): 1.7424538135528564\n",
      "Epoch: 9, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.6486396789550781\n",
      "LC Training Loss (Full): 1.6484705209732056\n",
      "Epoch: 9, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_37\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_37\n",
      "LoRA+LC Training Loss (Decomposed): 1.7539379596710205\n",
      "LC Training Loss (Full): 1.74496591091156\n",
      "Epoch: 9, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7929894924163818\n",
      "LC Training Loss (Full): 1.7498928308486938\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.71875\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.745, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 9, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.8645025491714478\n",
      "LC Training Loss (Full): 1.8094429969787598\n",
      "Epoch: 9, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.6778234243392944\n",
      "LC Training Loss (Full): 1.6540426015853882\n",
      "Epoch: 9, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.7103630304336548\n",
      "LC Training Loss (Full): 1.7114312648773193\n",
      "Epoch: 9, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.770720362663269\n",
      "LC Training Loss (Full): 1.771728515625\n",
      "Epoch: 9, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.6832594871520996\n",
      "LC Training Loss (Full): 1.68117094039917\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.745, Decomposed-Restored accuracy: 0.745\n",
      "Epoch: 9, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.5894265174865723\n",
      "LC Training Loss (Full): 1.5860072374343872\n",
      "Epoch: 9, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.648612141609192\n",
      "LC Training Loss (Full): 1.6489312648773193\n",
      "Epoch: 9, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.6172983646392822\n",
      "LC Training Loss (Full): 1.6177114248275757\n",
      "Epoch: 9, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_38\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_38\n",
      "LoRA+LC Training Loss (Decomposed): 1.6469004154205322\n",
      "LC Training Loss (Full): 1.6319423913955688\n",
      "Epoch: 9, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_39\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_39/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7161275148391724\n",
      "LC Training Loss (Full): 1.6794832944869995\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.745, Decomposed-Restored accuracy: 0.745\n",
      "Epoch: 9, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_39\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_39\n",
      "LoRA+LC Training Loss (Decomposed): 1.8361477851867676\n",
      "LC Training Loss (Full): 1.8361402750015259\n",
      "Epoch: 10, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7185016870498657\n",
      "LC Training Loss (Full): 1.7151391506195068\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "Epoch: 10, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.6490836143493652\n",
      "LC Training Loss (Full): 1.6487029790878296\n",
      "Epoch: 10, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.7747464179992676\n",
      "LC Training Loss (Full): 1.7742836475372314\n",
      "Epoch: 10, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.8140369653701782\n",
      "LC Training Loss (Full): 1.8062574863433838\n",
      "Epoch: 10, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.6525732278823853\n",
      "LC Training Loss (Full): 1.6504862308502197\n",
      "Epoch: 10, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.6778494119644165\n",
      "LC Training Loss (Full): 1.6772290468215942\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.746\n",
      "model accuracy: 0.746\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.746, Decomposed-Restored accuracy: 0.746\n",
      "Epoch: 10, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.6489591598510742\n",
      "LC Training Loss (Full): 1.6491652727127075\n",
      "Epoch: 10, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.8352365493774414\n",
      "LC Training Loss (Full): 1.834921956062317\n",
      "Epoch: 10, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.713036060333252\n",
      "LC Training Loss (Full): 1.7105786800384521\n",
      "Epoch: 10, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_40\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_40\n",
      "LoRA+LC Training Loss (Decomposed): 1.6537649631500244\n",
      "LC Training Loss (Full): 1.6521785259246826\n",
      "Epoch: 10, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6481399536132812\n",
      "LC Training Loss (Full): 1.6216235160827637\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.84375\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.744\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.745, Decomposed-Restored accuracy: 0.744\n",
      "Epoch: 10, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.7364795207977295\n",
      "LC Training Loss (Full): 1.650099515914917\n",
      "Epoch: 10, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.586350917816162\n",
      "LC Training Loss (Full): 1.5861177444458008\n",
      "Epoch: 10, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.7193615436553955\n",
      "LC Training Loss (Full): 1.7108010053634644\n",
      "Epoch: 10, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.6600170135498047\n",
      "LC Training Loss (Full): 1.651439905166626\n",
      "Epoch: 10, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.6754509210586548\n",
      "LC Training Loss (Full): 1.648951768875122\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.8125\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.745\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.745, Decomposed-Restored accuracy: 0.745\n",
      "Epoch: 10, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.660361886024475\n",
      "LC Training Loss (Full): 1.6491059064865112\n",
      "Epoch: 10, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.6186970472335815\n",
      "LC Training Loss (Full): 1.6176893711090088\n",
      "Epoch: 10, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.756222128868103\n",
      "LC Training Loss (Full): 1.7492707967758179\n",
      "Epoch: 10, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_41\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_41\n",
      "LoRA+LC Training Loss (Decomposed): 1.8393100500106812\n",
      "LC Training Loss (Full): 1.8080594539642334\n",
      "Epoch: 10, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7428785562515259\n",
      "LC Training Loss (Full): 1.7418545484542847\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.758\n",
      "model accuracy: 0.746\n",
      "model accuracy: 0.746\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.758, LC accuracy: 0.758, Decomposed-Full accuracy: 0.746, Decomposed-Restored accuracy: 0.746\n",
      "Epoch: 10, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.662654161453247\n",
      "LC Training Loss (Full): 1.6630487442016602\n",
      "Epoch: 10, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.8826581239700317\n",
      "LC Training Loss (Full): 1.8396925926208496\n",
      "Epoch: 10, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.8655221462249756\n",
      "LC Training Loss (Full): 1.8615374565124512\n",
      "Epoch: 10, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.8364800214767456\n",
      "LC Training Loss (Full): 1.8360934257507324\n",
      "Epoch: 10, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.8022910356521606\n",
      "LC Training Loss (Full): 1.7768573760986328\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.6875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.747\n",
      "model accuracy: 0.746\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.747, Decomposed-Restored accuracy: 0.746\n",
      "Epoch: 10, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.6494240760803223\n",
      "LC Training Loss (Full): 1.63838529586792\n",
      "Epoch: 10, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.6775178909301758\n",
      "LC Training Loss (Full): 1.6521875858306885\n",
      "Epoch: 10, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.6378206014633179\n",
      "LC Training Loss (Full): 1.619593858718872\n",
      "Epoch: 10, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_42\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_42\n",
      "LoRA+LC Training Loss (Decomposed): 1.771422266960144\n",
      "LC Training Loss (Full): 1.7704919576644897\n",
      "Epoch: 10, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_43\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_43/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.698244333267212\n",
      "LC Training Loss (Full): 1.676751732826233\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.747\n",
      "model accuracy: 0.747\n",
      "model accuracy: 0.758\n",
      "Full accuracy: 0.759, LC accuracy: 0.758, Decomposed-Full accuracy: 0.747, Decomposed-Restored accuracy: 0.747\n",
      "Epoch: 10, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_43\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_43\n",
      "LoRA+LC Training Loss (Decomposed): 1.5861512422561646\n",
      "LC Training Loss (Full): 1.5861507654190063\n",
      "Epoch: 11, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.687559962272644\n",
      "LC Training Loss (Full): 1.6506088972091675\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.8125\n",
      "Epoch: 11, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.6556776762008667\n",
      "LC Training Loss (Full): 1.653297781944275\n",
      "Epoch: 11, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.7225183248519897\n",
      "LC Training Loss (Full): 1.6994402408599854\n",
      "Epoch: 11, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.6503797769546509\n",
      "LC Training Loss (Full): 1.6520339250564575\n",
      "Epoch: 11, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.7470699548721313\n",
      "LC Training Loss (Full): 1.744576334953308\n",
      "Epoch: 11, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.7127102613449097\n",
      "LC Training Loss (Full): 1.7084619998931885\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.748, Decomposed-Restored accuracy: 0.748\n",
      "Epoch: 11, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.774032473564148\n",
      "LC Training Loss (Full): 1.7730779647827148\n",
      "Epoch: 11, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.6906776428222656\n",
      "LC Training Loss (Full): 1.6574000120162964\n",
      "Epoch: 11, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.7136034965515137\n",
      "LC Training Loss (Full): 1.7106289863586426\n",
      "Epoch: 11, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_44\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_44\n",
      "LoRA+LC Training Loss (Decomposed): 1.676949143409729\n",
      "LC Training Loss (Full): 1.6555991172790527\n",
      "Epoch: 11, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7456090450286865\n",
      "LC Training Loss (Full): 1.7435206174850464\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.748, Decomposed-Restored accuracy: 0.748\n",
      "Epoch: 11, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.6118215322494507\n",
      "LC Training Loss (Full): 1.5993938446044922\n",
      "Epoch: 11, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.7405208349227905\n",
      "LC Training Loss (Full): 1.7428276538848877\n",
      "Epoch: 11, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.7567684650421143\n",
      "LC Training Loss (Full): 1.7473126649856567\n",
      "Epoch: 11, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.6475132703781128\n",
      "LC Training Loss (Full): 1.6204625368118286\n",
      "Epoch: 11, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.682678461074829\n",
      "LC Training Loss (Full): 1.6809141635894775\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.748\n",
      "model accuracy: 0.747\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.748, Decomposed-Restored accuracy: 0.747\n",
      "Epoch: 11, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.633117437362671\n",
      "LC Training Loss (Full): 1.588947057723999\n",
      "Epoch: 11, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.8446457386016846\n",
      "LC Training Loss (Full): 1.805311918258667\n",
      "Epoch: 11, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.6812241077423096\n",
      "LC Training Loss (Full): 1.6804616451263428\n",
      "Epoch: 11, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_45\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_45\n",
      "LoRA+LC Training Loss (Decomposed): 1.6513047218322754\n",
      "LC Training Loss (Full): 1.6510260105133057\n",
      "Epoch: 11, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6887807846069336\n",
      "LC Training Loss (Full): 1.6808744668960571\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 11, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.7115087509155273\n",
      "LC Training Loss (Full): 1.7103664875030518\n",
      "Epoch: 11, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.7185850143432617\n",
      "LC Training Loss (Full): 1.7130084037780762\n",
      "Epoch: 11, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.804740309715271\n",
      "LC Training Loss (Full): 1.803710699081421\n",
      "Epoch: 11, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.6921452283859253\n",
      "LC Training Loss (Full): 1.6804369688034058\n",
      "Epoch: 11, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.6908913850784302\n",
      "LC Training Loss (Full): 1.6832785606384277\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 11, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.8103852272033691\n",
      "LC Training Loss (Full): 1.8063663244247437\n",
      "Epoch: 11, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.6875277757644653\n",
      "LC Training Loss (Full): 1.6789271831512451\n",
      "Epoch: 11, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.7418714761734009\n",
      "LC Training Loss (Full): 1.7409800291061401\n",
      "Epoch: 11, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_46\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_46\n",
      "LoRA+LC Training Loss (Decomposed): 1.7404543161392212\n",
      "LC Training Loss (Full): 1.7409719228744507\n",
      "Epoch: 11, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_47\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_47/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8021193742752075\n",
      "LC Training Loss (Full): 1.7670542001724243\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.6875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 11, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_47\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_47\n",
      "LoRA+LC Training Loss (Decomposed): 1.9574604034423828\n",
      "LC Training Loss (Full): 1.9523282051086426\n",
      "Epoch: 12, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6796320676803589\n",
      "LC Training Loss (Full): 1.6806937456130981\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "Epoch: 12, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.8082271814346313\n",
      "LC Training Loss (Full): 1.7760313749313354\n",
      "Epoch: 12, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.697764277458191\n",
      "LC Training Loss (Full): 1.6785054206848145\n",
      "Epoch: 12, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.6479732990264893\n",
      "LC Training Loss (Full): 1.6487951278686523\n",
      "Epoch: 12, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.8025953769683838\n",
      "LC Training Loss (Full): 1.802709698677063\n",
      "Epoch: 12, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.6887154579162598\n",
      "LC Training Loss (Full): 1.6841871738433838\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 12, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.6424078941345215\n",
      "LC Training Loss (Full): 1.6281927824020386\n",
      "Epoch: 12, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.7410192489624023\n",
      "LC Training Loss (Full): 1.7418960332870483\n",
      "Epoch: 12, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.801103115081787\n",
      "LC Training Loss (Full): 1.771844744682312\n",
      "Epoch: 12, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_48\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_48\n",
      "LoRA+LC Training Loss (Decomposed): 1.7183408737182617\n",
      "LC Training Loss (Full): 1.7151885032653809\n",
      "Epoch: 12, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7724719047546387\n",
      "LC Training Loss (Full): 1.6841622591018677\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 12, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.5259959697723389\n",
      "LC Training Loss (Full): 1.5238901376724243\n",
      "Epoch: 12, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.7106552124023438\n",
      "LC Training Loss (Full): 1.7112133502960205\n",
      "Epoch: 12, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.894033670425415\n",
      "LC Training Loss (Full): 1.8676834106445312\n",
      "Epoch: 12, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.6498117446899414\n",
      "LC Training Loss (Full): 1.6491442918777466\n",
      "Epoch: 12, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.6829719543457031\n",
      "LC Training Loss (Full): 1.6507753133773804\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.8125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 12, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.7263833284378052\n",
      "LC Training Loss (Full): 1.715352177619934\n",
      "Epoch: 12, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.7093323469161987\n",
      "LC Training Loss (Full): 1.6875501871109009\n",
      "Epoch: 12, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.7142688035964966\n",
      "LC Training Loss (Full): 1.7129042148590088\n",
      "Epoch: 12, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_49\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_49\n",
      "LoRA+LC Training Loss (Decomposed): 1.6802456378936768\n",
      "LC Training Loss (Full): 1.6788077354431152\n",
      "Epoch: 12, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.683921217918396\n",
      "LC Training Loss (Full): 1.684086561203003\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.749\n",
      "model accuracy: 0.749\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.749, Decomposed-Restored accuracy: 0.749\n",
      "Epoch: 12, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.8371107578277588\n",
      "LC Training Loss (Full): 1.8357208967208862\n",
      "Epoch: 12, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.8038350343704224\n",
      "LC Training Loss (Full): 1.803070306777954\n",
      "Epoch: 12, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.6790754795074463\n",
      "LC Training Loss (Full): 1.6795696020126343\n",
      "Epoch: 12, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.7111228704452515\n",
      "LC Training Loss (Full): 1.7117760181427002\n",
      "Epoch: 12, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.6977835893630981\n",
      "LC Training Loss (Full): 1.69081711769104\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.749\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.749\n",
      "Epoch: 12, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.6803427934646606\n",
      "LC Training Loss (Full): 1.680039405822754\n",
      "Epoch: 12, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.8357080221176147\n",
      "LC Training Loss (Full): 1.8348290920257568\n",
      "Epoch: 12, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.5343621969223022\n",
      "LC Training Loss (Full): 1.5288293361663818\n",
      "Epoch: 12, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_50\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_50\n",
      "LoRA+LC Training Loss (Decomposed): 1.6180886030197144\n",
      "LC Training Loss (Full): 1.617597222328186\n",
      "Epoch: 12, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_51\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_51/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7733577489852905\n",
      "LC Training Loss (Full): 1.7735180854797363\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 12, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_51\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_51\n",
      "LoRA+LC Training Loss (Decomposed): 1.586243987083435\n",
      "LC Training Loss (Full): 1.586154580116272\n",
      "Epoch: 13, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8360764980316162\n",
      "LC Training Loss (Full): 1.803889274597168\n",
      "Training Accuracy | Decomposed: 0.625, Full : 0.65625\n",
      "Epoch: 13, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.7055332660675049\n",
      "LC Training Loss (Full): 1.6892987489700317\n",
      "Epoch: 13, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.8049492835998535\n",
      "LC Training Loss (Full): 1.7995727062225342\n",
      "Epoch: 13, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.7758562564849854\n",
      "LC Training Loss (Full): 1.7747015953063965\n",
      "Epoch: 13, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.7122691869735718\n",
      "LC Training Loss (Full): 1.7137082815170288\n",
      "Epoch: 13, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.6789250373840332\n",
      "LC Training Loss (Full): 1.6799310445785522\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 13, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.8359053134918213\n",
      "LC Training Loss (Full): 1.805608868598938\n",
      "Epoch: 13, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.7190487384796143\n",
      "LC Training Loss (Full): 1.7133829593658447\n",
      "Epoch: 13, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.6216802597045898\n",
      "LC Training Loss (Full): 1.6183900833129883\n",
      "Epoch: 13, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_52\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_52\n",
      "LoRA+LC Training Loss (Decomposed): 1.710618495941162\n",
      "LC Training Loss (Full): 1.711140513420105\n",
      "Epoch: 13, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6385700702667236\n",
      "LC Training Loss (Full): 1.6208959817886353\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 13, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.6913591623306274\n",
      "LC Training Loss (Full): 1.685589075088501\n",
      "Epoch: 13, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.6816495656967163\n",
      "LC Training Loss (Full): 1.6817163228988647\n",
      "Epoch: 13, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.7702633142471313\n",
      "LC Training Loss (Full): 1.7699947357177734\n",
      "Epoch: 13, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.8381726741790771\n",
      "LC Training Loss (Full): 1.834628701210022\n",
      "Epoch: 13, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.628866195678711\n",
      "LC Training Loss (Full): 1.6259957551956177\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 13, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.652957797050476\n",
      "LC Training Loss (Full): 1.650891900062561\n",
      "Epoch: 13, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.6178393363952637\n",
      "LC Training Loss (Full): 1.5903143882751465\n",
      "Epoch: 13, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.6166054010391235\n",
      "LC Training Loss (Full): 1.6162503957748413\n",
      "Epoch: 13, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_53\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_53\n",
      "LoRA+LC Training Loss (Decomposed): 1.7431976795196533\n",
      "LC Training Loss (Full): 1.7144328355789185\n",
      "Epoch: 13, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.59587824344635\n",
      "LC Training Loss (Full): 1.5881692171096802\n",
      "Training Accuracy | Decomposed: 0.875, Full : 0.875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 13, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.6810163259506226\n",
      "LC Training Loss (Full): 1.6798622608184814\n",
      "Epoch: 13, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.7752865552902222\n",
      "LC Training Loss (Full): 1.773799180984497\n",
      "Epoch: 13, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.7101826667785645\n",
      "LC Training Loss (Full): 1.7100539207458496\n",
      "Epoch: 13, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.6797993183135986\n",
      "LC Training Loss (Full): 1.679986596107483\n",
      "Epoch: 13, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.645964503288269\n",
      "LC Training Loss (Full): 1.6220574378967285\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.75\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.75, Decomposed-Restored accuracy: 0.75\n",
      "Epoch: 13, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.7384328842163086\n",
      "LC Training Loss (Full): 1.7383205890655518\n",
      "Epoch: 13, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.7223598957061768\n",
      "LC Training Loss (Full): 1.718091368675232\n",
      "Epoch: 13, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.6996160745620728\n",
      "LC Training Loss (Full): 1.6793190240859985\n",
      "Epoch: 13, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_54\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_54\n",
      "LoRA+LC Training Loss (Decomposed): 1.8399039506912231\n",
      "LC Training Loss (Full): 1.8367964029312134\n",
      "Epoch: 13, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_55\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_55/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6808370351791382\n",
      "LC Training Loss (Full): 1.651647686958313\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.8125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.751\n",
      "Epoch: 13, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_55\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_55\n",
      "LoRA+LC Training Loss (Decomposed): 1.83555269241333\n",
      "LC Training Loss (Full): 1.8338210582733154\n",
      "Epoch: 14, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.771836757659912\n",
      "LC Training Loss (Full): 1.7728303670883179\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "Epoch: 14, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.780187964439392\n",
      "LC Training Loss (Full): 1.7768634557724\n",
      "Epoch: 14, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.7115644216537476\n",
      "LC Training Loss (Full): 1.712536334991455\n",
      "Epoch: 14, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.8025710582733154\n",
      "LC Training Loss (Full): 1.7730499505996704\n",
      "Epoch: 14, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.6116966009140015\n",
      "LC Training Loss (Full): 1.587368369102478\n",
      "Epoch: 14, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.8645331859588623\n",
      "LC Training Loss (Full): 1.8573659658432007\n",
      "Training Accuracy | Decomposed: 0.59375, Full : 0.59375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.751\n",
      "Epoch: 14, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.7390261888504028\n",
      "LC Training Loss (Full): 1.710360050201416\n",
      "Epoch: 14, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.8061715364456177\n",
      "LC Training Loss (Full): 1.775905728340149\n",
      "Epoch: 14, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.804897427558899\n",
      "LC Training Loss (Full): 1.8050047159194946\n",
      "Epoch: 14, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_56\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_56\n",
      "LoRA+LC Training Loss (Decomposed): 1.7434903383255005\n",
      "LC Training Loss (Full): 1.7440727949142456\n",
      "Epoch: 14, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6478444337844849\n",
      "LC Training Loss (Full): 1.6478705406188965\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.751\n",
      "Epoch: 14, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.61735200881958\n",
      "LC Training Loss (Full): 1.6169965267181396\n",
      "Epoch: 14, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.774930715560913\n",
      "LC Training Loss (Full): 1.7736561298370361\n",
      "Epoch: 14, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.7358158826828003\n",
      "LC Training Loss (Full): 1.7146226167678833\n",
      "Epoch: 14, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.6825668811798096\n",
      "LC Training Loss (Full): 1.6819528341293335\n",
      "Epoch: 14, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.5876145362854004\n",
      "LC Training Loss (Full): 1.5871514081954956\n",
      "Training Accuracy | Decomposed: 0.875, Full : 0.875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 14, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.6824426651000977\n",
      "LC Training Loss (Full): 1.680924415588379\n",
      "Epoch: 14, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.6861988306045532\n",
      "LC Training Loss (Full): 1.6809459924697876\n",
      "Epoch: 14, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.77217435836792\n",
      "LC Training Loss (Full): 1.7488396167755127\n",
      "Epoch: 14, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_57\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_57\n",
      "LoRA+LC Training Loss (Decomposed): 1.6813857555389404\n",
      "LC Training Loss (Full): 1.6802064180374146\n",
      "Epoch: 14, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7448303699493408\n",
      "LC Training Loss (Full): 1.7425315380096436\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 14, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.6490082740783691\n",
      "LC Training Loss (Full): 1.6474767923355103\n",
      "Epoch: 14, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.7154450416564941\n",
      "LC Training Loss (Full): 1.7123762369155884\n",
      "Epoch: 14, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.7234493494033813\n",
      "LC Training Loss (Full): 1.7188866138458252\n",
      "Epoch: 14, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.7754911184310913\n",
      "LC Training Loss (Full): 1.7722989320755005\n",
      "Epoch: 14, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.7233076095581055\n",
      "LC Training Loss (Full): 1.6837965250015259\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 14, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.678845763206482\n",
      "LC Training Loss (Full): 1.6780747175216675\n",
      "Epoch: 14, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.6803375482559204\n",
      "LC Training Loss (Full): 1.6802077293395996\n",
      "Epoch: 14, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.5953397750854492\n",
      "LC Training Loss (Full): 1.5919967889785767\n",
      "Epoch: 14, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_58\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_58\n",
      "LoRA+LC Training Loss (Decomposed): 1.6737542152404785\n",
      "LC Training Loss (Full): 1.6564170122146606\n",
      "Epoch: 14, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_59\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_59/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.617047905921936\n",
      "LC Training Loss (Full): 1.617428183555603\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.751\n",
      "Epoch: 14, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_59\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_59\n",
      "LoRA+LC Training Loss (Decomposed): 1.5864075422286987\n",
      "LC Training Loss (Full): 1.5867842435836792\n",
      "Epoch: 15, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.711330771446228\n",
      "LC Training Loss (Full): 1.7108027935028076\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "Epoch: 15, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.7440751791000366\n",
      "LC Training Loss (Full): 1.7437838315963745\n",
      "Epoch: 15, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.739678144454956\n",
      "LC Training Loss (Full): 1.7347607612609863\n",
      "Epoch: 15, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.6038483381271362\n",
      "LC Training Loss (Full): 1.5566643476486206\n",
      "Epoch: 15, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.5583692789077759\n",
      "LC Training Loss (Full): 1.5579255819320679\n",
      "Epoch: 15, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.742576003074646\n",
      "LC Training Loss (Full): 1.742371916770935\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.751\n",
      "Epoch: 15, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.7745851278305054\n",
      "LC Training Loss (Full): 1.7730873823165894\n",
      "Epoch: 15, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.7161760330200195\n",
      "LC Training Loss (Full): 1.7125072479248047\n",
      "Epoch: 15, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.6791343688964844\n",
      "LC Training Loss (Full): 1.6787374019622803\n",
      "Epoch: 15, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_60\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_60\n",
      "LoRA+LC Training Loss (Decomposed): 1.8680380582809448\n",
      "LC Training Loss (Full): 1.8663272857666016\n",
      "Epoch: 15, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7147414684295654\n",
      "LC Training Loss (Full): 1.711033582687378\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.751\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.751, Decomposed-Restored accuracy: 0.751\n",
      "Epoch: 15, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.6473828554153442\n",
      "LC Training Loss (Full): 1.6485916376113892\n",
      "Epoch: 15, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.5891822576522827\n",
      "LC Training Loss (Full): 1.5871425867080688\n",
      "Epoch: 15, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.8029118776321411\n",
      "LC Training Loss (Full): 1.7782188653945923\n",
      "Epoch: 15, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.7733514308929443\n",
      "LC Training Loss (Full): 1.7727352380752563\n",
      "Epoch: 15, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.6174442768096924\n",
      "LC Training Loss (Full): 1.6173330545425415\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 15, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.6549222469329834\n",
      "LC Training Loss (Full): 1.6524889469146729\n",
      "Epoch: 15, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.754307508468628\n",
      "LC Training Loss (Full): 1.7505037784576416\n",
      "Epoch: 15, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.8082175254821777\n",
      "LC Training Loss (Full): 1.7458903789520264\n",
      "Epoch: 15, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_61\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_61\n",
      "LoRA+LC Training Loss (Decomposed): 1.741425633430481\n",
      "LC Training Loss (Full): 1.7402015924453735\n",
      "Epoch: 15, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7754013538360596\n",
      "LC Training Loss (Full): 1.7731661796569824\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 15, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.6176443099975586\n",
      "LC Training Loss (Full): 1.616145372390747\n",
      "Epoch: 15, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.773381233215332\n",
      "LC Training Loss (Full): 1.7730845212936401\n",
      "Epoch: 15, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.7123545408248901\n",
      "LC Training Loss (Full): 1.6831730604171753\n",
      "Epoch: 15, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.7655620574951172\n",
      "LC Training Loss (Full): 1.71993088722229\n",
      "Epoch: 15, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.804693579673767\n",
      "LC Training Loss (Full): 1.802974820137024\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 15, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.6486583948135376\n",
      "LC Training Loss (Full): 1.6502654552459717\n",
      "Epoch: 15, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.7436379194259644\n",
      "LC Training Loss (Full): 1.7439008951187134\n",
      "Epoch: 15, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.5896337032318115\n",
      "LC Training Loss (Full): 1.5894405841827393\n",
      "Epoch: 15, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_62\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_62\n",
      "LoRA+LC Training Loss (Decomposed): 1.6475839614868164\n",
      "LC Training Loss (Full): 1.6471134424209595\n",
      "Epoch: 15, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_63\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_63/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6803442239761353\n",
      "LC Training Loss (Full): 1.679465651512146\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 15, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_63\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_63\n",
      "LoRA+LC Training Loss (Decomposed): 1.8421108722686768\n",
      "LC Training Loss (Full): 1.83961820602417\n",
      "Epoch: 16, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7110244035720825\n",
      "LC Training Loss (Full): 1.711178183555603\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "Epoch: 16, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.7530970573425293\n",
      "LC Training Loss (Full): 1.7465678453445435\n",
      "Epoch: 16, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.6183109283447266\n",
      "LC Training Loss (Full): 1.6181211471557617\n",
      "Epoch: 16, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.802805781364441\n",
      "LC Training Loss (Full): 1.80220627784729\n",
      "Epoch: 16, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.6502392292022705\n",
      "LC Training Loss (Full): 1.649316668510437\n",
      "Epoch: 16, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.6797021627426147\n",
      "LC Training Loss (Full): 1.6789536476135254\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 16, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.687211513519287\n",
      "LC Training Loss (Full): 1.6828746795654297\n",
      "Epoch: 16, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.8039623498916626\n",
      "LC Training Loss (Full): 1.8037208318710327\n",
      "Epoch: 16, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.8253897428512573\n",
      "LC Training Loss (Full): 1.7987060546875\n",
      "Epoch: 16, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_64\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_64\n",
      "LoRA+LC Training Loss (Decomposed): 1.6799898147583008\n",
      "LC Training Loss (Full): 1.6800999641418457\n",
      "Epoch: 16, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7136571407318115\n",
      "LC Training Loss (Full): 1.6800012588500977\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 16, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.6850075721740723\n",
      "LC Training Loss (Full): 1.684417486190796\n",
      "Epoch: 16, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.710882544517517\n",
      "LC Training Loss (Full): 1.7098848819732666\n",
      "Epoch: 16, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.7188448905944824\n",
      "LC Training Loss (Full): 1.713995337486267\n",
      "Epoch: 16, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.587936282157898\n",
      "LC Training Loss (Full): 1.5860471725463867\n",
      "Epoch: 16, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.6838668584823608\n",
      "LC Training Loss (Full): 1.681809663772583\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 16, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.8030437231063843\n",
      "LC Training Loss (Full): 1.8058816194534302\n",
      "Epoch: 16, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.6361361742019653\n",
      "LC Training Loss (Full): 1.6170414686203003\n",
      "Epoch: 16, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.711584448814392\n",
      "LC Training Loss (Full): 1.6803381443023682\n",
      "Epoch: 16, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_65\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_65\n",
      "LoRA+LC Training Loss (Decomposed): 1.7728908061981201\n",
      "LC Training Loss (Full): 1.7732481956481934\n",
      "Epoch: 16, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6193311214447021\n",
      "LC Training Loss (Full): 1.6174890995025635\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.753\n",
      "model accuracy: 0.753\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.753, Decomposed-Restored accuracy: 0.753\n",
      "Epoch: 16, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.6797164678573608\n",
      "LC Training Loss (Full): 1.6804310083389282\n",
      "Epoch: 16, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.7054128646850586\n",
      "LC Training Loss (Full): 1.682570219039917\n",
      "Epoch: 16, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.8429862260818481\n",
      "LC Training Loss (Full): 1.8416972160339355\n",
      "Epoch: 16, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.623124361038208\n",
      "LC Training Loss (Full): 1.6179167032241821\n",
      "Epoch: 16, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.7402377128601074\n",
      "LC Training Loss (Full): 1.7404793500900269\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.752\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.752, Decomposed-Restored accuracy: 0.752\n",
      "Epoch: 16, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.702128529548645\n",
      "LC Training Loss (Full): 1.6802653074264526\n",
      "Epoch: 16, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.7473316192626953\n",
      "LC Training Loss (Full): 1.7443095445632935\n",
      "Epoch: 16, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.6594483852386475\n",
      "LC Training Loss (Full): 1.6540738344192505\n",
      "Epoch: 16, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_66\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_66\n",
      "LoRA+LC Training Loss (Decomposed): 1.618981957435608\n",
      "LC Training Loss (Full): 1.5893498659133911\n",
      "Epoch: 16, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_67\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_67/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8022679090499878\n",
      "LC Training Loss (Full): 1.8013938665390015\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.754, Decomposed-Restored accuracy: 0.754\n",
      "Epoch: 16, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_67\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_67\n",
      "LoRA+LC Training Loss (Decomposed): 1.836404800415039\n",
      "LC Training Loss (Full): 1.8367103338241577\n",
      "Epoch: 17, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.8056552410125732\n",
      "LC Training Loss (Full): 1.8051128387451172\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "Epoch: 17, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.712998390197754\n",
      "LC Training Loss (Full): 1.7112802267074585\n",
      "Epoch: 17, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.7730441093444824\n",
      "LC Training Loss (Full): 1.7722034454345703\n",
      "Epoch: 17, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.6786597967147827\n",
      "LC Training Loss (Full): 1.678606390953064\n",
      "Epoch: 17, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.773608684539795\n",
      "LC Training Loss (Full): 1.7735707759857178\n",
      "Epoch: 17, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.7312898635864258\n",
      "LC Training Loss (Full): 1.7105109691619873\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.75\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.753\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.754, Decomposed-Restored accuracy: 0.753\n",
      "Epoch: 17, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.6818370819091797\n",
      "LC Training Loss (Full): 1.681260347366333\n",
      "Epoch: 17, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.7201027870178223\n",
      "LC Training Loss (Full): 1.715177297592163\n",
      "Epoch: 17, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.5853612422943115\n",
      "LC Training Loss (Full): 1.5857740640640259\n",
      "Epoch: 17, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_68\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_68\n",
      "LoRA+LC Training Loss (Decomposed): 1.6311811208724976\n",
      "LC Training Loss (Full): 1.5938080549240112\n",
      "Epoch: 17, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.805723786354065\n",
      "LC Training Loss (Full): 1.8050414323806763\n",
      "Training Accuracy | Decomposed: 0.65625, Full : 0.65625\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.754, Decomposed-Restored accuracy: 0.754\n",
      "Epoch: 17, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.6790539026260376\n",
      "LC Training Loss (Full): 1.679134488105774\n",
      "Epoch: 17, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.6840078830718994\n",
      "LC Training Loss (Full): 1.6827669143676758\n",
      "Epoch: 17, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.6790837049484253\n",
      "LC Training Loss (Full): 1.6787830591201782\n",
      "Epoch: 17, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.5782852172851562\n",
      "LC Training Loss (Full): 1.559043049812317\n",
      "Epoch: 17, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.6813788414001465\n",
      "LC Training Loss (Full): 1.680338740348816\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.754, Decomposed-Restored accuracy: 0.754\n",
      "Epoch: 17, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.7812765836715698\n",
      "LC Training Loss (Full): 1.7702821493148804\n",
      "Epoch: 17, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.6727982759475708\n",
      "LC Training Loss (Full): 1.6521536111831665\n",
      "Epoch: 17, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.7756552696228027\n",
      "LC Training Loss (Full): 1.7746464014053345\n",
      "Epoch: 17, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_69\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_69\n",
      "LoRA+LC Training Loss (Decomposed): 1.6802740097045898\n",
      "LC Training Loss (Full): 1.6795125007629395\n",
      "Epoch: 17, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6578004360198975\n",
      "LC Training Loss (Full): 1.6525315046310425\n",
      "Training Accuracy | Decomposed: 0.8125, Full : 0.8125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.754, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 17, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.6186425685882568\n",
      "LC Training Loss (Full): 1.6173099279403687\n",
      "Epoch: 17, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.745111346244812\n",
      "LC Training Loss (Full): 1.7130346298217773\n",
      "Epoch: 17, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.8165206909179688\n",
      "LC Training Loss (Full): 1.8043938875198364\n",
      "Epoch: 17, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.619251012802124\n",
      "LC Training Loss (Full): 1.6169495582580566\n",
      "Epoch: 17, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.7375518083572388\n",
      "LC Training Loss (Full): 1.74027681350708\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.71875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.755, Decomposed-Restored accuracy: 0.755\n",
      "Epoch: 17, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.750976800918579\n",
      "LC Training Loss (Full): 1.7429964542388916\n",
      "Epoch: 17, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.7724601030349731\n",
      "LC Training Loss (Full): 1.7719509601593018\n",
      "Epoch: 17, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.7142291069030762\n",
      "LC Training Loss (Full): 1.712913990020752\n",
      "Epoch: 17, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_70\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_70\n",
      "LoRA+LC Training Loss (Decomposed): 1.7471923828125\n",
      "LC Training Loss (Full): 1.7449506521224976\n",
      "Epoch: 17, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_71\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_71/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.698035717010498\n",
      "LC Training Loss (Full): 1.671860933303833\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.754\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.755, Decomposed-Restored accuracy: 0.754\n",
      "Epoch: 17, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_71\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_71\n",
      "LoRA+LC Training Loss (Decomposed): 1.714962124824524\n",
      "LC Training Loss (Full): 1.7163881063461304\n",
      "Epoch: 18, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.744816780090332\n",
      "LC Training Loss (Full): 1.710518717765808\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.75\n",
      "Epoch: 18, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.711111307144165\n",
      "LC Training Loss (Full): 1.7100045680999756\n",
      "Epoch: 18, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.7182210683822632\n",
      "LC Training Loss (Full): 1.7125153541564941\n",
      "Epoch: 18, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.7324219942092896\n",
      "LC Training Loss (Full): 1.7167017459869385\n",
      "Epoch: 18, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.5252572298049927\n",
      "LC Training Loss (Full): 1.5245412588119507\n",
      "Epoch: 18, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.6464190483093262\n",
      "LC Training Loss (Full): 1.626484990119934\n",
      "Training Accuracy | Decomposed: 0.84375, Full : 0.84375\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.755\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.755, Decomposed-Restored accuracy: 0.755\n",
      "Epoch: 18, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.6578360795974731\n",
      "LC Training Loss (Full): 1.6492547988891602\n",
      "Epoch: 18, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.7192013263702393\n",
      "LC Training Loss (Full): 1.7162305116653442\n",
      "Epoch: 18, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.7315497398376465\n",
      "LC Training Loss (Full): 1.7363812923431396\n",
      "Epoch: 18, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_72\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_72\n",
      "LoRA+LC Training Loss (Decomposed): 1.6796847581863403\n",
      "LC Training Loss (Full): 1.6788506507873535\n",
      "Epoch: 18, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7093456983566284\n",
      "LC Training Loss (Full): 1.7094870805740356\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 18, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.7134318351745605\n",
      "LC Training Loss (Full): 1.7123228311538696\n",
      "Epoch: 18, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.904778003692627\n",
      "LC Training Loss (Full): 1.8990281820297241\n",
      "Epoch: 18, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.651235818862915\n",
      "LC Training Loss (Full): 1.6504907608032227\n",
      "Epoch: 18, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.7429968118667603\n",
      "LC Training Loss (Full): 1.7411549091339111\n",
      "Epoch: 18, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.678497314453125\n",
      "LC Training Loss (Full): 1.6772314310073853\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 18, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.7729517221450806\n",
      "LC Training Loss (Full): 1.7719272375106812\n",
      "Epoch: 18, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.6506656408309937\n",
      "LC Training Loss (Full): 1.6490201950073242\n",
      "Epoch: 18, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.5879185199737549\n",
      "LC Training Loss (Full): 1.5884238481521606\n",
      "Epoch: 18, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_73\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_73\n",
      "LoRA+LC Training Loss (Decomposed): 1.6828962564468384\n",
      "LC Training Loss (Full): 1.6805845499038696\n",
      "Epoch: 18, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7723190784454346\n",
      "LC Training Loss (Full): 1.7640262842178345\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 18, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.7192658185958862\n",
      "LC Training Loss (Full): 1.6830226182937622\n",
      "Epoch: 18, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.8060743808746338\n",
      "LC Training Loss (Full): 1.8056749105453491\n",
      "Epoch: 18, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.711532711982727\n",
      "LC Training Loss (Full): 1.7111730575561523\n",
      "Epoch: 18, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.8225195407867432\n",
      "LC Training Loss (Full): 1.8083891868591309\n",
      "Epoch: 18, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.7726194858551025\n",
      "LC Training Loss (Full): 1.77308988571167\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 18, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.8077811002731323\n",
      "LC Training Loss (Full): 1.806326150894165\n",
      "Epoch: 18, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.6757256984710693\n",
      "LC Training Loss (Full): 1.678437352180481\n",
      "Epoch: 18, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.556678056716919\n",
      "LC Training Loss (Full): 1.5540021657943726\n",
      "Epoch: 18, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_74\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_74\n",
      "LoRA+LC Training Loss (Decomposed): 1.6301103830337524\n",
      "LC Training Loss (Full): 1.6178929805755615\n",
      "Epoch: 18, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_75\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_75/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7334201335906982\n",
      "LC Training Loss (Full): 1.7109169960021973\n",
      "Training Accuracy | Decomposed: 0.71875, Full : 0.75\n",
      "model accuracy: 0.759\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.759\n",
      "Full accuracy: 0.759, LC accuracy: 0.759, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 18, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_75\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_75\n",
      "LoRA+LC Training Loss (Decomposed): 1.7111501693725586\n",
      "LC Training Loss (Full): 1.7111501693725586\n",
      "Epoch: 19, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7060972452163696\n",
      "LC Training Loss (Full): 1.7082313299179077\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "Epoch: 19, Iteration: 1\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.6918190717697144\n",
      "LC Training Loss (Full): 1.7023078203201294\n",
      "Epoch: 19, Iteration: 2\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.719853162765503\n",
      "LC Training Loss (Full): 1.7108687162399292\n",
      "Epoch: 19, Iteration: 3\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.5550501346588135\n",
      "LC Training Loss (Full): 1.5543769598007202\n",
      "Epoch: 19, Iteration: 4\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.7946051359176636\n",
      "LC Training Loss (Full): 1.7776036262512207\n",
      "Epoch: 19, Iteration: 5\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.7717982530593872\n",
      "LC Training Loss (Full): 1.771276831626892\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.76\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.76\n",
      "Full accuracy: 0.76, LC accuracy: 0.76, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 19, Iteration: 6\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.568381428718567\n",
      "LC Training Loss (Full): 1.556999921798706\n",
      "Epoch: 19, Iteration: 7\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.6392109394073486\n",
      "LC Training Loss (Full): 1.6250741481781006\n",
      "Epoch: 19, Iteration: 8\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.618767499923706\n",
      "LC Training Loss (Full): 1.6180827617645264\n",
      "Epoch: 19, Iteration: 9\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_76\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_76\n",
      "LoRA+LC Training Loss (Decomposed): 1.6797000169754028\n",
      "LC Training Loss (Full): 1.6786149740219116\n",
      "Epoch: 19, Iteration: 10\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.7140564918518066\n",
      "LC Training Loss (Full): 1.7091776132583618\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.76\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.76\n",
      "Full accuracy: 0.76, LC accuracy: 0.76, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 19, Iteration: 11\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.7417362928390503\n",
      "LC Training Loss (Full): 1.7432785034179688\n",
      "Epoch: 19, Iteration: 12\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.6491000652313232\n",
      "LC Training Loss (Full): 1.6176891326904297\n",
      "Epoch: 19, Iteration: 13\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.776417851448059\n",
      "LC Training Loss (Full): 1.776692509651184\n",
      "Epoch: 19, Iteration: 14\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.775182843208313\n",
      "LC Training Loss (Full): 1.7767665386199951\n",
      "Epoch: 19, Iteration: 15\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.7717597484588623\n",
      "LC Training Loss (Full): 1.7714219093322754\n",
      "Training Accuracy | Decomposed: 0.6875, Full : 0.6875\n",
      "model accuracy: 0.76\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.76\n",
      "Full accuracy: 0.76, LC accuracy: 0.76, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 19, Iteration: 16\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.7453080415725708\n",
      "LC Training Loss (Full): 1.7428011894226074\n",
      "Epoch: 19, Iteration: 17\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.6665325164794922\n",
      "LC Training Loss (Full): 1.6266504526138306\n",
      "Epoch: 19, Iteration: 18\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.8047746419906616\n",
      "LC Training Loss (Full): 1.804372787475586\n",
      "Epoch: 19, Iteration: 19\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_77\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_77\n",
      "LoRA+LC Training Loss (Decomposed): 1.8038737773895264\n",
      "LC Training Loss (Full): 1.8033902645111084\n",
      "Epoch: 19, Iteration: 20\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.6764205694198608\n",
      "LC Training Loss (Full): 1.6792770624160767\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.76\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.76\n",
      "Full accuracy: 0.76, LC accuracy: 0.76, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 19, Iteration: 21\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.8419549465179443\n",
      "LC Training Loss (Full): 1.837303876876831\n",
      "Epoch: 19, Iteration: 22\n",
      "Saving Checkpoint: lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_1.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.66594660282135\n",
      "LC Training Loss (Full): 1.650496006011963\n",
      "Epoch: 19, Iteration: 23\n",
      "Saving Checkpoint: lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_2.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.8052958250045776\n",
      "LC Training Loss (Full): 1.8045473098754883\n",
      "Epoch: 19, Iteration: 24\n",
      "Saving Checkpoint: lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_3.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.6490384340286255\n",
      "LC Training Loss (Full): 1.6502997875213623\n",
      "Epoch: 19, Iteration: 25\n",
      "Saving Checkpoint: lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_4.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.7108772993087769\n",
      "LC Training Loss (Full): 1.7105257511138916\n",
      "Training Accuracy | Decomposed: 0.75, Full : 0.75\n",
      "model accuracy: 0.76\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.76\n",
      "Full accuracy: 0.76, LC accuracy: 0.76, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 19, Iteration: 26\n",
      "Saving Checkpoint: lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_5.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.6542539596557617\n",
      "LC Training Loss (Full): 1.6496278047561646\n",
      "Epoch: 19, Iteration: 27\n",
      "Saving Checkpoint: lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_6.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.7412056922912598\n",
      "LC Training Loss (Full): 1.741286277770996\n",
      "Epoch: 19, Iteration: 28\n",
      "Saving Checkpoint: lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_7.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.6783751249313354\n",
      "LC Training Loss (Full): 1.6783198118209839\n",
      "Epoch: 19, Iteration: 29\n",
      "Saving Checkpoint: lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_78\n",
      "Saving Checkpoint: old_lc_checkpoint_8.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_78\n",
      "LoRA+LC Training Loss (Decomposed): 1.672833800315857\n",
      "LC Training Loss (Full): 1.6411263942718506\n",
      "Epoch: 19, Iteration: 30\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_79\\base_model.pt\n",
      "old_lc | saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_79/initial_model.pt\n",
      "LoRA+LC Training Loss (Decomposed): 1.682599425315857\n",
      "LC Training Loss (Full): 1.6791361570358276\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.78125\n",
      "model accuracy: 0.76\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.756\n",
      "model accuracy: 0.76\n",
      "Full accuracy: 0.76, LC accuracy: 0.76, Decomposed-Full accuracy: 0.756, Decomposed-Restored accuracy: 0.756\n",
      "Epoch: 19, Iteration: 31\n",
      "Saving Checkpoint: lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/lobranch/set_79\n",
      "Saving Checkpoint: old_lc_checkpoint_0.pt @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/old-lc/set_79\n",
      "LoRA+LC Training Loss (Decomposed): 1.5884549617767334\n",
      "LC Training Loss (Full): 1.5870239734649658\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store the maximum and minimum values of the delta and decomposed delta\n",
    "delta_normal_max = []\n",
    "delta_normal_min = []\n",
    "delta_decomposed_max = []\n",
    "delta_decomposed_min = []\n",
    "\n",
    "# Initialize lists to store the maximum and minimum values of the compressed delta and decomposed delta\n",
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "\n",
    "# Initialize the current iteration and set to 0\n",
    "current_iter = 0\n",
    "current_set = 0\n",
    "\n",
    "# Initialize the current iteration and set to 0 for the old LC method\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0\n",
    "\n",
    "# Define a function to evaluate the accuracy of the model on the test set\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n",
    "\n",
    "# Train the model for 20 epochs\n",
    "for epch in range(20):\n",
    "    # Iterate over the training data\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"Epoch: {}, Iteration: {}\".format(epch, i))\n",
    "        \n",
    "        # Create a new set directory if it does not exist\n",
    "        set_path = \"/set_{}\".format(current_set)\n",
    "        if not os.path.exists(SAVE_LOC + set_path):\n",
    "            os.makedirs(SAVE_LOC + set_path)\n",
    "\n",
    "        # Check if it is the first iteration of the first epoch\n",
    "        if i == 0 and epch == 0: # first iteration, create baseline model\n",
    "            base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "        # Check if it is a full snapshot\n",
    "        else:\n",
    "            # Check if the iteration is a multiple of 10\n",
    "            if i % 10 == 0: \n",
    "                # full snapshot!\n",
    "\n",
    "                new_model = lazy_restore_gpu(base, base_decomp, bias, ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "                # Changing previous \"original model\" used to restore the loRA model.\n",
    "                original = new_model \n",
    "                \n",
    "                current_set += 1\n",
    "                current_iter = 0\n",
    "\n",
    "                # Create a new set directory if it does not exist\n",
    "                set_path = \"/set_{}\".format(current_set)\n",
    "                if not os.path.exists(SAVE_LOC + set_path):\n",
    "                    os.makedirs(SAVE_LOC + set_path)\n",
    "                \n",
    "                # Rebuilding LoRA layers => reset model!\n",
    "\n",
    "                # Get the base model weights and biases\n",
    "                w, b = getBase(original)\n",
    "                # Create a new model with the base weights and specified rank\n",
    "                model = ViT_LowRank(w, b , RANK, (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "                # Optimizer for the model's parameters with Stochastic Gradient Descent (SGD) and specified learning rate\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "                # Load state dictionary from full snapshot, including specified decomposed layers, and load it into the model\n",
    "                load_sd_decomp(original.state_dict(), model, DECOMPOSED_LAYERS)\n",
    "                # The base for all delta calculations\n",
    "                base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "            # If it is not a full snapshot, perform delta compression\n",
    "            else:\n",
    "                # Delta-compression : The delta for the weights of the normal and decomposed layers.\n",
    "                # Also returns the full dictionary, which holds the bias.\n",
    "                delta, decomp_delta, bias = lc.generate_delta_gpu(base, \n",
    "                                                                base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                \n",
    "                # Compressing the delta and decomposed delta\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                \n",
    "                # Saving checkpoint\n",
    "                lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                                \"/set_{}\".format(current_set))\n",
    "    \n",
    "                # Update base and base_decomp\n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                # Update current iteration\n",
    "                current_iter += 1\n",
    "            \n",
    "        # # ==========================\n",
    "        # # Saving using LC-Checkpoint\n",
    "        # # ==========================\n",
    "                \n",
    "        if i == 0 and epch == 0:\n",
    "            cstate = model_original.state_dict()\n",
    "            set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "            if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "            prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path,DECOMPOSED_LAYERS)\n",
    "        else:\n",
    "            if i % 10 == 0:\n",
    "                cstate = model_original.state_dict()\n",
    "                current_set_old_lc += 1\n",
    "                current_iter_old_lc = 0\n",
    "                set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                    os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path, DECOMPOSED_LAYERS)\n",
    "            else:\n",
    "                cstate = model_original.state_dict()\n",
    "                old_lc_delta, old_lc_bias = olc.generate_delta_gpu(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "                olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                                    old_lc_bias, current_iter_old_lc)\n",
    "                prev_state = np.add(prev_state, update_prev)\n",
    "                current_iter_old_lc += 1\n",
    "        \n",
    "        # ==========================\n",
    "        # Training on Low-Rank Model\n",
    "        # ==========================\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs,labels)\n",
    "        print(\"LoRA+LC Training Loss (Decomposed): {}\".format(loss.item()))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # ======================\n",
    "        # Training on Full Model\n",
    "        # ======================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_full.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs_full = model_original(inputs)\n",
    "        loss_full = torch.nn.functional.cross_entropy(outputs_full,labels)\n",
    "        print(\"LC Training Loss (Full): {}\".format(loss_full.item()))\n",
    "        loss_full.backward()\n",
    "        optimizer_full.step()\n",
    "\n",
    "        # Print training accuracy every 20 iterations\n",
    "        if i % 5 == 0:\n",
    "            print(\"Training Accuracy | Decomposed: {}, Full : {}\".format(acc(outputs, labels), \n",
    "                                                                         acc(outputs_full, labels)))\n",
    "\n",
    "        # Evaluation every 5 iterations\n",
    "        if i != 0  and i % 5 == 0: # Evaluation on testing set\n",
    "\n",
    "            # Evaluate the accuracy of the model on the test set\n",
    "            full_accuracy.append(evaluate_accuracy_gpu(model_original, test_loader))\n",
    "\n",
    "            # Evaluate the accuracy of the model on the test set for decomposed model\n",
    "            decomposed_full_accuracy.append(evaluate_accuracy_gpu(model, test_loader))\n",
    "\n",
    "            # Restore the model from the full snapshot\n",
    "            restored_model = lazy_restore_gpu(base, base_decomp, bias, ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                          rank = RANK, scaling = SCALING)\n",
    "\n",
    "            # Evaluate the accuracy of the restored model on the test set\n",
    "            restored_accuracy.append(evaluate_accuracy(restored_model, test_loader))\n",
    "\n",
    "            # Restore VGG16NoLite model from the old LC method\n",
    "            restored_lc_model = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "            restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "                                                                  restored_model.state_dict(), DECOMPOSED_LAYERS))\n",
    "            lc_accuracy.append(evaluate_accuracy_gpu(restored_lc_model, test_loader))\n",
    "            print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\n",
    "                full_accuracy[-1], lc_accuracy[-1], decomposed_full_accuracy[-1], restored_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For recovery and not restart from scratch : having the plots and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Open a file to save the data (if it exists, otherwise ignore this cell)\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Update the data with the new values\n",
    "full_accuracy = data['full_acc']\n",
    "lc_accuracy = data[\"lc_restored_accuracy\"]\n",
    "restored_accuracy = data[\"decomposed_restored_accuracy\"]\n",
    "decomposed_full_accuracy = data[\"decomposed_full_accuracy\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"Vision Transformer (ViT), Accuracy, Branched @ {} Accuracy\".format(BRANCH_ACC))\n",
    "plt.plot(full_accuracy, label = \"Default Vision Transformer (ViT)\")\n",
    "plt.plot(lc_accuracy, label = \"LC Vision Transformer (ViT)\")\n",
    "plt.plot(decomposed_full_accuracy, label = \"dLoRA Vision Transformer (ViT)\")\n",
    "plt.plot(restored_accuracy, label = \"dLoRA + LC Vision Transformer (ViT)\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the absolute accuracy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 4000x1000 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rangex = [x for x in range(0, 120) if x % 6 == 0]\n",
    "rangey = [x for x in range(0, 20)]\n",
    "plt.figure(figsize = (40, 10))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"Vision Transformer (ViT) Absolute Accuracy Loss (Default VGG-16 vs LC + dLoRA VGG-16)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(restored_accuracy))), label = \"LC + dLoRA Vision Transformer (ViT)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC Vision Transformer (ViT)\")\n",
    "plt.legend()\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.ylim(0, 0.5)\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title(\"Vision Transformer (ViT) Absolute Restoration Accuracy Loss (LC + dLoRA Vision Transformer (ViT) & LC Vision Transformer (ViT))\")\n",
    "plt.plot(np.abs(np.subtract(np.array(restored_accuracy), \n",
    "                     np.array(decomposed_full_accuracy))), label = \"LC + dLoRA Vision Transformer (ViT)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC Vision Transformer (ViT)\")\n",
    "plt.legend()\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function which compute the size of compressed and uncompressed model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getsize(sl):\n",
    "    dir = [x for x in os.listdir(sl)]\n",
    "    csize, usize = 0, 0\n",
    "    for set in dir:\n",
    "        for f in os.listdir(sl + \"/\" + set):\n",
    "            fp = sl + \"/{}/{}\".format(set, f)\n",
    "            csize += os.path.getsize(fp)\n",
    "            usize += 24 * math.pow(2, 10) # torch checkpoint same size\n",
    "    return csize, usize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print compression ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LC-Checkpoint + GZIP\n",
      "Compression Ratio: 244.15200000000002%, Space Savings: 59.041999999999994%\n",
      "LoRA + LC-Checkpoint + GZIP\n",
      "Compression Ratio: 223.928%, Space Savings: 55.342999999999996%\n"
     ]
    }
   ],
   "source": [
    "compressed_size, uncompressed_size = getsize(SAVE_LOC)\n",
    "a, b = evaluate_compression(uncompressed_size, compressed_size)\n",
    "compressed_size, uncompressed_size = getsize(SAVE_LOC_OLC)\n",
    "a1, b1 = evaluate_compression(uncompressed_size, compressed_size)\n",
    "\n",
    "print(\"LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a1, b1))\n",
    "print(\"LoRA + LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_comparison = ViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "\n",
    "# save model into \"comparison_model.pt\"\n",
    "torch.save(model_for_comparison.state_dict(), HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/baseline_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store data in a dictionary and save it in a file data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"full_acc\" : full_accuracy,\n",
    "    \"decomposed_restored_accuracy\" : restored_accuracy,\n",
    "    \"decomposed_full_accuracy\" : decomposed_full_accuracy,\n",
    "    \"lc_restored_accuracy\" : lc_accuracy\n",
    "}\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vit/data.json\", 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
