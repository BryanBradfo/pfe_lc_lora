{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Efficient Checkpointing on LeNet** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare these 3 approaches : \n",
    "1. **Initial accuracy measurement:** Train LeNet on MNIST and achieve a baseline accuracy of around 99.9% without considering poisoned models.\n",
    "2. **Incremental learning:** Implement incremental learning on the divided MNIST subsets and measure the accuracy drop due to this method.\n",
    "3. **LC-checkpoint and Delta LoRA:** Apply LC-checkpoint and Delta LoRA on top of incremental learning and observe the resulting accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "import old_lc.main as olc\n",
    "from src.models.LeNet import LeNet\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.LeNet_LowRank import getBase, LeNet_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, evaluate_accuracy_gpu, lazy_restore,lazy_restore_gpu, evaluate_compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Connexion to wandb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\Bradf\\.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "# Connect to W&B\n",
    "wandb.login(key=\"beb938fdf67db528128a4298e19b9997afd83dfd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variables and Constants**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 64\n",
    "test_batch_size = 1000\n",
    "num_work = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load MNIST dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader():\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize(0.1307, 0.3081)\n",
    "    ])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "\n",
    "    trainset.data = trainset.data.clone()[:]\n",
    "    trainset.targets = trainset.targets.clone()[:]\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = train_batch_size,\n",
    "                                              shuffle=True, num_workers=num_work)\n",
    "\n",
    "    testset = datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    testset.data = testset.data.clone()[:]\n",
    "    testset.targets = testset.targets.clone()[:]\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = test_batch_size,\n",
    "                                             shuffle=False, num_workers=num_work)\n",
    "    \n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Bypass the matplotlib error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verify if data loaded correctly**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify the 10 classes of training dataset and 10 classes of testing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the dataloader to extract an image per class\n",
    "def get_images_by_class(dataloader):\n",
    "    images_by_class = {i: None for i in range(10)}\n",
    "    for images, labels in dataloader:\n",
    "        for i in range(len(labels)):\n",
    "            label = labels[i].item()\n",
    "            if images_by_class[label] is None:\n",
    "                images_by_class[label] = images[i]\n",
    "            if all(v is not None for v in images_by_class.values()):\n",
    "                return images_by_class\n",
    "    return images_by_class\n",
    "\n",
    "def plot_images(images_by_class, title):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(15, 1.5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    for i in range(10):\n",
    "        ax = axes[i]\n",
    "        ax.imshow(images_by_class[i].squeeze(), cmap='gray')\n",
    "        ax.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_images = get_images_by_class(train_loader)\n",
    "test_images = get_images_by_class(test_loader)\n",
    "\n",
    "plot_images(train_images, \"Trainset Images by Class\")\n",
    "plot_images(test_images, \"Testset Images by Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify that the 10 first images are visually different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_first_images(dataloader, title, num_images=10):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(15, 1.5))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    \n",
    "    images_shown = 0\n",
    "    for images, labels in dataloader:\n",
    "        for i in range(len(images)):\n",
    "            if images_shown >= num_images:\n",
    "                break\n",
    "            ax = axes[images_shown]\n",
    "            ax.imshow(images[i].squeeze(), cmap='gray')\n",
    "            ax.axis('off')\n",
    "            images_shown += 1\n",
    "        if images_shown >= num_images:\n",
    "            break\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_first_images(train_loader, \"First 10 Trainset Images\")\n",
    "plot_first_images(test_loader, \"First 10 Testset Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the dataset into three subsets having each all classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_loader1: 29997\n",
      "Size of train_loader2: 14998\n",
      "Size of train_loader3: 15005\n",
      "Size of train_loader: 60000\n",
      "Size of test_loader: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "def stratified_split(dataset, proportions):\n",
    "    class_indices = [np.where(np.array(dataset.targets) == i)[0] for i in range(10)]\n",
    "    \n",
    "    split_indices = []\n",
    "    for proportion in proportions:\n",
    "        class_split_indices = [np.split(indices, [int(proportion[0]*len(indices)), int((proportion[0]+proportion[1])*len(indices))]) for indices in class_indices]\n",
    "        split_indices.append([np.concatenate([split[i] for split in class_split_indices]) for i in range(3)])\n",
    "    \n",
    "    return split_indices\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(), \n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    # Load the whole MNIST dataset\n",
    "    full_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    # Proportions pour les splits\n",
    "    proportions = [(0.5, 0.25, 0.25)] * 10\n",
    "    \n",
    "    # Obtenir les indices pour chaque split\n",
    "    split_indices = stratified_split(full_trainset, proportions)\n",
    "    \n",
    "    # Cr√©er des Subsets\n",
    "    trainset1 = Subset(full_trainset, split_indices[0][0])\n",
    "    trainset2 = Subset(full_trainset, split_indices[0][1])\n",
    "    trainset3 = Subset(full_trainset, split_indices[0][2])\n",
    "    \n",
    "    # Cr√©er des DataLoaders pour chacun des sous-ensembles\n",
    "    train_loader1 = DataLoader(trainset1, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader2 = DataLoader(trainset2, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "    train_loader3 = DataLoader(trainset3, batch_size=train_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Charger le jeu de donn√©es de test complet\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(testset, batch_size=test_batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    return train_loader1, train_loader2, train_loader3, test_loader\n",
    "\n",
    "# Load DataLoaders\n",
    "train_loader1, train_loader2, train_loader3, test_loader = data_loader()\n",
    "\n",
    "# V√©rification des tailles des DataLoaders\n",
    "print(f'Size of train_loader1: {len(train_loader1.dataset)}')\n",
    "print(f'Size of train_loader2: {len(train_loader2.dataset)}')\n",
    "print(f'Size of train_loader3: {len(train_loader3.dataset)}')\n",
    "print(f'Size of train_loader: {len(train_loader1.dataset) + len(train_loader2.dataset) + len(train_loader3.dataset)}')\n",
    "print(f'Size of test_loader: {len(test_loader.dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Verify the content of train_loader subset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify the 10 classes of training dataset and 10 classes of testing dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going through the dataloader to extract an image per class\n",
    "\n",
    "train_images1 = get_images_by_class(train_loader1)\n",
    "train_images2 = get_images_by_class(train_loader2)\n",
    "train_images3 = get_images_by_class(train_loader3)\n",
    "test_images = get_images_by_class(test_loader)\n",
    "\n",
    "plot_images(train_images1, \"Trainset Subset 1 Images by Class\")\n",
    "plot_images(train_images2, \"Trainset Subset 2 Images by Class\")\n",
    "plot_images(train_images3, \"Trainset Subset 3 Images by Class\")\n",
    "plot_images(test_images, \"Testset Images by Class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Verify that the 10 first images are visually different**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_first_images(train_loader1, \"First 10 Trainset Subset 1 Images\")\n",
    "plot_first_images(train_loader2, \"First 10 Trainset Subset 2 Images\")\n",
    "plot_first_images(train_loader3, \"First 10 Trainset Subset 3 Images\")\n",
    "plot_first_images(test_loader, \"First 10 Testset Images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Defining some variables and creating files & folders**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "SAVE_LOC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/lobranch\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "\n",
    "SAVE_LOC_OLC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/lenet/old-lc\"\n",
    "if not os.path.exists(SAVE_LOC_OLC):\n",
    "    os.makedirs(SAVE_LOC_OLC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **First version : LeNet without Incremental Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_for_checkpoint = LeNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_for_checkpoint = nn.DataParallel(model_for_checkpoint)\n",
    "    model_for_checkpoint.to(device)\n",
    "else:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU!\")\n",
    "    model_for_checkpoint.to(device)\n",
    "\n",
    "# Training code\n",
    "NUM_EPOCHES = 50\n",
    "learning_rate = 0.02\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_for_checkpoint.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"LeNet\", \n",
    "           name=\"LeNet-Without-Incremental-Learning\", \n",
    "           tags=[\"LeNet\", \"Without-Incremental-Learning\", \"MNIST\"],\n",
    "           config={\"num_epoches\": NUM_EPOCHES,\n",
    "                    \"model\": \"LeNet\",\n",
    "                    \"train dataset\": \"CIFAR10 train dataset[:]\",\n",
    "                    \"test dataset\": \"CIFAR10 test dataset[:]\",\n",
    "                    \"batch_size on training\": train_batch_size,\n",
    "                    \"batch_size on testing\": test_batch_size,\n",
    "                    \"num_workers\": num_work,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"optimizer\": \"SGD\",\n",
    "                }\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start of model training...\")\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_for_checkpoint.train()\n",
    "        for iter, data in enumerate(train_loader):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_for_checkpoint(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_for_checkpoint.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_for_checkpoint(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training...\")\n",
    "\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Second version : LeNet with Incremental Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_incremental_learning = LeNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_with_incremental_learning = nn.DataParallel(model_with_incremental_learning)\n",
    "    model_with_incremental_learning.to(device)\n",
    "else:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU!\")\n",
    "    model_with_incremental_learning.to(device)\n",
    "\n",
    "# Training code\n",
    "NUM_EPOCHES = 45\n",
    "learning_rate = 0.01\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_with_incremental_learning.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"LeNet\", \n",
    "           name=\"LeNet-With-Incremental-Learning\", \n",
    "           tags=[\"LeNet\", \"With-Incremental-Learning\", \"MNIST\"],\n",
    "           config={\"num_epoches\": NUM_EPOCHES,\n",
    "                    \"model\": \"LeNet\",\n",
    "                    \"train dataset\": \"CIFAR10 train dataloader1\",\n",
    "                    \"test dataset\": \"CIFAR10 test dataset[:]\",\n",
    "                    \"batch_size on training\": train_batch_size,\n",
    "                    \"batch_size on testing\": test_batch_size,\n",
    "                    \"num_workers\": num_work,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"optimizer\": \"SGD\",\n",
    "                }\n",
    "           )\n",
    "\n",
    "\n",
    "\n",
    "print(\"Start of model training on dataloader1...\")\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning.train()\n",
    "        for iter, data in enumerate(train_loader1):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader1.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader1...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# Training code on dataloader2\n",
    "print(\"Start of model training on dataloader2...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning.train()\n",
    "        for iter, data in enumerate(train_loader2):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader2.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader2...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# Training code on dataloader2\n",
    "print(\"Start of model training on dataloader3...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning.train()\n",
    "        for iter, data in enumerate(train_loader3):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader3.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader3...\")\n",
    "# Finish the wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Third version : LeNet with Incremental Learning, LC-checkpoint, and Delta-LoRA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Training on trainloader1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 1 GPU!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194c5cb9f492409dadee75d2298d3a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011111111111111112, max=1.0‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.4 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Personal\\Singapour\\PFE\\code_of_shu-heng_with_models\\pfe_lc_lora\\wandb\\run-20240704_155939-rj3c3ftq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bryanbradfo/LeNet/runs/rj3c3ftq/workspace' target=\"_blank\">LeNet-With-Incremental-Learning_LC_DLORA-Without-Restore</a></strong> to <a href='https://wandb.ai/bryanbradfo/LeNet' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bryanbradfo/LeNet' target=\"_blank\">https://wandb.ai/bryanbradfo/LeNet</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bryanbradfo/LeNet/runs/rj3c3ftq/workspace' target=\"_blank\">https://wandb.ai/bryanbradfo/LeNet/runs/rj3c3ftq/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of model training on dataloader1...\n",
      "Epoch: [0/29], Training Loss: 2.196876, Validation Loss: 1.965957, Training Accuracy: 0.315798, Validation Accuracy: 0.498500\n",
      "Epoch: [1/29], Training Loss: 1.586502, Validation Loss: 1.225370, Training Accuracy: 0.571190, Validation Accuracy: 0.682700\n",
      "Model saved at accuracy: 0.8018\n",
      "End of model training on dataloader1...\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_with_incremental_learning_lc_dlora = LeNet()\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model_with_incremental_learning_lc_dlora = nn.DataParallel(model_with_incremental_learning_lc_dlora)\n",
    "    model_with_incremental_learning_lc_dlora.to(device)\n",
    "else:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPU!\")\n",
    "    model_with_incremental_learning_lc_dlora.to(device)\n",
    "\n",
    "# Training code\n",
    "NUM_EPOCHES = 30\n",
    "learning_rate = 0.1\n",
    "learning_rate_dloralc = 0.1\n",
    "learning_rate1 = 0.005\n",
    "# super_step = len(train_loader2)\n",
    "# super_step = 20\n",
    "isLoop = True\n",
    "\n",
    "optimizer = torch.optim.SGD(model_with_incremental_learning_lc_dlora.parameters(), lr=learning_rate1)\n",
    "\n",
    "# Initialize a new W&B run\n",
    "wandb.init(project=\"LeNet\", \n",
    "           name=\"LeNet-With-Incremental-Learning_LC_DLORA-Without-Restore\", \n",
    "           tags=[\"LeNet\", \"With-Incremental-Learning_LC_DLORA\", \"MNIST\"],\n",
    "           config={\"num_epoches\": NUM_EPOCHES,\n",
    "                    \"model\": \"LeNet\",\n",
    "                    \"splitting\": \"50-25-25\",\n",
    "                    \"train dataset 1\": \"MNIST train dataloader1\",\n",
    "                    \"test dataset\": \"MNIST test dataset[:]\",\n",
    "                    \"batch_size on training\": train_batch_size,\n",
    "                    \"batch_size on testing\": test_batch_size,\n",
    "                    \"num_workers\": num_work,\n",
    "                    \"learning_rate_nothing\": learning_rate,\n",
    "                    \"learning_rate_dloralc\": learning_rate_dloralc,\n",
    "                    \"optimizer\": \"SGD\"\n",
    "                }\n",
    "           )\n",
    "\n",
    "print(\"Start of model training on dataloader1...\")\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_total = 0\n",
    "        \n",
    "        model_with_incremental_learning_lc_dlora.train()\n",
    "        for iter, data in enumerate(train_loader1):\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model_with_incremental_learning_lc_dlora(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model_with_incremental_learning_lc_dlora.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                output = model_with_incremental_learning_lc_dlora(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader1.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "\n",
    "    if valid_accuracy > 0.7:\n",
    "        rounded_valid_acc = round(valid_accuracy, 4)\n",
    "        # torch.save(model_for_checkpoint.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/vit/branch_{}.pt\".format(rounded_valid_acc))\n",
    "        torch.save(model_with_incremental_learning_lc_dlora.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(rounded_valid_acc))\n",
    "        print(\"Model saved at accuracy: {:.4f}\".format(rounded_valid_acc))\n",
    "        isLoop = False\n",
    "        break\n",
    "    # if valid_accuracy > 0.90:\n",
    "    #     isLoop = False\n",
    "    #     break\n",
    "\n",
    "    print(\"Epoch: [{}/{}], Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, train_loss, valid_loss, train_accuarcy, valid_accuracy))\n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss_dloralc\": train_loss,\n",
    "        \"valid_loss_dloralc\": valid_loss,\n",
    "        \"train_accuracy_dloralc\": train_accuarcy,\n",
    "        \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "        \"train_loss_lc\": train_loss,\n",
    "        \"valid_loss_lc\": valid_loss,\n",
    "        \"train_accuracy_lc\": train_accuarcy,\n",
    "        \"valid_accuracy_lc\": valid_accuracy, \n",
    "        \"train_loss\": train_loss,\n",
    "        \"valid_loss\": valid_loss,\n",
    "        \"train_accuracy\": train_accuarcy,\n",
    "        \"valid_accuracy\": valid_accuracy,\n",
    "        \"epoch\": epoch,\n",
    "    })\n",
    "    # wandb.log({\n",
    "    #     \"train_loss_dloralc\": train_loss,\n",
    "    #     \"valid_loss_dloralc\": valid_loss,\n",
    "    #     \"train_accuracy_dloralc\": train_accuarcy,\n",
    "    #     \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "    #     \"train_loss_lc\": train_loss,\n",
    "    #     \"valid_loss_lc\": valid_loss,\n",
    "    #     \"train_accuracy_lc\": train_accuarcy,\n",
    "    #     \"valid_accuracy_lc\": valid_accuracy, \n",
    "    #     \"train_loss\": train_loss,\n",
    "    #     \"valid_loss\": valid_loss,\n",
    "    #     \"train_accuracy\": train_accuarcy,\n",
    "    #     \"valid_accuracy\": valid_accuracy,\n",
    "    #     \"valid_loss_dloralc_restored\": valid_loss,\n",
    "    #     \"valid_accuracy_dloralc_restored\": valid_accuracy,\n",
    "    #     \"epoch\": epoch,\n",
    "    # })\n",
    "\n",
    "print(\"End of model training on dataloader1...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working on training with delta-LoRA and LC-checkpoint on dataloader2** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DECOMPOSED_LAYERS = [\"classifier.1.weight\", \"classifier.3.weight\"]\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "BRANCH_ACC = \"0.8018\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = LeNet().to(device)\n",
    "model_original = LeNet().to(device)\n",
    "model_no_touch = LeNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_no_touch.load_state_dict(torch.load(BRANCH_LOC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = getBase(original)\n",
    "model = LeNet_LowRank(w, b, rank = RANK).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_sd_decomp(torch.load(BRANCH_LOC, map_location=device), model, DECOMPOSED_LAYERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_dloralc)\n",
    "optimizer_lc_only = torch.optim.SGD(model_original.parameters(), lr=learning_rate)\n",
    "optimizer_no_touch = torch.optim.SGD(model_no_touch.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "\n",
    "# Initialize the current iteration and set to 0\n",
    "current_iter = 0\n",
    "current_set = 0\n",
    "\n",
    "# Initialize the current iteration and set to 0 for the old LC method\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0\n",
    "\n",
    "\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Without Restored Model DeltaLoRA + LC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Start of model training on dataloader2...\n",
      "Epoch: [0/29], Training Loss DLoRALC: 0.781994, Validation Loss DLoRALC: 0.705412, Training Accuracy DLoRALC: 0.802240, Validation Accuracy DLoRALC: 0.817800 \n",
      "           \t Training Loss LC: 0.735144, Validation Loss LC: 0.620002, Training Accuracy LC: 0.817309, Validation Accuracy LC: 0.845100, \n",
      "           \t Training Loss: 0.735144, Validation Loss: 0.620002, Training Accuracy: 0.817309, Validation Accuracy: 0.845100\n",
      "Epoch: [1/29], Training Loss DLoRALC: 0.693374, Validation Loss DLoRALC: 0.633056, Training Accuracy DLoRALC: 0.817842, Validation Accuracy DLoRALC: 0.836100 \n",
      "           \t Training Loss LC: 0.583925, Validation Loss LC: 0.503890, Training Accuracy LC: 0.849780, Validation Accuracy LC: 0.874000, \n",
      "           \t Training Loss: 0.583925, Validation Loss: 0.503890, Training Accuracy: 0.849780, Validation Accuracy: 0.874000\n",
      "Epoch: [2/29], Training Loss DLoRALC: 0.631597, Validation Loss DLoRALC: 0.579355, Training Accuracy DLoRALC: 0.834911, Validation Accuracy DLoRALC: 0.848800 \n",
      "           \t Training Loss LC: 0.490882, Validation Loss LC: 0.432018, Training Accuracy LC: 0.872650, Validation Accuracy LC: 0.887500, \n",
      "           \t Training Loss: 0.490883, Validation Loss: 0.432018, Training Accuracy: 0.872650, Validation Accuracy: 0.887500\n",
      "Epoch: [3/29], Training Loss DLoRALC: 0.583913, Validation Loss DLoRALC: 0.536553, Training Accuracy DLoRALC: 0.845513, Validation Accuracy DLoRALC: 0.860100 \n",
      "           \t Training Loss LC: 0.430559, Validation Loss LC: 0.384491, Training Accuracy LC: 0.886585, Validation Accuracy LC: 0.896300, \n",
      "           \t Training Loss: 0.430559, Validation Loss: 0.384491, Training Accuracy: 0.886585, Validation Accuracy: 0.896300\n",
      "Epoch: [4/29], Training Loss DLoRALC: 0.544663, Validation Loss DLoRALC: 0.500337, Training Accuracy DLoRALC: 0.855247, Validation Accuracy DLoRALC: 0.870100 \n",
      "           \t Training Loss LC: 0.388896, Validation Loss LC: 0.350385, Training Accuracy LC: 0.896453, Validation Accuracy LC: 0.903600, \n",
      "           \t Training Loss: 0.388896, Validation Loss: 0.350385, Training Accuracy: 0.896453, Validation Accuracy: 0.903600\n",
      "Epoch: [5/29], Training Loss DLoRALC: 0.511262, Validation Loss DLoRALC: 0.470127, Training Accuracy DLoRALC: 0.864115, Validation Accuracy DLoRALC: 0.877400 \n",
      "           \t Training Loss LC: 0.358097, Validation Loss LC: 0.328566, Training Accuracy LC: 0.900853, Validation Accuracy LC: 0.908800, \n",
      "           \t Training Loss: 0.358097, Validation Loss: 0.328566, Training Accuracy: 0.900853, Validation Accuracy: 0.908800\n",
      "Epoch: [6/29], Training Loss DLoRALC: 0.482275, Validation Loss DLoRALC: 0.442843, Training Accuracy DLoRALC: 0.871316, Validation Accuracy DLoRALC: 0.882900 \n",
      "           \t Training Loss LC: 0.333964, Validation Loss LC: 0.305533, Training Accuracy LC: 0.907588, Validation Accuracy LC: 0.913000, \n",
      "           \t Training Loss: 0.333964, Validation Loss: 0.305533, Training Accuracy: 0.907588, Validation Accuracy: 0.913000\n",
      "Epoch: [7/29], Training Loss DLoRALC: 0.456293, Validation Loss DLoRALC: 0.418890, Training Accuracy DLoRALC: 0.879651, Validation Accuracy DLoRALC: 0.890000 \n",
      "           \t Training Loss LC: 0.313192, Validation Loss LC: 0.287781, Training Accuracy LC: 0.912255, Validation Accuracy LC: 0.918300, \n",
      "           \t Training Loss: 0.313192, Validation Loss: 0.287781, Training Accuracy: 0.912255, Validation Accuracy: 0.918300\n",
      "Epoch: [8/29], Training Loss DLoRALC: 0.433337, Validation Loss DLoRALC: 0.397952, Training Accuracy DLoRALC: 0.885251, Validation Accuracy DLoRALC: 0.893600 \n",
      "           \t Training Loss LC: 0.295636, Validation Loss LC: 0.272915, Training Accuracy LC: 0.916122, Validation Accuracy LC: 0.922100, \n",
      "           \t Training Loss: 0.295636, Validation Loss: 0.272915, Training Accuracy: 0.916122, Validation Accuracy: 0.922100\n",
      "Epoch: [9/29], Training Loss DLoRALC: 0.412613, Validation Loss DLoRALC: 0.379188, Training Accuracy DLoRALC: 0.889585, Validation Accuracy DLoRALC: 0.897500 \n",
      "           \t Training Loss LC: 0.279473, Validation Loss LC: 0.258978, Training Accuracy LC: 0.919856, Validation Accuracy LC: 0.925000, \n",
      "           \t Training Loss: 0.279473, Validation Loss: 0.258977, Training Accuracy: 0.919856, Validation Accuracy: 0.925000\n",
      "Epoch: [10/29], Training Loss DLoRALC: 0.394094, Validation Loss DLoRALC: 0.361664, Training Accuracy DLoRALC: 0.894119, Validation Accuracy DLoRALC: 0.901400 \n",
      "           \t Training Loss LC: 0.264981, Validation Loss LC: 0.244562, Training Accuracy LC: 0.924190, Validation Accuracy LC: 0.929200, \n",
      "           \t Training Loss: 0.264981, Validation Loss: 0.244562, Training Accuracy: 0.924190, Validation Accuracy: 0.929200\n",
      "Epoch: [11/29], Training Loss DLoRALC: 0.377253, Validation Loss DLoRALC: 0.347299, Training Accuracy DLoRALC: 0.897120, Validation Accuracy DLoRALC: 0.905200 \n",
      "           \t Training Loss LC: 0.251122, Validation Loss LC: 0.234670, Training Accuracy LC: 0.928657, Validation Accuracy LC: 0.933100, \n",
      "           \t Training Loss: 0.251122, Validation Loss: 0.234670, Training Accuracy: 0.928657, Validation Accuracy: 0.933100\n",
      "Epoch: [12/29], Training Loss DLoRALC: 0.362364, Validation Loss DLoRALC: 0.333609, Training Accuracy DLoRALC: 0.900720, Validation Accuracy DLoRALC: 0.907900 \n",
      "           \t Training Loss LC: 0.239008, Validation Loss LC: 0.222211, Training Accuracy LC: 0.931858, Validation Accuracy LC: 0.937400, \n",
      "           \t Training Loss: 0.239008, Validation Loss: 0.222211, Training Accuracy: 0.931858, Validation Accuracy: 0.937400\n",
      "Epoch: [13/29], Training Loss DLoRALC: 0.348553, Validation Loss DLoRALC: 0.321967, Training Accuracy DLoRALC: 0.904587, Validation Accuracy DLoRALC: 0.909500 \n",
      "           \t Training Loss LC: 0.226901, Validation Loss LC: 0.213615, Training Accuracy LC: 0.934325, Validation Accuracy LC: 0.937900, \n",
      "           \t Training Loss: 0.226901, Validation Loss: 0.213615, Training Accuracy: 0.934325, Validation Accuracy: 0.937900\n",
      "Epoch: [14/29], Training Loss DLoRALC: 0.336127, Validation Loss DLoRALC: 0.311046, Training Accuracy DLoRALC: 0.906721, Validation Accuracy DLoRALC: 0.912300 \n",
      "           \t Training Loss LC: 0.216182, Validation Loss LC: 0.204473, Training Accuracy LC: 0.936658, Validation Accuracy LC: 0.941900, \n",
      "           \t Training Loss: 0.216182, Validation Loss: 0.204472, Training Accuracy: 0.936658, Validation Accuracy: 0.941900\n",
      "Epoch: [15/29], Training Loss DLoRALC: 0.324581, Validation Loss DLoRALC: 0.300459, Training Accuracy DLoRALC: 0.909255, Validation Accuracy DLoRALC: 0.915100 \n",
      "           \t Training Loss LC: 0.206054, Validation Loss LC: 0.194231, Training Accuracy LC: 0.940259, Validation Accuracy LC: 0.943000, \n",
      "           \t Training Loss: 0.206054, Validation Loss: 0.194231, Training Accuracy: 0.940259, Validation Accuracy: 0.943000\n",
      "Epoch: [16/29], Training Loss DLoRALC: 0.313572, Validation Loss DLoRALC: 0.291176, Training Accuracy DLoRALC: 0.911922, Validation Accuracy DLoRALC: 0.917300 \n",
      "           \t Training Loss LC: 0.196532, Validation Loss LC: 0.187164, Training Accuracy LC: 0.943192, Validation Accuracy LC: 0.944500, \n",
      "           \t Training Loss: 0.196532, Validation Loss: 0.187164, Training Accuracy: 0.943192, Validation Accuracy: 0.944500\n",
      "Epoch: [17/29], Training Loss DLoRALC: 0.303070, Validation Loss DLoRALC: 0.282481, Training Accuracy DLoRALC: 0.915255, Validation Accuracy DLoRALC: 0.920000 \n",
      "           \t Training Loss LC: 0.187655, Validation Loss LC: 0.178796, Training Accuracy LC: 0.945659, Validation Accuracy LC: 0.947700, \n",
      "           \t Training Loss: 0.187655, Validation Loss: 0.178796, Training Accuracy: 0.945659, Validation Accuracy: 0.947700\n",
      "Epoch: [18/29], Training Loss DLoRALC: 0.292976, Validation Loss DLoRALC: 0.273089, Training Accuracy DLoRALC: 0.918189, Validation Accuracy DLoRALC: 0.922000 \n",
      "           \t Training Loss LC: 0.179375, Validation Loss LC: 0.171054, Training Accuracy LC: 0.948660, Validation Accuracy LC: 0.949500, \n",
      "           \t Training Loss: 0.179375, Validation Loss: 0.171054, Training Accuracy: 0.948660, Validation Accuracy: 0.949500\n",
      "Epoch: [19/29], Training Loss DLoRALC: 0.283448, Validation Loss DLoRALC: 0.264946, Training Accuracy DLoRALC: 0.919989, Validation Accuracy DLoRALC: 0.924600 \n",
      "           \t Training Loss LC: 0.171747, Validation Loss LC: 0.165642, Training Accuracy LC: 0.951193, Validation Accuracy LC: 0.951900, \n",
      "           \t Training Loss: 0.171747, Validation Loss: 0.165642, Training Accuracy: 0.951193, Validation Accuracy: 0.951900\n",
      "Epoch: [20/29], Training Loss DLoRALC: 0.273877, Validation Loss DLoRALC: 0.256068, Training Accuracy DLoRALC: 0.923656, Validation Accuracy DLoRALC: 0.926500 \n",
      "           \t Training Loss LC: 0.164604, Validation Loss LC: 0.157472, Training Accuracy LC: 0.953594, Validation Accuracy LC: 0.954300, \n",
      "           \t Training Loss: 0.164604, Validation Loss: 0.157472, Training Accuracy: 0.953594, Validation Accuracy: 0.954300\n",
      "Epoch: [21/29], Training Loss DLoRALC: 0.264528, Validation Loss DLoRALC: 0.248060, Training Accuracy DLoRALC: 0.926190, Validation Accuracy DLoRALC: 0.928300 \n",
      "           \t Training Loss LC: 0.157757, Validation Loss LC: 0.152392, Training Accuracy LC: 0.954727, Validation Accuracy LC: 0.954800, \n",
      "           \t Training Loss: 0.157756, Validation Loss: 0.152392, Training Accuracy: 0.954727, Validation Accuracy: 0.954800\n",
      "Epoch: [22/29], Training Loss DLoRALC: 0.255449, Validation Loss DLoRALC: 0.239982, Training Accuracy DLoRALC: 0.928257, Validation Accuracy DLoRALC: 0.930300 \n",
      "           \t Training Loss LC: 0.151378, Validation Loss LC: 0.147081, Training Accuracy LC: 0.957194, Validation Accuracy LC: 0.955900, \n",
      "           \t Training Loss: 0.151378, Validation Loss: 0.147081, Training Accuracy: 0.957194, Validation Accuracy: 0.955900\n",
      "Epoch: [23/29], Training Loss DLoRALC: 0.246419, Validation Loss DLoRALC: 0.233010, Training Accuracy DLoRALC: 0.929924, Validation Accuracy DLoRALC: 0.933500 \n",
      "           \t Training Loss LC: 0.145316, Validation Loss LC: 0.142483, Training Accuracy LC: 0.958061, Validation Accuracy LC: 0.957800, \n",
      "           \t Training Loss: 0.145316, Validation Loss: 0.142483, Training Accuracy: 0.958061, Validation Accuracy: 0.957800\n",
      "Epoch: [24/29], Training Loss DLoRALC: 0.237715, Validation Loss DLoRALC: 0.223979, Training Accuracy DLoRALC: 0.931924, Validation Accuracy DLoRALC: 0.935500 \n",
      "           \t Training Loss LC: 0.139868, Validation Loss LC: 0.135902, Training Accuracy LC: 0.960995, Validation Accuracy LC: 0.959900, \n",
      "           \t Training Loss: 0.139868, Validation Loss: 0.135902, Training Accuracy: 0.960995, Validation Accuracy: 0.959900\n",
      "Epoch: [25/29], Training Loss DLoRALC: 0.229191, Validation Loss DLoRALC: 0.216981, Training Accuracy DLoRALC: 0.933991, Validation Accuracy DLoRALC: 0.937400 \n",
      "           \t Training Loss LC: 0.134560, Validation Loss LC: 0.132844, Training Accuracy LC: 0.960728, Validation Accuracy LC: 0.960800, \n",
      "           \t Training Loss: 0.134560, Validation Loss: 0.132845, Training Accuracy: 0.960728, Validation Accuracy: 0.960800\n",
      "Epoch: [26/29], Training Loss DLoRALC: 0.220957, Validation Loss DLoRALC: 0.209435, Training Accuracy DLoRALC: 0.936325, Validation Accuracy DLoRALC: 0.939700 \n",
      "           \t Training Loss LC: 0.129640, Validation Loss LC: 0.128285, Training Accuracy LC: 0.962728, Validation Accuracy LC: 0.962600, \n",
      "           \t Training Loss: 0.129639, Validation Loss: 0.128284, Training Accuracy: 0.962728, Validation Accuracy: 0.962600\n",
      "Epoch: [27/29], Training Loss DLoRALC: 0.212890, Validation Loss DLoRALC: 0.202357, Training Accuracy DLoRALC: 0.938192, Validation Accuracy DLoRALC: 0.942200 \n",
      "           \t Training Loss LC: 0.124935, Validation Loss LC: 0.124109, Training Accuracy LC: 0.964862, Validation Accuracy LC: 0.963400, \n",
      "           \t Training Loss: 0.124935, Validation Loss: 0.124109, Training Accuracy: 0.964862, Validation Accuracy: 0.963400\n",
      "Epoch: [28/29], Training Loss DLoRALC: 0.205261, Validation Loss DLoRALC: 0.195551, Training Accuracy DLoRALC: 0.939992, Validation Accuracy DLoRALC: 0.943900 \n",
      "           \t Training Loss LC: 0.120649, Validation Loss LC: 0.120300, Training Accuracy LC: 0.965729, Validation Accuracy LC: 0.964900, \n",
      "           \t Training Loss: 0.120649, Validation Loss: 0.120300, Training Accuracy: 0.965729, Validation Accuracy: 0.964900\n",
      "Epoch: [29/29], Training Loss DLoRALC: 0.198017, Validation Loss DLoRALC: 0.188727, Training Accuracy DLoRALC: 0.943059, Validation Accuracy DLoRALC: 0.946700 \n",
      "           \t Training Loss LC: 0.116817, Validation Loss LC: 0.117586, Training Accuracy LC: 0.967462, Validation Accuracy LC: 0.966100, \n",
      "           \t Training Loss: 0.116817, Validation Loss: 0.117586, Training Accuracy: 0.967462, Validation Accuracy: 0.966100\n",
      "End of model training on dataloader2...\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------\")\n",
    "\n",
    "wandb.config[\"branch_accuracy\"] = BRANCH_ACC\n",
    "wandb.config[\"train dataset 2\"] = \"MNIST train dataloader2\"\n",
    "\n",
    "# Training code on dataloader2\n",
    "print(\"Start of model training on dataloader2...\")\n",
    "\n",
    "isLoop = True\n",
    "\n",
    "valid_accuracy_list = []\n",
    "\n",
    "base = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        train_loss_lc = 0.0\n",
    "        train_loss_no_touch = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_loss_lc = 0.0\n",
    "        valid_loss_no_touch = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_correct_lc = 0\n",
    "        train_correct_no_touch = 0\n",
    "        train_total = 0\n",
    "        train_total_lc = 0\n",
    "        train_total_no_touch = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_correct_lc = 0\n",
    "        valid_correct_no_touch = 0\n",
    "        valid_total = 0\n",
    "        valid_total_lc = 0\n",
    "        valid_total_no_touch = 0\n",
    "        \n",
    "        model.train()\n",
    "        model_original.train()\n",
    "        model_no_touch.train()\n",
    "\n",
    "        for iter, data in enumerate(train_loader2):\n",
    "\n",
    "            # Check if it is the first iteration of the first epoch\n",
    "            if iter == 0: # first iteration, create baseline model\n",
    "            # if iter == 0 and epoch == 0: # first iteration, create baseline model\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "                \n",
    "                base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "                                                        \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                cstate = model_original.state_dict()\n",
    "                set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                    os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path,DECOMPOSED_LAYERS)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC \n",
    "                ########################################################\n",
    "\n",
    "                # Delta-compression: The delta for the weights of the normal and decomposed layers.\n",
    "                # Also returns the full dictionary, which holds the bias.\n",
    "\n",
    "                #Calculate the time before generate_delta_gpu function\n",
    "                delta, decomp_delta, bias = lc.generate_delta_gpu(base, \n",
    "                                                                base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                # Compressing the delta and decomposed delta\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                # Saving checkpoint\n",
    "                # lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                #                 \"/set_{}\".format(current_set))\n",
    "            \n",
    "                # Update base and base_decomp\n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                # Update current iteration\n",
    "                current_iter += 1\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                cstate = model_original.state_dict()\n",
    "                old_lc_delta, old_lc_bias = olc.generate_delta_gpu(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "                olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                # olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                #                     old_lc_bias, current_iter_old_lc)\n",
    "                prev_state = np.add(prev_state, update_prev)\n",
    "                current_iter_old_lc += 1\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            ########################################################\n",
    "            ### DELTA-LORA + LC\n",
    "            ########################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "            ########################################################\n",
    "            ### LC\n",
    "            ########################################################\n",
    "\n",
    "            optimizer_lc_only.zero_grad()\n",
    "            output_lc = model_original(inputs)\n",
    "\n",
    "            loss_lc = torch.nn.CrossEntropyLoss()(output_lc, labels)\n",
    "            loss_lc.backward()\n",
    "            optimizer_lc_only.step()\n",
    "            train_loss_lc += loss_lc.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_lc = torch.max(output_lc, 1)\n",
    "            train_correct_lc += (predicted_lc == labels).sum().item()\n",
    "            train_total_lc += labels.size(0)\n",
    "\n",
    "            train_acc_lc = torch.eq(output_lc.argmax(-1), labels).float().mean()\n",
    "\n",
    "            ########################################################\n",
    "            ### NO TOUCH\n",
    "            ########################################################\n",
    "\n",
    "            optimizer_no_touch.zero_grad()\n",
    "            output_no_touch = model_no_touch(inputs)\n",
    "\n",
    "            loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, labels)\n",
    "            loss_no_touch.backward()\n",
    "            optimizer_no_touch.step()\n",
    "            train_loss_no_touch += loss_no_touch.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "            train_correct_no_touch += (predicted_no_touch == labels).sum().item()\n",
    "            train_total_no_touch += labels.size(0)\n",
    "\n",
    "            train_acc_no_touch = torch.eq(output_no_touch.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model.eval()\n",
    "        model_original.eval()\n",
    "        model_no_touch.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "\n",
    "                output = model(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                output_lc = model_original(data)\n",
    "                loss_lc = torch.nn.CrossEntropyLoss()(output_lc, target)\n",
    "                valid_loss_lc += loss_lc.item() * data.size(0)\n",
    "\n",
    "                _, predicted_lc = torch.max(output_lc, 1)\n",
    "                valid_correct_lc += (predicted_lc == target).sum().item()\n",
    "                valid_total_lc += target.size(0)\n",
    "\n",
    "                valid_acc_lc = torch.eq(output_lc.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### NO TOUCH\n",
    "                ########################################################\n",
    "\n",
    "                output_no_touch = model_no_touch(data)\n",
    "                loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, target)\n",
    "                valid_loss_no_touch += loss_no_touch.item() * data.size(0)\n",
    "\n",
    "                _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "                valid_correct_no_touch += (predicted_no_touch == target).sum().item()\n",
    "                valid_total_no_touch += target.size(0)\n",
    "\n",
    "                valid_acc_no_touch = torch.eq(output_no_touch.argmax(-1), target).float().mean()\n",
    "\n",
    "                \n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader2.dataset)\n",
    "    train_loss_lc /= len(train_loader2.dataset)\n",
    "    train_loss_no_touch /= len(train_loader2.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "    valid_loss_lc /= len(test_loader.dataset)\n",
    "    valid_loss_no_touch /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    train_accuarcy_lc = train_correct_lc / train_total_lc\n",
    "    train_accuarcy_no_touch = train_correct_no_touch / train_total_no_touch\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "    valid_accuracy_lc = valid_correct_lc / valid_total_lc\n",
    "    valid_accuracy_no_touch = valid_correct_no_touch / valid_total_no_touch\n",
    "\n",
    "    valid_accuracy_list.append(valid_accuracy)\n",
    "    \n",
    "    print(\"Epoch: [{}/{}], Training Loss DLoRALC: {:.6f}, Validation Loss DLoRALC: {:.6f}, Training Accuracy DLoRALC: {:.6f}, Validation Accuracy DLoRALC: {:.6f} \\n \\\n",
    "          \\t Training Loss LC: {:.6f}, Validation Loss LC: {:.6f}, Training Accuracy LC: {:.6f}, Validation Accuracy LC: {:.6f}, \\n \\\n",
    "          \\t Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, \n",
    "        train_loss, valid_loss, train_accuarcy, valid_accuracy, \n",
    "        train_loss_lc, valid_loss_lc, train_accuarcy_lc, valid_accuracy_lc, \n",
    "        train_loss_no_touch, valid_loss_no_touch, train_accuarcy_no_touch, valid_accuracy_no_touch)) \n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss_dloralc\": train_loss,\n",
    "        \"valid_loss_dloralc\": valid_loss,\n",
    "        \"train_accuracy_dloralc\": train_accuarcy,\n",
    "        \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "        \"train_loss_lc\": train_loss_lc,\n",
    "        \"valid_loss_lc\": valid_loss_lc,\n",
    "        \"train_accuracy_lc\": train_accuarcy_lc,\n",
    "        \"valid_accuracy_lc\": valid_accuracy_lc,\n",
    "        \"train_loss\": train_loss_no_touch,\n",
    "        \"valid_loss\": valid_loss_no_touch,\n",
    "        \"train_accuracy\": train_accuarcy_no_touch,\n",
    "        \"valid_accuracy\": valid_accuracy_no_touch,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader2...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **With Restored Model DeltaLoRA + LC**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Without Super Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print(\"-----------------------------------\")\n",
    "\n",
    "# wandb.config[\"branch_accuracy\"] = BRANCH_ACC\n",
    "\n",
    "# # Training code on dataloader2\n",
    "# print(\"Start of model training on dataloader2...\")\n",
    "\n",
    "# isLoop = True\n",
    "\n",
    "# valid_accuracy_list = []\n",
    "\n",
    "# base = None\n",
    "\n",
    "# for epoch in range(NUM_EPOCHES):\n",
    "#     if not isLoop:\n",
    "#         break\n",
    "#     else:\n",
    "#         train_loss = 0.0\n",
    "#         train_loss_lc = 0.0\n",
    "#         train_loss_no_touch = 0.0\n",
    "#         valid_loss = 0.0\n",
    "#         valid_loss_lc = 0.0\n",
    "#         valid_loss_no_touch = 0.0\n",
    "#         valid_loss_dlora_lc_restored = 0.0\n",
    "\n",
    "#         train_correct = 0\n",
    "#         train_correct_lc = 0\n",
    "#         train_correct_no_touch = 0\n",
    "#         train_total = 0\n",
    "#         train_total_lc = 0\n",
    "#         train_total_no_touch = 0\n",
    "\n",
    "#         valid_correct = 0\n",
    "#         valid_correct_lc = 0\n",
    "#         valid_correct_no_touch = 0\n",
    "#         valid_correct_dlora_lc_restored = 0\n",
    "#         valid_total = 0\n",
    "#         valid_total_lc = 0\n",
    "#         valid_total_no_touch = 0\n",
    "#         valid_total_dlora_lc_restored = 0\n",
    "        \n",
    "#         model.train()\n",
    "#         model_original.train()\n",
    "#         model_no_touch.train()\n",
    "\n",
    "#         for iter, data in enumerate(train_loader2):\n",
    "\n",
    "#             # Check if it is the first iteration of the first epoch\n",
    "#             if iter == 0 and epoch == 0: # first iteration, create baseline model\n",
    "#                 ########################################################\n",
    "#                 ### DELTA-LORA + LC\n",
    "#                 ########################################################\n",
    "                \n",
    "#                 base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "#                                                         \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "#                 ########################################################\n",
    "#                 ### LC\n",
    "#                 ########################################################\n",
    "\n",
    "#                 cstate = model_original.state_dict()\n",
    "#                 set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "#                 if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "#                     os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "#                 prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path,DECOMPOSED_LAYERS)\n",
    "            \n",
    "#             else:\n",
    "#                 ########################################################\n",
    "#                 ### DELTA-LORA + LC \n",
    "#                 ########################################################\n",
    "\n",
    "#                 # Delta-compression: The delta for the weights of the normal and decomposed layers.\n",
    "#                 # Also returns the full dictionary, which holds the bias.\n",
    "\n",
    "#                 #Calculate the time before generate_delta_gpu function\n",
    "#                 delta, decomp_delta, bias = lc.generate_delta_gpu(base, \n",
    "#                                                                 base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "#                 # Compressing the delta and decomposed delta\n",
    "#                 compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "#                                                                                                             decomp_delta)\n",
    "#                 # Saving checkpoint\n",
    "#                 lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "#                                 \"/set_{}\".format(current_set))\n",
    "            \n",
    "#                 # Update base and base_decomp\n",
    "#                 base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "#                 base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "#                 # Update current iteration\n",
    "#                 current_iter += 1\n",
    "\n",
    "#                 ########################################################\n",
    "#                 ### LC\n",
    "#                 ########################################################\n",
    "\n",
    "#                 cstate = model_original.state_dict()\n",
    "#                 old_lc_delta, old_lc_bias = olc.generate_delta_gpu(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "#                 olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "#                 olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "#                                     old_lc_bias, current_iter_old_lc)\n",
    "#                 prev_state = np.add(prev_state, update_prev)\n",
    "#                 current_iter_old_lc += 1\n",
    "\n",
    "\n",
    "\n",
    "#             inputs, labels = data\n",
    "#             inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#             ########################################################\n",
    "#             ### DELTA-LORA + LC\n",
    "#             ########################################################\n",
    "\n",
    "#             optimizer.zero_grad()\n",
    "#             output = model(inputs)\n",
    "\n",
    "#             loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "#             _, predicted = torch.max(output, 1)\n",
    "#             train_correct += (predicted == labels).sum().item()\n",
    "#             train_total += labels.size(0)\n",
    "\n",
    "#             train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "#             ########################################################\n",
    "#             ### LC\n",
    "#             ########################################################\n",
    "\n",
    "#             optimizer_lc_only.zero_grad()\n",
    "#             output_lc = model_original(inputs)\n",
    "\n",
    "#             loss_lc = torch.nn.CrossEntropyLoss()(output_lc, labels)\n",
    "#             loss_lc.backward()\n",
    "#             optimizer_lc_only.step()\n",
    "#             train_loss_lc += loss_lc.item() * inputs.size(0)\n",
    "\n",
    "#             _, predicted_lc = torch.max(output_lc, 1)\n",
    "#             train_correct_lc += (predicted_lc == labels).sum().item()\n",
    "#             train_total_lc += labels.size(0)\n",
    "\n",
    "#             train_acc_lc = torch.eq(output_lc.argmax(-1), labels).float().mean()\n",
    "\n",
    "#             ########################################################\n",
    "#             ### NO TOUCH\n",
    "#             ########################################################\n",
    "\n",
    "#             optimizer_no_touch.zero_grad()\n",
    "#             output_no_touch = model_no_touch(inputs)\n",
    "\n",
    "#             loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, labels)\n",
    "#             loss_no_touch.backward()\n",
    "#             optimizer_no_touch.step()\n",
    "#             train_loss_no_touch += loss_no_touch.item() * inputs.size(0)\n",
    "\n",
    "#             _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "#             train_correct_no_touch += (predicted_no_touch == labels).sum().item()\n",
    "#             train_total_no_touch += labels.size(0)\n",
    "\n",
    "#             train_acc_no_touch = torch.eq(output_no_touch.argmax(-1), labels).float().mean()\n",
    "\n",
    "#         model.eval()\n",
    "#         model_original.eval()\n",
    "#         model_no_touch.eval()\n",
    "#         with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "#             for data, target in test_loader:\n",
    "#                 # Move data and target to the correct device\n",
    "#                 data, target = data.to(device), target.to(device)\n",
    "\n",
    "#                 ########################################################\n",
    "#                 ### DELTA-LORA + LC\n",
    "#                 ########################################################\n",
    "\n",
    "#                 output = model(data)\n",
    "#                 loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "#                 valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "#                 _, predicted = torch.max(output, 1)\n",
    "#                 valid_correct += (predicted == target).sum().item()\n",
    "#                 valid_total += target.size(0)\n",
    "\n",
    "#                 valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "#                 ########################################################\n",
    "#                 ### LC\n",
    "#                 ########################################################\n",
    "\n",
    "#                 output_lc = model_original(data)\n",
    "#                 loss_lc = torch.nn.CrossEntropyLoss()(output_lc, target)\n",
    "#                 valid_loss_lc += loss_lc.item() * data.size(0)\n",
    "\n",
    "#                 _, predicted_lc = torch.max(output_lc, 1)\n",
    "#                 valid_correct_lc += (predicted_lc == target).sum().item()\n",
    "#                 valid_total_lc += target.size(0)\n",
    "\n",
    "#                 valid_acc_lc = torch.eq(output_lc.argmax(-1), target).float().mean()\n",
    "\n",
    "#                 ########################################################\n",
    "#                 ### NO TOUCH\n",
    "#                 ########################################################\n",
    "\n",
    "#                 output_no_touch = model_no_touch(data)\n",
    "#                 loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, target)\n",
    "#                 valid_loss_no_touch += loss_no_touch.item() * data.size(0)\n",
    "\n",
    "#                 _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "#                 valid_correct_no_touch += (predicted_no_touch == target).sum().item()\n",
    "#                 valid_total_no_touch += target.size(0)\n",
    "\n",
    "#                 valid_acc_no_touch = torch.eq(output_no_touch.argmax(-1), target).float().mean()\n",
    "\n",
    "#                 ########################################################\n",
    "#                 ### DELTA-LORA + LC (restored)\n",
    "#                 ########################################################\n",
    "#                 folder = SAVE_LOC + \"/set_{}\".format(current_set)\n",
    "#                 # print(folder)\n",
    "#                 num_files = len([f for f in os.listdir(folder)if os.path.isfile(os.path.join(folder, f))])\n",
    "#                 # print(num_files)\n",
    "#                 restored_model = lc.restore_checkpoint(original, LeNet(), SAVE_LOC, 0, num_files-3 , DECOMPOSED_LAYERS)\n",
    "\n",
    "#                 # restored_model = lazy_restore_gpu(base, base_decomp, bias, LeNet(), \n",
    "#                 #                             original.state_dict(), DECOMPOSED_LAYERS, \n",
    "#                 #                             rank = RANK, scaling = SCALING)\n",
    "#                 restored_model = restored_model.to(device)\n",
    "                \n",
    "#                 output_dlora_lc_restored = restored_model(data)\n",
    "#                 loss_dlora_lc_restored = torch.nn.CrossEntropyLoss()(output_dlora_lc_restored, target)\n",
    "#                 valid_loss_dlora_lc_restored += loss_dlora_lc_restored.item() * data.size(0)\n",
    "\n",
    "#                 _, predicted_dlora_lc_restored = torch.max(output_dlora_lc_restored, 1)\n",
    "#                 valid_correct_dlora_lc_restored += (predicted_dlora_lc_restored == target).sum().item()\n",
    "#                 valid_total_dlora_lc_restored += target.size(0)\n",
    "\n",
    "#                 valid_acc_dlora_lc_restored = torch.eq(output_dlora_lc_restored.argmax(-1), target).float().mean()\n",
    "                \n",
    "#     # Calculate average losses\n",
    "#     train_loss /= len(train_loader2.dataset)\n",
    "#     train_loss_lc /= len(train_loader2.dataset)\n",
    "#     train_loss_no_touch /= len(train_loader2.dataset)\n",
    "#     valid_loss /= len(test_loader.dataset)\n",
    "#     valid_loss_lc /= len(test_loader.dataset)\n",
    "#     valid_loss_no_touch /= len(test_loader.dataset)\n",
    "#     valid_loss_dlora_lc_restored /= len(test_loader.dataset)\n",
    "\n",
    "#     train_accuarcy = train_correct / train_total\n",
    "#     train_accuarcy_lc = train_correct_lc / train_total_lc\n",
    "#     train_accuarcy_no_touch = train_correct_no_touch / train_total_no_touch\n",
    "#     valid_accuracy = valid_correct / valid_total\n",
    "#     valid_accuracy_lc = valid_correct_lc / valid_total_lc\n",
    "#     valid_accuracy_no_touch = valid_correct_no_touch / valid_total_no_touch\n",
    "#     valid_accuracy_dlora_lc_restored = valid_correct_dlora_lc_restored / valid_total_dlora_lc_restored\n",
    "\n",
    "#     valid_accuracy_list.append(valid_accuracy)\n",
    "    \n",
    "#     print(\"Epoch: [{}/{}], Training Loss DLoRALC: {:.6f}, Validation Loss DLoRALC: {:.6f}, Training Accuracy DLoRALC: {:.6f}, Validation Accuracy DLoRALC: {:.6f} \\n \\\n",
    "#           \\t Training Loss LC: {:.6f}, Validation Loss LC: {:.6f}, Training Accuracy LC: {:.6f}, Validation Accuracy LC: {:.6f}, \\n \\\n",
    "#           \\t Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}, \\n \\t Validation Loss dLoRALC restored: {:.6f},  Validation Accuracy dLoRALC restored: {:.6f}\".format(epoch, NUM_EPOCHES-1, \n",
    "#         train_loss, valid_loss, train_accuarcy, valid_accuracy, \n",
    "#         train_loss_lc, valid_loss_lc, train_accuarcy_lc, valid_accuracy_lc, \n",
    "#         train_loss_no_touch, valid_loss_no_touch, train_accuarcy_no_touch, valid_accuracy_no_touch, \n",
    "#         valid_loss_dlora_lc_restored, valid_accuracy_dlora_lc_restored)) \n",
    "\n",
    "#     wandb.log({\n",
    "#         \"train_loss_dloralc\": train_loss,\n",
    "#         \"valid_loss_dloralc\": valid_loss,\n",
    "#         \"train_accuracy_dloralc\": train_accuarcy,\n",
    "#         \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "#         \"train_loss_lc\": train_loss_lc,\n",
    "#         \"valid_loss_lc\": valid_loss_lc,\n",
    "#         \"train_accuracy_lc\": train_accuarcy_lc,\n",
    "#         \"valid_accuracy_lc\": valid_accuracy_lc,\n",
    "#         \"train_loss\": train_loss_no_touch,\n",
    "#         \"valid_loss\": valid_loss_no_touch,\n",
    "#         \"train_accuracy\": train_accuarcy_no_touch,\n",
    "#         \"valid_accuracy\": valid_accuracy_no_touch,\n",
    "#         \"valid_loss_dloralc_restored\": valid_loss_dlora_lc_restored,\n",
    "#         \"valid_accuracy_dloralc_restored\": valid_accuracy_dlora_lc_restored,\n",
    "#         \"epoch\": epoch\n",
    "#     })\n",
    "\n",
    "\n",
    "# print(\"End of model training on dataloader2...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **With Super Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "wandb.config[\"branch_accuracy\"] = BRANCH_ACC\n",
    "\n",
    "# Training code on dataloader2\n",
    "print(\"Start of model training on dataloader2...\")\n",
    "\n",
    "isLoop = True\n",
    "\n",
    "valid_accuracy_list = []\n",
    "\n",
    "base = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        train_loss_lc = 0.0\n",
    "        train_loss_no_touch = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_loss_lc = 0.0\n",
    "        valid_loss_no_touch = 0.0\n",
    "        valid_loss_dlora_lc_restored = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_correct_lc = 0\n",
    "        train_correct_no_touch = 0\n",
    "        train_total = 0\n",
    "        train_total_lc = 0\n",
    "        train_total_no_touch = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_correct_lc = 0\n",
    "        valid_correct_no_touch = 0\n",
    "        valid_correct_dlora_lc_restored = 0\n",
    "        valid_total = 0\n",
    "        valid_total_lc = 0\n",
    "        valid_total_no_touch = 0\n",
    "        valid_total_dlora_lc_restored = 0\n",
    "        \n",
    "        model.train()\n",
    "        model_original.train()\n",
    "        model_no_touch.train()\n",
    "\n",
    "        for iter, data in enumerate(train_loader2):\n",
    "\n",
    "            # Check if it is the first iteration of the first epoch\n",
    "            if iter == 0 and epoch == 0: # first iteration, create baseline model\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "                \n",
    "                base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "                                                        \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                cstate = model_original.state_dict()\n",
    "                set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                    os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path,DECOMPOSED_LAYERS)\n",
    "            \n",
    "            else:\n",
    "                if iter % super_step == 0:\n",
    "                    ########################################################\n",
    "                    ### DELTA-LORA + LC \n",
    "                    ########################################################\n",
    "\n",
    "                    new_model = lazy_restore_gpu(base, base_decomp, bias, LeNet(), \n",
    "                                            original.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "                    # Changing previous \"original model\" used to restore the loRA model.\n",
    "                    original = new_model \n",
    "                    \n",
    "                    current_set += 1\n",
    "                    current_iter = 0\n",
    "\n",
    "                    # Create a new set directory if it does not exist\n",
    "                    set_path = \"/set_{}\".format(current_set)\n",
    "                    if not os.path.exists(SAVE_LOC + set_path):\n",
    "                        os.makedirs(SAVE_LOC + set_path)\n",
    "                    \n",
    "                    # Rebuilding LoRA layers => reset model!\n",
    "\n",
    "                    # Get the base model weights and biases\n",
    "                    w, b = getBase(original)\n",
    "                    # Create a new model with the base weights and specified rank\n",
    "                    model = LeNet_LowRank(w, b, rank = RANK).to(device)\n",
    "                    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate_dloralc)\n",
    "                    \n",
    "                    # Load state dictionary from full snapshot, including specified decomposed layers, and load it into the model\n",
    "                    load_sd_decomp(original.state_dict(), model, DECOMPOSED_LAYERS)\n",
    "                    # The base for all delta calculations\n",
    "                    base, base_decomp = lc.extract_weights_gpu(model, SAVE_LOC + \n",
    "                                                        \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "                    ########################################################\n",
    "                    ### LC\n",
    "                    ########################################################\n",
    "\n",
    "                    cstate = model_original.state_dict()\n",
    "                    current_set_old_lc += 1\n",
    "                    current_iter_old_lc = 0\n",
    "                    set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                    if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                        os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                    prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path, DECOMPOSED_LAYERS)\n",
    "\n",
    "                else:\n",
    "                    ########################################################\n",
    "                    ### DELTA-LORA + LC \n",
    "                    ########################################################\n",
    "\n",
    "                    # Delta-compression: The delta for the weights of the normal and decomposed layers.\n",
    "                    # Also returns the full dictionary, which holds the bias.\n",
    "\n",
    "                    #Calculate the time before generate_delta_gpu function\n",
    "                    delta, decomp_delta, bias = lc.generate_delta_gpu(base, \n",
    "                                                                    base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                    # Compressing the delta and decomposed delta\n",
    "                    compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                                decomp_delta)\n",
    "                    # Saving checkpoint\n",
    "                    lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                                    \"/set_{}\".format(current_set))\n",
    "                \n",
    "                    # Update base and base_decomp\n",
    "                    base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                    base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                    # Update current iteration\n",
    "                    current_iter += 1\n",
    "\n",
    "                    ########################################################\n",
    "                    ### LC\n",
    "                    ########################################################\n",
    "\n",
    "                    cstate = model_original.state_dict()\n",
    "                    old_lc_delta, old_lc_bias = olc.generate_delta_gpu(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "                    olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                    olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                                        old_lc_bias, current_iter_old_lc)\n",
    "                    prev_state = np.add(prev_state, update_prev)\n",
    "                    current_iter_old_lc += 1\n",
    "\n",
    "\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            ########################################################\n",
    "            ### DELTA-LORA + LC\n",
    "            ########################################################\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "            ########################################################\n",
    "            ### LC\n",
    "            ########################################################\n",
    "\n",
    "            optimizer_lc_only.zero_grad()\n",
    "            output_lc = model_original(inputs)\n",
    "\n",
    "            loss_lc = torch.nn.CrossEntropyLoss()(output_lc, labels)\n",
    "            loss_lc.backward()\n",
    "            optimizer_lc_only.step()\n",
    "            train_loss_lc += loss_lc.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_lc = torch.max(output_lc, 1)\n",
    "            train_correct_lc += (predicted_lc == labels).sum().item()\n",
    "            train_total_lc += labels.size(0)\n",
    "\n",
    "            train_acc_lc = torch.eq(output_lc.argmax(-1), labels).float().mean()\n",
    "\n",
    "            ########################################################\n",
    "            ### NO TOUCH\n",
    "            ########################################################\n",
    "\n",
    "            optimizer_no_touch.zero_grad()\n",
    "            output_no_touch = model_no_touch(inputs)\n",
    "\n",
    "            loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, labels)\n",
    "            loss_no_touch.backward()\n",
    "            optimizer_no_touch.step()\n",
    "            train_loss_no_touch += loss_no_touch.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "            train_correct_no_touch += (predicted_no_touch == labels).sum().item()\n",
    "            train_total_no_touch += labels.size(0)\n",
    "\n",
    "            train_acc_no_touch = torch.eq(output_no_touch.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model.eval()\n",
    "        model_original.eval()\n",
    "        model_no_touch.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "\n",
    "                output = model(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                output_lc = model_original(data)\n",
    "                loss_lc = torch.nn.CrossEntropyLoss()(output_lc, target)\n",
    "                valid_loss_lc += loss_lc.item() * data.size(0)\n",
    "\n",
    "                _, predicted_lc = torch.max(output_lc, 1)\n",
    "                valid_correct_lc += (predicted_lc == target).sum().item()\n",
    "                valid_total_lc += target.size(0)\n",
    "\n",
    "                valid_acc_lc = torch.eq(output_lc.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### NO TOUCH\n",
    "                ########################################################\n",
    "\n",
    "                output_no_touch = model_no_touch(data)\n",
    "                loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, target)\n",
    "                valid_loss_no_touch += loss_no_touch.item() * data.size(0)\n",
    "\n",
    "                _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "                valid_correct_no_touch += (predicted_no_touch == target).sum().item()\n",
    "                valid_total_no_touch += target.size(0)\n",
    "\n",
    "                valid_acc_no_touch = torch.eq(output_no_touch.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC (restored)\n",
    "                ########################################################\n",
    "\n",
    "                # restored_model = lc.restore_checkpoint(original, LeNet(), SAVE_LOC, current_set, current_iter, DECOMPOSED_LAYERS)\n",
    "\n",
    "                restored_model = lazy_restore_gpu(base, base_decomp, bias, LeNet(), \n",
    "                                            original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                            rank = RANK, scaling = SCALING)\n",
    "                restored_model = restored_model.to(device)\n",
    "                \n",
    "                output_dlora_lc_restored = restored_model(data)\n",
    "                loss_dlora_lc_restored = torch.nn.CrossEntropyLoss()(output_dlora_lc_restored, target)\n",
    "                valid_loss_dlora_lc_restored += loss_dlora_lc_restored.item() * data.size(0)\n",
    "\n",
    "                _, predicted_dlora_lc_restored = torch.max(output_dlora_lc_restored, 1)\n",
    "                valid_correct_dlora_lc_restored += (predicted_dlora_lc_restored == target).sum().item()\n",
    "                valid_total_dlora_lc_restored += target.size(0)\n",
    "\n",
    "                valid_acc_dlora_lc_restored = torch.eq(output_dlora_lc_restored.argmax(-1), target).float().mean()\n",
    "                \n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader2.dataset)\n",
    "    train_loss_lc /= len(train_loader2.dataset)\n",
    "    train_loss_no_touch /= len(train_loader2.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "    valid_loss_lc /= len(test_loader.dataset)\n",
    "    valid_loss_no_touch /= len(test_loader.dataset)\n",
    "    valid_loss_dlora_lc_restored /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    train_accuarcy_lc = train_correct_lc / train_total_lc\n",
    "    train_accuarcy_no_touch = train_correct_no_touch / train_total_no_touch\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "    valid_accuracy_lc = valid_correct_lc / valid_total_lc\n",
    "    valid_accuracy_no_touch = valid_correct_no_touch / valid_total_no_touch\n",
    "    valid_accuracy_dlora_lc_restored = valid_correct_dlora_lc_restored / valid_total_dlora_lc_restored\n",
    "\n",
    "    valid_accuracy_list.append(valid_accuracy)\n",
    "    \n",
    "    print(\"Epoch: [{}/{}], Training Loss DLoRALC: {:.6f}, Validation Loss DLoRALC: {:.6f}, Training Accuracy DLoRALC: {:.6f}, Validation Accuracy DLoRALC: {:.6f} \\n \\\n",
    "          \\t Training Loss LC: {:.6f}, Validation Loss LC: {:.6f}, Training Accuracy LC: {:.6f}, Validation Accuracy LC: {:.6f}, \\n \\\n",
    "          \\t Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}, \\n \\t Validation Loss dLoRALC restored: {:.6f},  Validation Accuracy dLoRALC restored: {:.6f}\".format(epoch, NUM_EPOCHES-1, \n",
    "        train_loss, valid_loss, train_accuarcy, valid_accuracy, \n",
    "        train_loss_lc, valid_loss_lc, train_accuarcy_lc, valid_accuracy_lc, \n",
    "        train_loss_no_touch, valid_loss_no_touch, train_accuarcy_no_touch, valid_accuracy_no_touch, \n",
    "        valid_loss_dlora_lc_restored, valid_accuracy_dlora_lc_restored)) \n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss_dloralc\": train_loss,\n",
    "        \"valid_loss_dloralc\": valid_loss,\n",
    "        \"train_accuracy_dloralc\": train_accuarcy,\n",
    "        \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "        \"train_loss_lc\": train_loss_lc,\n",
    "        \"valid_loss_lc\": valid_loss_lc,\n",
    "        \"train_accuracy_lc\": train_accuarcy_lc,\n",
    "        \"valid_accuracy_lc\": valid_accuracy_lc,\n",
    "        \"train_loss\": train_loss_no_touch,\n",
    "        \"valid_loss\": valid_loss_no_touch,\n",
    "        \"train_accuracy\": train_accuarcy_no_touch,\n",
    "        \"valid_accuracy\": valid_accuracy_no_touch,\n",
    "        \"valid_loss_dloralc_restored\": valid_loss_dlora_lc_restored,\n",
    "        \"valid_accuracy_dloralc_restored\": valid_accuracy_dlora_lc_restored,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader2...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### **After the training on dataloader2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **We store the delta-LoRA + LC model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_valid_acc = valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9467\n"
     ]
    }
   ],
   "source": [
    "print(rounded_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at accuracy: 0.9467\n"
     ]
    }
   ],
   "source": [
    "# rounded_valid_acc = max(valid_accuracy_list)\n",
    "torch.save(model.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(rounded_valid_acc))\n",
    "print(\"Model saved at accuracy: {:.4f}\".format(rounded_valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Working on training with delta-LoRA and LC-checkpoint on dataloader3** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = getBase(original)\n",
    "model3 = LeNet_LowRank(w, b, rank = RANK).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.load_state_dict(torch.load(HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(rounded_valid_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['feature.0.weight', 'feature.0.bias', 'feature.3.weight', 'feature.3.bias', 'classifier.1.alpha', 'classifier.1.beta', 'classifier.1.bias', 'classifier.3.alpha', 'classifier.3.beta', 'classifier.3.bias', 'classifier.5.weight', 'classifier.5.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(model3.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer3 = torch.optim.SGD(model3.parameters(), lr=learning_rate_dloralc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the current iteration and set to 0\n",
    "current_iter = 0\n",
    "current_set = 0\n",
    "\n",
    "# Initialize the current iteration and set to 0 for the old LC method\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "Start of model training on dataloader3...\n",
      "Epoch: [0/29], Training Loss DLoRALC: 0.185937, Validation Loss DLoRALC: 0.182628, Training Accuracy DLoRALC: 0.945885, Validation Accuracy DLoRALC: 0.948400 \n",
      "           \t Training Loss LC: 0.118471, Validation Loss LC: 0.112789, Training Accuracy LC: 0.966211, Validation Accuracy LC: 0.967100, \n",
      "           \t Training Loss: 0.118470, Validation Loss: 0.112789, Training Accuracy: 0.966211, Validation Accuracy: 0.967100\n",
      "Epoch: [1/29], Training Loss DLoRALC: 0.177219, Validation Loss DLoRALC: 0.175408, Training Accuracy DLoRALC: 0.948417, Validation Accuracy DLoRALC: 0.949100 \n",
      "           \t Training Loss LC: 0.112122, Validation Loss LC: 0.108420, Training Accuracy LC: 0.968744, Validation Accuracy LC: 0.968500, \n",
      "           \t Training Loss: 0.112123, Validation Loss: 0.108421, Training Accuracy: 0.968744, Validation Accuracy: 0.968500\n",
      "Epoch: [2/29], Training Loss DLoRALC: 0.169946, Validation Loss DLoRALC: 0.169833, Training Accuracy DLoRALC: 0.950150, Validation Accuracy DLoRALC: 0.950500 \n",
      "           \t Training Loss LC: 0.107295, Validation Loss LC: 0.106857, Training Accuracy LC: 0.969743, Validation Accuracy LC: 0.968500, \n",
      "           \t Training Loss: 0.107295, Validation Loss: 0.106858, Training Accuracy: 0.969743, Validation Accuracy: 0.968500\n",
      "Epoch: [3/29], Training Loss DLoRALC: 0.163540, Validation Loss DLoRALC: 0.163159, Training Accuracy DLoRALC: 0.952216, Validation Accuracy DLoRALC: 0.953100 \n",
      "           \t Training Loss LC: 0.103485, Validation Loss LC: 0.102789, Training Accuracy LC: 0.970743, Validation Accuracy LC: 0.971100, \n",
      "           \t Training Loss: 0.103486, Validation Loss: 0.102789, Training Accuracy: 0.970743, Validation Accuracy: 0.971100\n",
      "Epoch: [4/29], Training Loss DLoRALC: 0.157668, Validation Loss DLoRALC: 0.158602, Training Accuracy DLoRALC: 0.954349, Validation Accuracy DLoRALC: 0.954200 \n",
      "           \t Training Loss LC: 0.099979, Validation Loss LC: 0.099780, Training Accuracy LC: 0.971476, Validation Accuracy LC: 0.971600, \n",
      "           \t Training Loss: 0.099979, Validation Loss: 0.099779, Training Accuracy: 0.971476, Validation Accuracy: 0.971600\n",
      "Epoch: [5/29], Training Loss DLoRALC: 0.152154, Validation Loss DLoRALC: 0.154762, Training Accuracy DLoRALC: 0.957214, Validation Accuracy DLoRALC: 0.955600 \n",
      "           \t Training Loss LC: 0.096554, Validation Loss LC: 0.098555, Training Accuracy LC: 0.973009, Validation Accuracy LC: 0.971400, \n",
      "           \t Training Loss: 0.096554, Validation Loss: 0.098555, Training Accuracy: 0.973009, Validation Accuracy: 0.971400\n",
      "Epoch: [6/29], Training Loss DLoRALC: 0.146963, Validation Loss DLoRALC: 0.149536, Training Accuracy DLoRALC: 0.958081, Validation Accuracy DLoRALC: 0.956800 \n",
      "           \t Training Loss LC: 0.093538, Validation Loss LC: 0.095814, Training Accuracy LC: 0.973809, Validation Accuracy LC: 0.972800, \n",
      "           \t Training Loss: 0.093538, Validation Loss: 0.095814, Training Accuracy: 0.973809, Validation Accuracy: 0.972800\n",
      "Epoch: [7/29], Training Loss DLoRALC: 0.142318, Validation Loss DLoRALC: 0.145025, Training Accuracy DLoRALC: 0.960347, Validation Accuracy DLoRALC: 0.958900 \n",
      "           \t Training Loss LC: 0.090849, Validation Loss LC: 0.093499, Training Accuracy LC: 0.974542, Validation Accuracy LC: 0.971500, \n",
      "           \t Training Loss: 0.090849, Validation Loss: 0.093499, Training Accuracy: 0.974542, Validation Accuracy: 0.971500\n",
      "Epoch: [8/29], Training Loss DLoRALC: 0.137742, Validation Loss DLoRALC: 0.139935, Training Accuracy DLoRALC: 0.960680, Validation Accuracy DLoRALC: 0.959000 \n",
      "           \t Training Loss LC: 0.088082, Validation Loss LC: 0.090463, Training Accuracy LC: 0.975275, Validation Accuracy LC: 0.973100, \n",
      "           \t Training Loss: 0.088082, Validation Loss: 0.090463, Training Accuracy: 0.975275, Validation Accuracy: 0.973100\n",
      "Epoch: [9/29], Training Loss DLoRALC: 0.133501, Validation Loss DLoRALC: 0.135680, Training Accuracy DLoRALC: 0.963146, Validation Accuracy DLoRALC: 0.960900 \n",
      "           \t Training Loss LC: 0.085732, Validation Loss LC: 0.088326, Training Accuracy LC: 0.976208, Validation Accuracy LC: 0.973900, \n",
      "           \t Training Loss: 0.085732, Validation Loss: 0.088325, Training Accuracy: 0.976208, Validation Accuracy: 0.973900\n",
      "Epoch: [10/29], Training Loss DLoRALC: 0.129437, Validation Loss DLoRALC: 0.132129, Training Accuracy DLoRALC: 0.963745, Validation Accuracy DLoRALC: 0.961800 \n",
      "           \t Training Loss LC: 0.083500, Validation Loss LC: 0.086792, Training Accuracy LC: 0.975941, Validation Accuracy LC: 0.975300, \n",
      "           \t Training Loss: 0.083500, Validation Loss: 0.086792, Training Accuracy: 0.975941, Validation Accuracy: 0.975300\n",
      "Epoch: [11/29], Training Loss DLoRALC: 0.125729, Validation Loss DLoRALC: 0.130117, Training Accuracy DLoRALC: 0.964812, Validation Accuracy DLoRALC: 0.962500 \n",
      "           \t Training Loss LC: 0.081154, Validation Loss LC: 0.086262, Training Accuracy LC: 0.977274, Validation Accuracy LC: 0.975000, \n",
      "           \t Training Loss: 0.081154, Validation Loss: 0.086263, Training Accuracy: 0.977274, Validation Accuracy: 0.975000\n",
      "Epoch: [12/29], Training Loss DLoRALC: 0.122322, Validation Loss DLoRALC: 0.124685, Training Accuracy DLoRALC: 0.965745, Validation Accuracy DLoRALC: 0.964600 \n",
      "           \t Training Loss LC: 0.079221, Validation Loss LC: 0.082760, Training Accuracy LC: 0.977941, Validation Accuracy LC: 0.976200, \n",
      "           \t Training Loss: 0.079220, Validation Loss: 0.082760, Training Accuracy: 0.977941, Validation Accuracy: 0.976200\n",
      "Epoch: [13/29], Training Loss DLoRALC: 0.118946, Validation Loss DLoRALC: 0.122970, Training Accuracy DLoRALC: 0.966411, Validation Accuracy DLoRALC: 0.964700 \n",
      "           \t Training Loss LC: 0.077213, Validation Loss LC: 0.082302, Training Accuracy LC: 0.978474, Validation Accuracy LC: 0.976400, \n",
      "           \t Training Loss: 0.077213, Validation Loss: 0.082302, Training Accuracy: 0.978474, Validation Accuracy: 0.976400\n",
      "Epoch: [14/29], Training Loss DLoRALC: 0.115688, Validation Loss DLoRALC: 0.119596, Training Accuracy DLoRALC: 0.968011, Validation Accuracy DLoRALC: 0.965800 \n",
      "           \t Training Loss LC: 0.075318, Validation Loss LC: 0.080515, Training Accuracy LC: 0.979407, Validation Accuracy LC: 0.976700, \n",
      "           \t Training Loss: 0.075318, Validation Loss: 0.080515, Training Accuracy: 0.979407, Validation Accuracy: 0.976700\n",
      "Epoch: [15/29], Training Loss DLoRALC: 0.112772, Validation Loss DLoRALC: 0.116407, Training Accuracy DLoRALC: 0.969610, Validation Accuracy DLoRALC: 0.967400 \n",
      "           \t Training Loss LC: 0.073559, Validation Loss LC: 0.078913, Training Accuracy LC: 0.979607, Validation Accuracy LC: 0.977000, \n",
      "           \t Training Loss: 0.073559, Validation Loss: 0.078913, Training Accuracy: 0.979673, Validation Accuracy: 0.977000\n",
      "Epoch: [16/29], Training Loss DLoRALC: 0.109688, Validation Loss DLoRALC: 0.113792, Training Accuracy DLoRALC: 0.969810, Validation Accuracy DLoRALC: 0.968500 \n",
      "           \t Training Loss LC: 0.071804, Validation Loss LC: 0.077217, Training Accuracy LC: 0.980540, Validation Accuracy LC: 0.977500, \n",
      "           \t Training Loss: 0.071804, Validation Loss: 0.077217, Training Accuracy: 0.980540, Validation Accuracy: 0.977500\n",
      "Epoch: [17/29], Training Loss DLoRALC: 0.107204, Validation Loss DLoRALC: 0.110246, Training Accuracy DLoRALC: 0.970410, Validation Accuracy DLoRALC: 0.968100 \n",
      "           \t Training Loss LC: 0.070194, Validation Loss LC: 0.076089, Training Accuracy LC: 0.980606, Validation Accuracy LC: 0.978200, \n",
      "           \t Training Loss: 0.070194, Validation Loss: 0.076089, Training Accuracy: 0.980606, Validation Accuracy: 0.978200\n",
      "Epoch: [18/29], Training Loss DLoRALC: 0.104555, Validation Loss DLoRALC: 0.107836, Training Accuracy DLoRALC: 0.971010, Validation Accuracy DLoRALC: 0.968400 \n",
      "           \t Training Loss LC: 0.068614, Validation Loss LC: 0.075021, Training Accuracy LC: 0.980940, Validation Accuracy LC: 0.978400, \n",
      "           \t Training Loss: 0.068614, Validation Loss: 0.075021, Training Accuracy: 0.980940, Validation Accuracy: 0.978400\n",
      "Epoch: [19/29], Training Loss DLoRALC: 0.101925, Validation Loss DLoRALC: 0.105919, Training Accuracy DLoRALC: 0.972542, Validation Accuracy DLoRALC: 0.970700 \n",
      "           \t Training Loss LC: 0.066849, Validation Loss LC: 0.075314, Training Accuracy LC: 0.981473, Validation Accuracy LC: 0.977800, \n",
      "           \t Training Loss: 0.066849, Validation Loss: 0.075314, Training Accuracy: 0.981473, Validation Accuracy: 0.977800\n",
      "Epoch: [20/29], Training Loss DLoRALC: 0.099635, Validation Loss DLoRALC: 0.103023, Training Accuracy DLoRALC: 0.972676, Validation Accuracy DLoRALC: 0.970900 \n",
      "           \t Training Loss LC: 0.065526, Validation Loss LC: 0.072470, Training Accuracy LC: 0.981873, Validation Accuracy LC: 0.978700, \n",
      "           \t Training Loss: 0.065526, Validation Loss: 0.072469, Training Accuracy: 0.981873, Validation Accuracy: 0.978700\n",
      "Epoch: [21/29], Training Loss DLoRALC: 0.097425, Validation Loss DLoRALC: 0.100997, Training Accuracy DLoRALC: 0.973276, Validation Accuracy DLoRALC: 0.971100 \n",
      "           \t Training Loss LC: 0.064073, Validation Loss LC: 0.071539, Training Accuracy LC: 0.982206, Validation Accuracy LC: 0.979100, \n",
      "           \t Training Loss: 0.064073, Validation Loss: 0.071539, Training Accuracy: 0.982206, Validation Accuracy: 0.979100\n",
      "Epoch: [22/29], Training Loss DLoRALC: 0.095307, Validation Loss DLoRALC: 0.099769, Training Accuracy DLoRALC: 0.974209, Validation Accuracy DLoRALC: 0.971200 \n",
      "           \t Training Loss LC: 0.062702, Validation Loss LC: 0.070325, Training Accuracy LC: 0.983339, Validation Accuracy LC: 0.979300, \n",
      "           \t Training Loss: 0.062701, Validation Loss: 0.070325, Training Accuracy: 0.983339, Validation Accuracy: 0.979300\n",
      "Epoch: [23/29], Training Loss DLoRALC: 0.093399, Validation Loss DLoRALC: 0.098229, Training Accuracy DLoRALC: 0.974075, Validation Accuracy DLoRALC: 0.972000 \n",
      "           \t Training Loss LC: 0.061608, Validation Loss LC: 0.069452, Training Accuracy LC: 0.983206, Validation Accuracy LC: 0.979700, \n",
      "           \t Training Loss: 0.061608, Validation Loss: 0.069452, Training Accuracy: 0.983272, Validation Accuracy: 0.979700\n",
      "Epoch: [24/29], Training Loss DLoRALC: 0.091210, Validation Loss DLoRALC: 0.095969, Training Accuracy DLoRALC: 0.974608, Validation Accuracy DLoRALC: 0.972600 \n",
      "           \t Training Loss LC: 0.060004, Validation Loss LC: 0.068605, Training Accuracy LC: 0.983872, Validation Accuracy LC: 0.979800, \n",
      "           \t Training Loss: 0.060004, Validation Loss: 0.068605, Training Accuracy: 0.983872, Validation Accuracy: 0.979800\n",
      "Epoch: [25/29], Training Loss DLoRALC: 0.089331, Validation Loss DLoRALC: 0.094949, Training Accuracy DLoRALC: 0.975675, Validation Accuracy DLoRALC: 0.972500 \n",
      "           \t Training Loss LC: 0.059090, Validation Loss LC: 0.069157, Training Accuracy LC: 0.984405, Validation Accuracy LC: 0.979700, \n",
      "           \t Training Loss: 0.059090, Validation Loss: 0.069157, Training Accuracy: 0.984405, Validation Accuracy: 0.979700\n",
      "Epoch: [26/29], Training Loss DLoRALC: 0.087738, Validation Loss DLoRALC: 0.092455, Training Accuracy DLoRALC: 0.976674, Validation Accuracy DLoRALC: 0.973000 \n",
      "           \t Training Loss LC: 0.058014, Validation Loss LC: 0.066428, Training Accuracy LC: 0.984605, Validation Accuracy LC: 0.980100, \n",
      "           \t Training Loss: 0.058014, Validation Loss: 0.066428, Training Accuracy: 0.984605, Validation Accuracy: 0.980100\n",
      "Epoch: [27/29], Training Loss DLoRALC: 0.086131, Validation Loss DLoRALC: 0.091504, Training Accuracy DLoRALC: 0.976808, Validation Accuracy DLoRALC: 0.973600 \n",
      "           \t Training Loss LC: 0.056891, Validation Loss LC: 0.066030, Training Accuracy LC: 0.984538, Validation Accuracy LC: 0.980200, \n",
      "           \t Training Loss: 0.056891, Validation Loss: 0.066031, Training Accuracy: 0.984538, Validation Accuracy: 0.980100\n",
      "Epoch: [28/29], Training Loss DLoRALC: 0.084303, Validation Loss DLoRALC: 0.089447, Training Accuracy DLoRALC: 0.977141, Validation Accuracy DLoRALC: 0.974200 \n",
      "           \t Training Loss LC: 0.055727, Validation Loss LC: 0.064992, Training Accuracy LC: 0.984738, Validation Accuracy LC: 0.981000, \n",
      "           \t Training Loss: 0.055727, Validation Loss: 0.064991, Training Accuracy: 0.984738, Validation Accuracy: 0.981000\n",
      "Epoch: [29/29], Training Loss DLoRALC: 0.082636, Validation Loss DLoRALC: 0.087886, Training Accuracy DLoRALC: 0.978007, Validation Accuracy DLoRALC: 0.974700 \n",
      "           \t Training Loss LC: 0.054613, Validation Loss LC: 0.064615, Training Accuracy LC: 0.984938, Validation Accuracy LC: 0.980800, \n",
      "           \t Training Loss: 0.054613, Validation Loss: 0.064615, Training Accuracy: 0.984938, Validation Accuracy: 0.980800\n",
      "End of model training on dataloader3...\n"
     ]
    }
   ],
   "source": [
    "print(\"-----------------------------------\")\n",
    "\n",
    "wandb.config[\"branch_accuracy 3\"] = rounded_valid_acc\n",
    "wandb.config[\"train dataset 3\"] = \"MNIST train dataloader3\"\n",
    "\n",
    "# Training code on dataloader3\n",
    "print(\"Start of model training on dataloader3...\")\n",
    "\n",
    "isLoop = True\n",
    "\n",
    "valid_accuracy_list = []\n",
    "\n",
    "base = None\n",
    "\n",
    "for epoch in range(NUM_EPOCHES):\n",
    "    if not isLoop:\n",
    "        break\n",
    "    else:\n",
    "        train_loss = 0.0\n",
    "        train_loss_lc = 0.0\n",
    "        train_loss_no_touch = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_loss_lc = 0.0\n",
    "        valid_loss_no_touch = 0.0\n",
    "\n",
    "        train_correct = 0\n",
    "        train_correct_lc = 0\n",
    "        train_correct_no_touch = 0\n",
    "        train_total = 0\n",
    "        train_total_lc = 0\n",
    "        train_total_no_touch = 0\n",
    "\n",
    "        valid_correct = 0\n",
    "        valid_correct_lc = 0\n",
    "        valid_correct_no_touch = 0\n",
    "        valid_total = 0\n",
    "        valid_total_lc = 0\n",
    "        valid_total_no_touch = 0\n",
    "        \n",
    "        model3.train()\n",
    "        model_original.train()\n",
    "        model_no_touch.train()\n",
    "\n",
    "        for iter, data in enumerate(train_loader3):\n",
    "\n",
    "            # Check if it is the first iteration of the first epoch\n",
    "            if iter == 0: # first iteration, create baseline model\n",
    "            # if iter == 0 and epoch == 0: # first iteration, create baseline model\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "                \n",
    "                base, base_decomp = lc.extract_weights_gpu(model3, SAVE_LOC + \n",
    "                                                        \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                cstate = model_original.state_dict()\n",
    "                set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "                if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "                    os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "                prev_state = olc.extract_weights_gpu(cstate, SAVE_LOC_OLC + set_path,DECOMPOSED_LAYERS)\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC \n",
    "                ########################################################\n",
    "\n",
    "                # Delta-compression: The delta for the weights of the normal and decomposed layers.\n",
    "                # Also returns the full dictionary, which holds the bias.\n",
    "\n",
    "                #Calculate the time before generate_delta_gpu function\n",
    "                delta, decomp_delta, bias = lc.generate_delta_gpu(base, \n",
    "                                                                base_decomp, model3.state_dict(), DECOMPOSED_LAYERS)\n",
    "                # Compressing the delta and decomposed delta\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                # Saving checkpoint\n",
    "                # lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                #                 \"/set_{}\".format(current_set))\n",
    "            \n",
    "                # Update base and base_decomp\n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                # Update current iteration\n",
    "                current_iter += 1\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                cstate = model_original.state_dict()\n",
    "                old_lc_delta, old_lc_bias = olc.generate_delta_gpu(prev_state, cstate, DECOMPOSED_LAYERS)\n",
    "                olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "                # olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "                #                     old_lc_bias, current_iter_old_lc)\n",
    "                prev_state = np.add(prev_state, update_prev)\n",
    "                current_iter_old_lc += 1\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            ########################################################\n",
    "            ### DELTA-LORA + LC\n",
    "            ########################################################\n",
    "\n",
    "            optimizer3.zero_grad()\n",
    "            output = model3(inputs)\n",
    "\n",
    "            loss = torch.nn.CrossEntropyLoss()(output, labels)\n",
    "            loss.backward()\n",
    "            optimizer3.step()\n",
    "            train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            train_acc = torch.eq(output.argmax(-1), labels).float().mean()\n",
    "\n",
    "            ########################################################\n",
    "            ### LC\n",
    "            ########################################################\n",
    "\n",
    "            optimizer_lc_only.zero_grad()\n",
    "            output_lc = model_original(inputs)\n",
    "\n",
    "            loss_lc = torch.nn.CrossEntropyLoss()(output_lc, labels)\n",
    "            loss_lc.backward()\n",
    "            optimizer_lc_only.step()\n",
    "            train_loss_lc += loss_lc.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_lc = torch.max(output_lc, 1)\n",
    "            train_correct_lc += (predicted_lc == labels).sum().item()\n",
    "            train_total_lc += labels.size(0)\n",
    "\n",
    "            train_acc_lc = torch.eq(output_lc.argmax(-1), labels).float().mean()\n",
    "\n",
    "            ########################################################\n",
    "            ### NO TOUCH\n",
    "            ########################################################\n",
    "\n",
    "            optimizer_no_touch.zero_grad()\n",
    "            output_no_touch = model_no_touch(inputs)\n",
    "\n",
    "            loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, labels)\n",
    "            loss_no_touch.backward()\n",
    "            optimizer_no_touch.step()\n",
    "            train_loss_no_touch += loss_no_touch.item() * inputs.size(0)\n",
    "\n",
    "            _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "            train_correct_no_touch += (predicted_no_touch == labels).sum().item()\n",
    "            train_total_no_touch += labels.size(0)\n",
    "\n",
    "            train_acc_no_touch = torch.eq(output_no_touch.argmax(-1), labels).float().mean()\n",
    "\n",
    "        model3.eval()\n",
    "        model_original.eval()\n",
    "        model_no_touch.eval()\n",
    "        with torch.no_grad():  # Gradient computation is not needed for validation\n",
    "            for data, target in test_loader:\n",
    "                # Move data and target to the correct device\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                ########################################################\n",
    "                ### DELTA-LORA + LC\n",
    "                ########################################################\n",
    "\n",
    "                output = model3(data)\n",
    "                loss = torch.nn.CrossEntropyLoss()(output, target)\n",
    "                valid_loss += loss.item() * data.size(0)\n",
    "\n",
    "                _, predicted = torch.max(output, 1)\n",
    "                valid_correct += (predicted == target).sum().item()\n",
    "                valid_total += target.size(0)\n",
    "\n",
    "                valid_acc = torch.eq(output.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### LC\n",
    "                ########################################################\n",
    "\n",
    "                output_lc = model_original(data)\n",
    "                loss_lc = torch.nn.CrossEntropyLoss()(output_lc, target)\n",
    "                valid_loss_lc += loss_lc.item() * data.size(0)\n",
    "\n",
    "                _, predicted_lc = torch.max(output_lc, 1)\n",
    "                valid_correct_lc += (predicted_lc == target).sum().item()\n",
    "                valid_total_lc += target.size(0)\n",
    "\n",
    "                valid_acc_lc = torch.eq(output_lc.argmax(-1), target).float().mean()\n",
    "\n",
    "                ########################################################\n",
    "                ### NO TOUCH\n",
    "                ########################################################\n",
    "\n",
    "                output_no_touch = model_no_touch(data)\n",
    "                loss_no_touch = torch.nn.CrossEntropyLoss()(output_no_touch, target)\n",
    "                valid_loss_no_touch += loss_no_touch.item() * data.size(0)\n",
    "\n",
    "                _, predicted_no_touch = torch.max(output_no_touch, 1)\n",
    "                valid_correct_no_touch += (predicted_no_touch == target).sum().item()\n",
    "                valid_total_no_touch += target.size(0)\n",
    "\n",
    "                valid_acc_no_touch = torch.eq(output_no_touch.argmax(-1), target).float().mean()\n",
    "\n",
    "                \n",
    "    # Calculate average losses\n",
    "    train_loss /= len(train_loader3.dataset)\n",
    "    train_loss_lc /= len(train_loader3.dataset)\n",
    "    train_loss_no_touch /= len(train_loader3.dataset)\n",
    "    valid_loss /= len(test_loader.dataset)\n",
    "    valid_loss_lc /= len(test_loader.dataset)\n",
    "    valid_loss_no_touch /= len(test_loader.dataset)\n",
    "\n",
    "    train_accuarcy = train_correct / train_total\n",
    "    train_accuarcy_lc = train_correct_lc / train_total_lc\n",
    "    train_accuarcy_no_touch = train_correct_no_touch / train_total_no_touch\n",
    "    valid_accuracy = valid_correct / valid_total\n",
    "    valid_accuracy_lc = valid_correct_lc / valid_total_lc\n",
    "    valid_accuracy_no_touch = valid_correct_no_touch / valid_total_no_touch\n",
    "\n",
    "    valid_accuracy_list.append(valid_accuracy)\n",
    "    \n",
    "    print(\"Epoch: [{}/{}], Training Loss DLoRALC: {:.6f}, Validation Loss DLoRALC: {:.6f}, Training Accuracy DLoRALC: {:.6f}, Validation Accuracy DLoRALC: {:.6f} \\n \\\n",
    "          \\t Training Loss LC: {:.6f}, Validation Loss LC: {:.6f}, Training Accuracy LC: {:.6f}, Validation Accuracy LC: {:.6f}, \\n \\\n",
    "          \\t Training Loss: {:.6f}, Validation Loss: {:.6f}, Training Accuracy: {:.6f}, Validation Accuracy: {:.6f}\".format(epoch, NUM_EPOCHES-1, \n",
    "        train_loss, valid_loss, train_accuarcy, valid_accuracy, \n",
    "        train_loss_lc, valid_loss_lc, train_accuarcy_lc, valid_accuracy_lc, \n",
    "        train_loss_no_touch, valid_loss_no_touch, train_accuarcy_no_touch, valid_accuracy_no_touch)) \n",
    "\n",
    "    wandb.log({\n",
    "        \"train_loss_dloralc\": train_loss,\n",
    "        \"valid_loss_dloralc\": valid_loss,\n",
    "        \"train_accuracy_dloralc\": train_accuarcy,\n",
    "        \"valid_accuracy_dloralc\": valid_accuracy,\n",
    "        \"train_loss_lc\": train_loss_lc,\n",
    "        \"valid_loss_lc\": valid_loss_lc,\n",
    "        \"train_accuracy_lc\": train_accuarcy_lc,\n",
    "        \"valid_accuracy_lc\": valid_accuracy_lc,\n",
    "        \"train_loss\": train_loss_no_touch,\n",
    "        \"valid_loss\": valid_loss_no_touch,\n",
    "        \"train_accuracy\": train_accuarcy_no_touch,\n",
    "        \"valid_accuracy\": valid_accuracy_no_touch,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "print(\"End of model training on dataloader3...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved at accuracy: 0.9747\n"
     ]
    }
   ],
   "source": [
    "rounded_valid_acc = valid_accuracy\n",
    "torch.save(model3.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/lenet/branch_{}.pt\".format(rounded_valid_acc))\n",
    "print(\"Model saved at accuracy: {:.4f}\".format(rounded_valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee539ebb22d44a8b8c8adef77d3f63f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñà‚ñà‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà</td></tr><tr><td>train_accuracy</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_accuracy_dloralc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_accuracy_lc</td><td>‚ñÅ‚ñÑ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train_loss</td><td>‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss_dloralc</td><td>‚ñà‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train_loss_lc</td><td>‚ñà‚ñÜ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>valid_accuracy</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>valid_accuracy_dloralc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>valid_accuracy_lc</td><td>‚ñÅ‚ñÑ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>valid_loss</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>valid_loss_dloralc</td><td>‚ñà‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>valid_loss_lc</td><td>‚ñà‚ñÖ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_accuracy</td><td>0.98494</td></tr><tr><td>train_accuracy_dloralc</td><td>0.97801</td></tr><tr><td>train_accuracy_lc</td><td>0.98494</td></tr><tr><td>train_loss</td><td>0.05461</td></tr><tr><td>train_loss_dloralc</td><td>0.08264</td></tr><tr><td>train_loss_lc</td><td>0.05461</td></tr><tr><td>valid_accuracy</td><td>0.9808</td></tr><tr><td>valid_accuracy_dloralc</td><td>0.9747</td></tr><tr><td>valid_accuracy_lc</td><td>0.9808</td></tr><tr><td>valid_loss</td><td>0.06461</td></tr><tr><td>valid_loss_dloralc</td><td>0.08789</td></tr><tr><td>valid_loss_lc</td><td>0.06462</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">LeNet-With-Incremental-Learning_LC_DLORA-Without-Restore</strong> at: <a href='https://wandb.ai/bryanbradfo/LeNet/runs/rj3c3ftq/workspace' target=\"_blank\">https://wandb.ai/bryanbradfo/LeNet/runs/rj3c3ftq/workspace</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240704_155939-rj3c3ftq\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model_dloralc = lazy_restore_gpu(base, base_decomp, bias, LeNet(), \n",
    "                                            original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                            rank = RANK, scaling = SCALING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restored_model_dloralc.eval()\n",
    "restored_model_dloralc.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = restored_model_dloralc(data)\n",
    "\n",
    "        loss_dlora_lc_restored = torch.nn.CrossEntropyLoss()(output_dlora_lc_restored, target.cpu())\n",
    "        valid_loss_dlora_lc_restored = loss_dlora_lc_restored.item() * data.cpu().size(0)\n",
    "\n",
    "        _, predicted_dlora_lc_restored = torch.max(output_dlora_lc_restored, 1)\n",
    "        valid_correct_dlora_lc_restored = (predicted_dlora_lc_restored == target.cpu()).sum().item()\n",
    "        valid_total_dlora_lc_restored += target.cpu().size(0)\n",
    "\n",
    "        valid_acc_dlora_lc_restored = torch.eq(output_dlora_lc_restored.argmax(-1), target.cpu()).float().mean()\n",
    "\n",
    "valid_loss_dlora_lc_restored /= len(test_loader.dataset)\n",
    "valid_accuracy_dlora_lc_restored = valid_correct_dlora_lc_restored / valid_total_dlora_lc_restored\n",
    "print(\"Validation Loss dLoRALC restored: {:.6f},  Validation Accuracy dLoRALC restored: {:.6f}\".format(valid_loss_dlora_lc_restored, valid_accuracy_dlora_lc_restored))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
