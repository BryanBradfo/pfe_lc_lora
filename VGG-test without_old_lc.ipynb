{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\utils\\generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "# import old_lc.main as olc\n",
    "from src.models.VGG16 import VGG16\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.VGG16_LowRank import getBase, VGG16_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, lazy_restore, evaluate_compression\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_img_train.tar is not present in the root directory or is corrupted. You need to download it externally and place it in ./data_imagenet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m ssl\u001b[38;5;241m.\u001b[39m_create_default_https_context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Print out some information about the datasets\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of training examples: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[2], line 18\u001b[0m, in \u001b[0;36mdata_loader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     11\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize(\u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# Resize the image to 256x256\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mCenterCrop(\u001b[38;5;241m224\u001b[39m),  \u001b[38;5;66;03m# Crop the center 224x224 portion of the image\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     14\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m])  \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[0;32m     15\u001b[0m ])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Load ImageNet training dataset\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data_imagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Load ImageNet validation dataset\u001b[39;00m\n\u001b[0;32m     21\u001b[0m testset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageNet(root\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data_imagenet\u001b[39m\u001b[38;5;124m'\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:52\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[1;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m verify_str_arg(split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_archives\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m wnid_to_classes \u001b[38;5;241m=\u001b[39m load_meta_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:69\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m         \u001b[43mparse_train_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     71\u001b[0m         parse_val_archive(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:175\u001b[0m, in \u001b[0;36mparse_train_archive\u001b[1;34m(root, file, folder)\u001b[0m\n\u001b[0;32m    172\u001b[0m     file \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    173\u001b[0m md5 \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 175\u001b[0m \u001b[43m_verify_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m train_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, folder)\n\u001b[0;32m    178\u001b[0m extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), train_root)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:102\u001b[0m, in \u001b[0;36m_verify_archive\u001b[1;34m(root, file, md5)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), md5):\n\u001b[0;32m     98\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe archive \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not present in the root directory or is corrupted. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to download it externally and place it in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(file, root))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The archive ILSVRC2012_img_train.tar is not present in the root directory or is corrupted. You need to download it externally and place it in ./data_imagenet."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import ssl\n",
    "\n",
    "# Placeholder for HDD path\n",
    "HDFP = \"./volumes/Ultra Touch\"\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(256),  # Resize the image to 256x256\n",
    "        transforms.CenterCrop(224),  # Crop the center 224x224 portion of the image\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize\n",
    "    ])\n",
    "\n",
    "    # Load ImageNet training dataset\n",
    "    trainset = datasets.ImageNet(root='./data_imagenet', split='train', download=True, transform=transform)\n",
    "\n",
    "    # Load ImageNet validation dataset\n",
    "    testset = datasets.ImageNet(root='./data_imagenet', split='val', download=True, transform=transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# Load datasets\n",
    "train_loader, test_loader = data_loader()\n",
    "\n",
    "# Print out some information about the datasets\n",
    "print(f\"Number of training examples: {len(train_loader.dataset)}\")\n",
    "print(f\"Number of test examples: {len(test_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(32, antialias=True)])\n",
    "\n",
    "    trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                          download=True, transform=transform)\n",
    "    # Reintroduce the 2000 datapoints model has not seen before.\n",
    "    trainset.data = trainset.data.clone()[-2000:-1000]\n",
    "    trainset.targets = trainset.targets.clone()[-2000:-1000]\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "    testset = datasets.MNIST(root='./data', train=False,\n",
    "                                         download=True, transform=transform)\n",
    "\n",
    "    testset.data = trainset.data[-1000:]\n",
    "    testset.targets = trainset.targets[-1000:]\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "    \n",
    "    return trainloader, testloader\n",
    "\n",
    "# HDFP = \"./volumes/Ultra Touch\"  # Placeholder for HDD path\n",
    "\n",
    "# def data_loader():\n",
    "#     transform = transforms.Compose([\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.1307,), (0.3081,))\n",
    "#     ])\n",
    "\n",
    "#     # Load the ImageNet training dataset\n",
    "#     trainset = datasets.ImageNet(root='./data_imagenet', train=True, download=True, transform=transform)\n",
    "#     # Use the last 10,000 images for training\n",
    "#     trainset.data = trainset.data.clone()[-1000:]\n",
    "#     trainset.targets = trainset.targets.clone()[-1000:]\n",
    "#     # trainset.data = trainset.data.clone()[-58000:]\n",
    "#     # trainset.targets = trainset.targets.clone()[-58000:]\n",
    "#     train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "#     # Load the MNIST test dataset\n",
    "#     testset = datasets.ImageNet(root='./data_imagenet', train=False, download=True, transform=transform)\n",
    "#     # Use the first 1,000 images from the test dataset\n",
    "#     testset.data = testset.data.clone()[:300]\n",
    "#     testset.targets = testset.targets.clone()[:300]\n",
    "#     test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "#     return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The archive ILSVRC2012_img_train.tar is not present in the root directory or is corrupted. You need to download it externally and place it in ./data_imagenet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m ssl\u001b[38;5;241m.\u001b[39m_create_default_https_context \u001b[38;5;241m=\u001b[39m ssl\u001b[38;5;241m.\u001b[39m_create_unverified_context\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# MNIST dataset \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m \u001b[43mdata_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 36\u001b[0m, in \u001b[0;36mdata_loader\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m     31\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m     32\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,))\n\u001b[0;32m     33\u001b[0m ])\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Load the ImageNet training dataset\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m trainset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./data_imagenet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Use the last 10,000 images for training\u001b[39;00m\n\u001b[0;32m     38\u001b[0m trainset\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m trainset\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mclone()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1000\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:52\u001b[0m, in \u001b[0;36mImageNet.__init__\u001b[1;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[0;32m     49\u001b[0m root \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexpanduser(root)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m=\u001b[39m verify_str_arg(split, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m, (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 52\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_archives\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m wnid_to_classes \u001b[38;5;241m=\u001b[39m load_meta_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:69\u001b[0m, in \u001b[0;36mImageNet.parse_archives\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_folder):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 69\u001b[0m         \u001b[43mparse_train_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     71\u001b[0m         parse_val_archive(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:175\u001b[0m, in \u001b[0;36mparse_train_archive\u001b[1;34m(root, file, folder)\u001b[0m\n\u001b[0;32m    172\u001b[0m     file \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    173\u001b[0m md5 \u001b[38;5;241m=\u001b[39m archive_meta[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 175\u001b[0m \u001b[43m_verify_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m train_root \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, folder)\n\u001b[0;32m    178\u001b[0m extract_archive(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), train_root)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torchvision\\datasets\\imagenet.py:102\u001b[0m, in \u001b[0;36m_verify_archive\u001b[1;34m(root, file, md5)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_integrity(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root, file), md5):\n\u001b[0;32m     98\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     99\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe archive \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not present in the root directory or is corrupted. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to download it externally and place it in \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m     )\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(file, root))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The archive ILSVRC2012_img_train.tar is not present in the root directory or is corrupted. You need to download it externally and place it in ./data_imagenet."
     ]
    }
   ],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Assuming train_loader is defined and loaded as before\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(\u001b[43mtrain_loader\u001b[49m)\n\u001b[0;32m     15\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataiter)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Show images\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# Adjust these values to match the normalization values used during the loading of your dataset\n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    # Adjusting unnormalization for potentially 3-channel images\n",
    "    img = img * std + mean  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Assuming train_loader is defined and loaded as before\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images[:3]))\n",
    "# Print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/lobranch\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "\n",
    "# SAVE_LOC_OLC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/old-lc\"\n",
    "# if not os.path.exists(SAVE_LOC_OLC):\n",
    "#     os.makedirs(SAVE_LOC_OLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binary(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = torch.sigmoid(model(input))  # Apply sigmoid to model output to get probabilities.\n",
    "            output = torch.where(output > 0.5, 1, 0)  # Threshold probabilities at 0.5 to decide between classes 0 and 1.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += (output == label).sum().item()  # Increment correct predictions by the number of matches in the batch.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n",
    "\n",
    "def accuracy_multiclass(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = model(input)  # Get the raw logits from the model.\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max logit which represents the predicted class.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += pred.eq(label.view_as(pred)).sum().item()  # Compare predictions with true labels and sum up correct predictions.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the base model (VGG16)\n",
    "\n",
    "model_for_checkpoint = VGG16()\n",
    "\n",
    "# creating branchpoints : \n",
    "\n",
    "epochs = 10\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_for_checkpoint.parameters(), lr=0.01, momentum=0.9) # momentum=0.9\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # print(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_for_checkpoint(inputs)\n",
    "\n",
    "        # if self.config.loss_function == \"binary_cross_entropy\":\n",
    "        #     outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # loss = loss_function(outputs, labels)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Epoch {} | Iteration {} : Loss {}\".format(epoch, iter, loss.item()))\n",
    "        if iter % 20 == 0:\n",
    "            print(\"Running validation for {} Epoch, {} Iteration...\".format(epoch, iter))\n",
    "            # Previously: res = accuracy_binary(model_for_checkpoint, test_loader)\n",
    "            res = accuracy_multiclass(model_for_checkpoint, test_loader)\n",
    "\n",
    "            # res = accuracy_binary(model_for_checkpoint, test_loader)\n",
    "            \n",
    "            print(\"ACCURACY: {}\".format(res))\n",
    "            if res > 0.7:\n",
    "                torch.save(model_for_checkpoint.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/vgg/branch_{}.pt\".format(res))\n",
    "            if res > 0.9:\n",
    "                isLoop = False\n",
    "                break\n",
    "    if not isLoop:\n",
    "        break\n",
    "    print(\"Length of train_loader is: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VGG16' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m BRANCH_ACC \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.809\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Set up weights for original AlexNet model\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m original \u001b[38;5;241m=\u001b[39m \u001b[43mVGG16\u001b[49m()\n\u001b[0;32m      8\u001b[0m model_original \u001b[38;5;241m=\u001b[39m VGG16()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load from \"branch point\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'VGG16' is not defined"
     ]
    }
   ],
   "source": [
    "DECOMPOSED_LAYERS = ['classifier.0.weight', 'classifier.3.weight', 'classifier.6.weight']\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "BRANCH_ACC = \"0.809\"\n",
    "\n",
    "# Set up weights for original AlexNet model\n",
    "original = VGG16()\n",
    "model_original = VGG16()\n",
    "\n",
    "# Load from \"branch point\"\n",
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/vgg/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "\n",
    "w, b = getBase(model_original)\n",
    "model = VGG16_LowRank(w, b, rank = RANK)\n",
    "load_sd_decomp(torch.load(BRANCH_LOC), model, DECOMPOSED_LAYERS)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "optimizer_full = torch.optim.SGD(model_original.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_normal_max = []\n",
    "delta_normal_min = []\n",
    "delta_decomposed_max = []\n",
    "delta_decomposed_min = []\n",
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "current_iter = 0\n",
    "current_set = 0\n",
    "\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0\n",
    "\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n",
    "\n",
    "for epch in range(20):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"Epoch: {}, Iteration: {}\".format(epch, i))\n",
    "        \n",
    "        set_path = \"/set_{}\".format(current_set)\n",
    "        if not os.path.exists(SAVE_LOC + set_path):\n",
    "            os.makedirs(SAVE_LOC + set_path)\n",
    "\n",
    "        if i == 0 and epch == 0: # first iteration, create baseline model\n",
    "            base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "        else:\n",
    "            if i % 10 == 0: \n",
    "                # full snapshot!\n",
    "                new_model = lazy_restore(base, base_decomp, bias, VGG16(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "                original = new_model # Changing previous \"original model\" used to restore the loRA model.\n",
    "                \n",
    "                current_set += 1\n",
    "                current_iter = 0\n",
    "\n",
    "                set_path = \"/set_{}\".format(current_set)\n",
    "                if not os.path.exists(SAVE_LOC + set_path):\n",
    "                    os.makedirs(SAVE_LOC + set_path)\n",
    "                \n",
    "                # Rebuilding LoRA layers => reset model!\n",
    "                w, b = getBase(original)\n",
    "                model = VGG16_LowRank(w, b, rank = RANK)\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "                load_sd_decomp(original.state_dict(), model, DECOMPOSED_LAYERS)\n",
    "                base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "            else:\n",
    "                # Delta-compression\n",
    "                delta, decomp_delta, bias = lc.generate_delta(base, \n",
    "                                                                base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                \n",
    "                # Saving checkpoint\n",
    "                lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                                \"/set_{}\".format(current_set))\n",
    "    \n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                current_iter += 1\n",
    "            \n",
    "        # # ==========================\n",
    "        # # Saving using LC-Checkpoint\n",
    "        # # ==========================\n",
    "                \n",
    "        # if i == 0 and epch == 0:\n",
    "        #     cstate = model_original.state_dict()\n",
    "        #     set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "        #     if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "        #         os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "        #     torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "        #     prev_state = olc.extract_weights(model_original)\n",
    "        # else:\n",
    "        #     if i % 10 == 0:\n",
    "        #         cstate = model_original.state_dict()\n",
    "        #         current_set_old_lc += 1\n",
    "        #         current_iter_old_lc = 0\n",
    "        #         set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "        #         if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "        #             os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "        #         torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "        #         prev_state = olc.extract_weights(model_original)\n",
    "        #     else:\n",
    "        #         cstate = model_original.state_dict()\n",
    "        #         old_lc_delta, old_lc_bias, _ = olc.generate_delta(prev_state, cstate)\n",
    "        #         olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "        #         olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "        #                             old_lc_bias, epch, current_iter_old_lc)\n",
    "        #         prev_state = np.add(prev_state, update_prev)\n",
    "        #         current_iter_old_lc += 1\n",
    "        \n",
    "        # ==========================\n",
    "        # Training on Low-Rank Model\n",
    "        # ==========================\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # ======================\n",
    "        # Training on Full Model\n",
    "        # ======================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_full.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs_full = model_original(inputs)\n",
    "        loss_full = torch.nn.functional.cross_entropy(outputs_full,labels)\n",
    "        loss_full.backward()\n",
    "        optimizer_full.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"Training Accuracy | Decomposed: {}, Full : {}\".format(acc(outputs, labels), \n",
    "                                                                         acc(outputs_full, labels)))\n",
    "\n",
    "        if i != 0  and i % 5 == 0: # Evaluation on testing set\n",
    "            full_accuracy.append(evaluate_accuracy(model_original, test_loader))\n",
    "            decomposed_full_accuracy.append(evaluate_accuracy(model, test_loader))\n",
    "            restored_model = lazy_restore(base, base_decomp, bias, VGG16(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                          rank = RANK, scaling = SCALING)\n",
    "            restored_accuracy.append(evaluate_accuracy(restored_model, test_loader))\n",
    "            restored_lc_model = VGG16()\n",
    "            # restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "            #                                                       restored_model.state_dict()))\n",
    "            lc_accuracy.append(evaluate_accuracy(restored_lc_model, test_loader))\n",
    "            print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\n",
    "                full_accuracy[-1], lc_accuracy[-1], decomposed_full_accuracy[-1], restored_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "full_accuracy = data['full_acc']\n",
    "lc_accuracy = data[\"lc_restored_accuracy\"]\n",
    "restored_accuracy = data[\"decomposed_restored_accuracy\"]\n",
    "decomposed_full_accuracy = data[\"decomposed_full_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BRANCH_ACC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVGG-16, Accuracy, Branched @ \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[43mBRANCH_ACC\u001b[49m))\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(full_accuracy, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDefault VGG-16\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# plt.plot(lc_accuracy, label = \"LC VGG-16\")\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BRANCH_ACC' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"VGG-16, Accuracy, Branched @ {} Accuracy\".format(BRANCH_ACC))\n",
    "plt.plot(full_accuracy, label = \"Default VGG-16\")\n",
    "# plt.plot(lc_accuracy, label = \"LC VGG-16\")\n",
    "plt.plot(decomposed_full_accuracy, label = \"dLoRA VGG-16\")\n",
    "plt.plot(restored_accuracy, label = \"dLoRA + LC VGG-16\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"VGG-16, Accuracy, Branched @ {} Accuracy\".format(BRANCH_ACC))\n",
    "plt.plot(lc_accuracy, label = \"LC VGG-16\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangex = [x for x in range(0, 120) if x % 6 == 0]\n",
    "rangey = [x for x in range(0, 20)]\n",
    "plt.figure(figsize = (40, 10))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"VGG-16 Absolute Accuracy Loss (Default VGG-16 vs LC + dLoRA VGG-16)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(restored_accuracy))), label = \"LC + dLoRA VGG-16\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC VGG-16\")\n",
    "plt.legend()\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.ylim(0, 0.5)\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title(\"VGG-16 Absolute Restoration Accuracy Loss (LC + dLoRA VGG-16 & LC VGG-16)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(restored_accuracy), \n",
    "                     np.array(decomposed_full_accuracy))), label = \"LC + dLoRA VGG-16\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC VGG-16\")\n",
    "plt.legend()\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getsize(sl):\n",
    "    dir = [x for x in os.listdir(sl)]\n",
    "    csize, usize = 0, 0\n",
    "    for set in dir:\n",
    "        for f in os.listdir(sl + \"/\" + set):\n",
    "            fp = sl + \"/{}/{}\".format(set, f)\n",
    "            csize += os.path.getsize(fp)\n",
    "            usize += 119.6 * math.pow(2, 20) # torch checkpoint same size\n",
    "    return csize, usize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_size, uncompressed_size = getsize(SAVE_LOC)\n",
    "a, b = evaluate_compression(uncompressed_size, compressed_size)\n",
    "# compressed_size, uncompressed_size = getsize(SAVE_LOC_OLC)\n",
    "# a1, b1 = evaluate_compression(uncompressed_size, compressed_size)\n",
    "\n",
    "# print(\"LC-Checkpoint + GZIP\")\n",
    "# print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a1, b1))\n",
    "print(\"LoRA + LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"full_acc\" : full_accuracy,\n",
    "    \"decomposed_restored_accuracy\" : restored_accuracy,\n",
    "    \"decomposed_full_accuracy\" : decomposed_full_accuracy,\n",
    "    \"lc_restored_accuracy\" : lc_accuracy\n",
    "}\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/data.json\", 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
