{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\transformers\\utils\\generic.py:485: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import scipy as spy\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import ssl\n",
    "import pickle, json\n",
    "import src.main as lc\n",
    "# import old_lc.main as olc\n",
    "from src.models.VGG16 import VGG16\n",
    "import src.compression.deltaCompress as lc_compress\n",
    "from src.models.VGG16_LowRank import getBase, VGG16_LowRank, load_sd_decomp\n",
    "from src.utils.utils import evaluate_accuracy, lazy_restore, evaluate_compression\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDFP = \"./volumes/Ultra Touch\" # Load HHD\n",
    "\n",
    "# def data_loader():\n",
    "#     transform = transforms.Compose([transforms.ToTensor(), transforms.Resize(32, antialias=True)])\n",
    "\n",
    "#     trainset = datasets.MNIST(root='./data', train=True,\n",
    "#                                           download=True, transform=transform)\n",
    "#     # Reintroduce the 2000 datapoints model has not seen before.\n",
    "#     trainset.data = trainset.data.clone()[-2000:-1000]\n",
    "#     trainset.targets = trainset.targets.clone()[-2000:-1000]\n",
    "#     trainloader = torch.utils.data.DataLoader(trainset, batch_size = 32,\n",
    "#                                               shuffle=False, num_workers=2)\n",
    "\n",
    "#     testset = datasets.MNIST(root='./data', train=False,\n",
    "#                                          download=True, transform=transform)\n",
    "\n",
    "#     testset.data = trainset.data[-1000:]\n",
    "#     testset.targets = trainset.targets[-1000:]\n",
    "#     testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "#                                              shuffle=False, num_workers=2)\n",
    "    \n",
    "#     testloader = torch.utils.data.DataLoader(testset, batch_size = 32,\n",
    "#                                              shuffle=False, num_workers=2)\n",
    "    \n",
    "#     return trainloader, testloader\n",
    "\n",
    "HDFP = \"./volumes/Ultra Touch\"  # Placeholder for HDD path\n",
    "\n",
    "def data_loader():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "    # Load the MNIST training dataset\n",
    "    trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    # Use the last 10,000 images for training\n",
    "    trainset.data = trainset.data.clone()[-58000:]\n",
    "    trainset.targets = trainset.targets.clone()[-58000:]\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True, num_workers=2)\n",
    "\n",
    "    # Load the MNIST test dataset\n",
    "    testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    # Use the first 1,000 images from the test dataset\n",
    "    testset.data = testset.data.clone()[:2000]\n",
    "    testset.targets = testset.targets.clone()[:2000]\n",
    "    test_loader = torch.utils.data.DataLoader(testset, batch_size=32, shuffle=False, num_workers=2)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass using SSL unverified\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "# MNIST dataset \n",
    "train_loader, test_loader = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8) tensor(4) tensor(3)\n"
     ]
    }
   ],
   "source": [
    "# Adjust these values to match the normalization values used during the loading of your dataset\n",
    "mean = 0.1307\n",
    "std = 0.3081\n",
    "\n",
    "# Function to show an image\n",
    "def imshow(img):\n",
    "    # Adjusting unnormalization for potentially 3-channel images\n",
    "    img = img * std + mean  # Unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# Assuming train_loader is defined and loaded as before\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Show images\n",
    "imshow(torchvision.utils.make_grid(images[:3]))\n",
    "# Print labels\n",
    "print(' '.join('%5s' % labels[j] for j in range(3)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_LOC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/lobranch\"\n",
    "if not os.path.exists(SAVE_LOC):\n",
    "    os.makedirs(SAVE_LOC)\n",
    "\n",
    "# SAVE_LOC_OLC = HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/old-lc\"\n",
    "# if not os.path.exists(SAVE_LOC_OLC):\n",
    "#     os.makedirs(SAVE_LOC_OLC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_binary(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = torch.sigmoid(model(input))  # Apply sigmoid to model output to get probabilities.\n",
    "            output = torch.where(output > 0.5, 1, 0)  # Threshold probabilities at 0.5 to decide between classes 0 and 1.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += (output == label).sum().item()  # Increment correct predictions by the number of matches in the batch.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n",
    "\n",
    "def accuracy_multiclass(model, evaluation_set):\n",
    "    model.eval()  # Switches the model to evaluation mode.\n",
    "\n",
    "    no_correct, no_seen = 0, 0  # Initialize counters for correct predictions and total samples seen.\n",
    "\n",
    "    with torch.no_grad():  # Disables gradient calculation.\n",
    "        for input, label in evaluation_set:  # Iterate over the evaluation dataset.\n",
    "            output = model(input)  # Get the raw logits from the model.\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # Get the index of the max logit which represents the predicted class.\n",
    "            no_seen += label.size(0)  # Count the number of samples seen (batch size).\n",
    "            no_correct += pred.eq(label.view_as(pred)).sum().item()  # Compare predictions with true labels and sum up correct predictions.\n",
    "    \n",
    "    acc = no_correct / no_seen  # Calculate accuracy as the ratio of correct predictions to total samples.\n",
    "    model.train()  # Switch the model back to training mode.\n",
    "    return acc  # Return the computed accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x512 and 2048x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(inputs, labels)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 16\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_for_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# if self.config.loss_function == \"binary_cross_entropy\":\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#     outputs = torch.sigmoid(outputs)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# loss = loss_function(outputs, labels)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Personal\\Singapour\\PFE\\code_of_shu-heng_with_models\\pfe_lc_lora\\src\\models\\VGG16.py:43\u001b[0m, in \u001b[0;36mVGG16.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature(x)\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x512 and 2048x4096)"
     ]
    }
   ],
   "source": [
    "# get the base model (VGG16)\n",
    "\n",
    "model_for_checkpoint = VGG16()\n",
    "\n",
    "# creating branchpoints : \n",
    "\n",
    "epochs = 10\n",
    "isLoop = True\n",
    "optimizer = torch.optim.SGD(model_for_checkpoint.parameters(), lr=0.01, momentum=0.9) # momentum=0.9\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for iter, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        # print(inputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_for_checkpoint(inputs)\n",
    "\n",
    "        # if self.config.loss_function == \"binary_cross_entropy\":\n",
    "        #     outputs = torch.sigmoid(outputs)\n",
    "        \n",
    "        # loss = loss_function(outputs, labels)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print(\"Epoch {} | Iteration {} : Loss {}\".format(epoch, iter, loss.item()))\n",
    "        if iter % 20 == 0:\n",
    "            print(\"Running validation for {} Epoch, {} Iteration...\".format(epoch, iter))\n",
    "            # Previously: res = accuracy_binary(model_for_checkpoint, test_loader)\n",
    "            res = accuracy_multiclass(model_for_checkpoint, test_loader)\n",
    "\n",
    "            # res = accuracy_binary(model_for_checkpoint, test_loader)\n",
    "            \n",
    "            print(\"ACCURACY: {}\".format(res))\n",
    "            if res > 0.7:\n",
    "                torch.save(model_for_checkpoint.state_dict(), HDFP + \"/lobranch-snapshot/branchpoints/vgg/branch_{}.pt\".format(res))\n",
    "            if res > 0.9:\n",
    "                isLoop = False\n",
    "                break\n",
    "    if not isLoop:\n",
    "        break\n",
    "    print(\"Length of train_loader is: \", len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for VGG16:\n\tsize mismatch for classifier.0.weight: copying a param with shape torch.Size([4096, 512]) from checkpoint, the shape in current model is torch.Size([4096, 2048]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load from \"branch point\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m BRANCH_LOC \u001b[38;5;241m=\u001b[39m HDFP \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/lobranch-snapshot/branchpoints/vgg/branch_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(BRANCH_ACC)\n\u001b[1;32m---> 12\u001b[0m \u001b[43moriginal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBRANCH_LOC\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model_original\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(BRANCH_LOC))\n\u001b[0;32m     15\u001b[0m w, b \u001b[38;5;241m=\u001b[39m getBase(model_original)\n",
      "File \u001b[1;32mc:\\Users\\Bradf\\anaconda3\\envs\\py310\\lib\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for VGG16:\n\tsize mismatch for classifier.0.weight: copying a param with shape torch.Size([4096, 512]) from checkpoint, the shape in current model is torch.Size([4096, 2048])."
     ]
    }
   ],
   "source": [
    "DECOMPOSED_LAYERS = ['classifier.0.weight', 'classifier.3.weight', 'classifier.6.weight']\n",
    "RANK = -1\n",
    "SCALING = -1\n",
    "BRANCH_ACC = \"0.809\"\n",
    "\n",
    "# Set up weights for original AlexNet model\n",
    "original = VGG16()\n",
    "model_original = VGG16()\n",
    "\n",
    "# Load from \"branch point\"\n",
    "BRANCH_LOC = HDFP + \"/lobranch-snapshot/branchpoints/vgg/branch_{}.pt\".format(BRANCH_ACC)\n",
    "original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "model_original.load_state_dict(torch.load(BRANCH_LOC))\n",
    "\n",
    "w, b = getBase(model_original)\n",
    "model = VGG16_LowRank(w, b, rank = RANK)\n",
    "load_sd_decomp(torch.load(BRANCH_LOC), model, DECOMPOSED_LAYERS)\n",
    "learning_rate = 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "optimizer_full = torch.optim.SGD(model_original.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 0\n",
      "saving full base model @ ./volumes/Ultra Touch/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/lobranch/set_0\\base_model.pt\n",
      "Training Accuracy | Decomposed: 0.78125, Full : 0.8125\n",
      "Epoch: 0, Iteration: 1\n"
     ]
    }
   ],
   "source": [
    "delta_normal_max = []\n",
    "delta_normal_min = []\n",
    "delta_decomposed_max = []\n",
    "delta_decomposed_min = []\n",
    "full_accuracy = []\n",
    "decomposed_full_accuracy = []\n",
    "restored_accuracy = []\n",
    "lc_accuracy = []\n",
    "current_iter = 0\n",
    "current_set = 0\n",
    "\n",
    "current_iter_old_lc = 0\n",
    "current_set_old_lc = 0\n",
    "\n",
    "acc = lambda x, y : (torch.max(x, 1)[1] == y).sum().item() / y.size(0)\n",
    "\n",
    "for epch in range(20):\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        print(\"Epoch: {}, Iteration: {}\".format(epch, i))\n",
    "        \n",
    "        set_path = \"/set_{}\".format(current_set)\n",
    "        if not os.path.exists(SAVE_LOC + set_path):\n",
    "            os.makedirs(SAVE_LOC + set_path)\n",
    "\n",
    "        if i == 0 and epch == 0: # first iteration, create baseline model\n",
    "            base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "        else:\n",
    "            if i % 10 == 0: \n",
    "                # full snapshot!\n",
    "                new_model = lazy_restore(base, base_decomp, bias, VGG16(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, rank = RANK, scaling = SCALING)\n",
    "                original = new_model # Changing previous \"original model\" used to restore the loRA model.\n",
    "                \n",
    "                current_set += 1\n",
    "                current_iter = 0\n",
    "\n",
    "                set_path = \"/set_{}\".format(current_set)\n",
    "                if not os.path.exists(SAVE_LOC + set_path):\n",
    "                    os.makedirs(SAVE_LOC + set_path)\n",
    "                \n",
    "                # Rebuilding LoRA layers => reset model!\n",
    "                w, b = getBase(original)\n",
    "                model = VGG16_LowRank(w, b, rank = RANK)\n",
    "                optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "                load_sd_decomp(original.state_dict(), model, DECOMPOSED_LAYERS)\n",
    "                base, base_decomp = lc.extract_weights(model, SAVE_LOC + \n",
    "                                                       \"/set_{}\".format(current_set), DECOMPOSED_LAYERS)\n",
    "\n",
    "            else:\n",
    "                # Delta-compression\n",
    "                delta, decomp_delta, bias = lc.generate_delta(base, \n",
    "                                                                base_decomp, model.state_dict(), DECOMPOSED_LAYERS)\n",
    "                compressed_delta, full_delta, compressed_dcomp_delta, full_dcomp_delta  = lc.compress_delta(delta, \n",
    "                                                                                                            decomp_delta)\n",
    "                \n",
    "                # Saving checkpoint\n",
    "                lc.save_checkpoint(compressed_delta, compressed_dcomp_delta, bias, current_iter, SAVE_LOC + \n",
    "                                \"/set_{}\".format(current_set))\n",
    "    \n",
    "                base = np.add(base, full_delta) # Replace base with latest for delta to accumulate.\n",
    "                base_decomp = np.add(full_dcomp_delta, base_decomp)\n",
    "\n",
    "                current_iter += 1\n",
    "            \n",
    "        # # ==========================\n",
    "        # # Saving using LC-Checkpoint\n",
    "        # # ==========================\n",
    "                \n",
    "        # if i == 0 and epch == 0:\n",
    "        #     cstate = model_original.state_dict()\n",
    "        #     set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "        #     if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "        #         os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "        #     torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "        #     prev_state = olc.extract_weights(model_original)\n",
    "        # else:\n",
    "        #     if i % 10 == 0:\n",
    "        #         cstate = model_original.state_dict()\n",
    "        #         current_set_old_lc += 1\n",
    "        #         current_iter_old_lc = 0\n",
    "        #         set_path = \"/set_{}\".format(current_set_old_lc)\n",
    "        #         if not os.path.exists(SAVE_LOC_OLC + set_path):\n",
    "        #             os.makedirs(SAVE_LOC_OLC + set_path)\n",
    "        #         torch.save(cstate, SAVE_LOC_OLC + set_path + \"/initial_model.pt\")\n",
    "        #         prev_state = olc.extract_weights(model_original)\n",
    "        #     else:\n",
    "        #         cstate = model_original.state_dict()\n",
    "        #         old_lc_delta, old_lc_bias, _ = olc.generate_delta(prev_state, cstate)\n",
    "        #         olc_compressed_delta, update_prev = olc.compress_data(old_lc_delta, num_bits = 3)\n",
    "        #         olc.save_checkpoint(SAVE_LOC_OLC + \"/set_{}\".format(current_set_old_lc), olc_compressed_delta, \n",
    "        #                             old_lc_bias, epch, current_iter_old_lc)\n",
    "        #         prev_state = np.add(prev_state, update_prev)\n",
    "        #         current_iter_old_lc += 1\n",
    "        \n",
    "        # ==========================\n",
    "        # Training on Low-Rank Model\n",
    "        # ==========================\n",
    "\n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        # ======================\n",
    "        # Training on Full Model\n",
    "        # ======================\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer_full.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs_full = model_original(inputs)\n",
    "        loss_full = torch.nn.functional.cross_entropy(outputs_full,labels)\n",
    "        loss_full.backward()\n",
    "        optimizer_full.step()\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            print(\"Training Accuracy | Decomposed: {}, Full : {}\".format(acc(outputs, labels), \n",
    "                                                                         acc(outputs_full, labels)))\n",
    "\n",
    "        if i != 0  and i % 5 == 0: # Evaluation on testing set\n",
    "            full_accuracy.append(evaluate_accuracy(model_original, test_loader))\n",
    "            decomposed_full_accuracy.append(evaluate_accuracy(model, test_loader))\n",
    "            restored_model = lazy_restore(base, base_decomp, bias, VGG16(), \n",
    "                                          original.state_dict(), DECOMPOSED_LAYERS, \n",
    "                                          rank = RANK, scaling = SCALING)\n",
    "            restored_accuracy.append(evaluate_accuracy(restored_model, test_loader))\n",
    "            restored_lc_model = VGG16()\n",
    "            # restored_lc_model.load_state_dict(olc.restore_state_dict(prev_state, old_lc_bias, \n",
    "            #                                                       restored_model.state_dict()))\n",
    "            lc_accuracy.append(evaluate_accuracy(restored_lc_model, test_loader))\n",
    "            print(\"Full accuracy: {}, LC accuracy: {}, Decomposed-Full accuracy: {}, Decomposed-Restored accuracy: {}\".format(\n",
    "                full_accuracy[-1], lc_accuracy[-1], decomposed_full_accuracy[-1], restored_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/data.json\") as f:\n",
    "    data = json.load(f)\n",
    "full_accuracy = data['full_acc']\n",
    "lc_accuracy = data[\"lc_restored_accuracy\"]\n",
    "restored_accuracy = data[\"decomposed_restored_accuracy\"]\n",
    "decomposed_full_accuracy = data[\"decomposed_full_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30, 5))\n",
    "plt.title(\"VGG-16, Accuracy, Branched @ {} Accuracy\".format(BRANCH_ACC))\n",
    "plt.plot(full_accuracy, label = \"Default VGG-16\")\n",
    "plt.plot(lc_accuracy, label = \"LC VGG-16\")\n",
    "plt.plot(decomposed_full_accuracy, label = \"dLoRA VGG-16\")\n",
    "plt.plot(restored_accuracy, label = \"dLoRA + LC VGG-16\")\n",
    "plt.xticks([x for x in range(0, 120) if x % 6 == 0], [x for x in range(0, 20)])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rangex = [x for x in range(0, 120) if x % 6 == 0]\n",
    "rangey = [x for x in range(0, 20)]\n",
    "plt.figure(figsize = (40, 10))\n",
    "ax1 = plt.subplot(1, 2, 1)\n",
    "ax1.set_title(\"VGG-16 Absolute Accuracy Loss (Default VGG-16 vs LC + dLoRA VGG-16)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(restored_accuracy))), label = \"LC + dLoRA VGG-16\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC VGG-16\")\n",
    "plt.legend()\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.ylim(0, 0.5)\n",
    "ax2 = plt.subplot(1, 2, 2)\n",
    "ax2.set_title(\"VGG-16 Absolute Restoration Accuracy Loss (LC + dLoRA VGG-16 & LC VGG-16)\")\n",
    "plt.plot(np.abs(np.subtract(np.array(restored_accuracy), \n",
    "                     np.array(decomposed_full_accuracy))), label = \"LC + dLoRA VGG-16\")\n",
    "plt.plot(np.abs(np.subtract(np.array(full_accuracy), \n",
    "                     np.array(lc_accuracy))), label = \"LC VGG-16\")\n",
    "plt.legend()\n",
    "plt.axhline(y = 0.05, color = 'r')\n",
    "plt.xticks(rangex, rangey)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.ylabel(\"Absolute Accuracy Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getsize(sl):\n",
    "    dir = [x for x in os.listdir(sl)]\n",
    "    csize, usize = 0, 0\n",
    "    for set in dir:\n",
    "        for f in os.listdir(sl + \"/\" + set):\n",
    "            fp = sl + \"/{}/{}\".format(set, f)\n",
    "            csize += os.path.getsize(fp)\n",
    "            usize += 119.6 * math.pow(2, 20) # torch checkpoint same size\n",
    "    return csize, usize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_size, uncompressed_size = getsize(SAVE_LOC)\n",
    "a, b = evaluate_compression(uncompressed_size, compressed_size)\n",
    "compressed_size, uncompressed_size = getsize(SAVE_LOC_OLC)\n",
    "a1, b1 = evaluate_compression(uncompressed_size, compressed_size)\n",
    "\n",
    "print(\"LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a1, b1))\n",
    "print(\"LoRA + LC-Checkpoint + GZIP\")\n",
    "print(\"Compression Ratio: {}%, Space Savings: {}%\".format(a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = {\n",
    "    \"full_acc\" : full_accuracy,\n",
    "    \"decomposed_restored_accuracy\" : restored_accuracy,\n",
    "    \"decomposed_full_accuracy\" : decomposed_full_accuracy,\n",
    "    \"lc_restored_accuracy\" : lc_accuracy\n",
    "}\n",
    "with open(HDFP + \"/lobranch-snapshot/diffbitwidth-adaptive-rank/vgg/data.json\", 'w') as f:\n",
    "    json.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
